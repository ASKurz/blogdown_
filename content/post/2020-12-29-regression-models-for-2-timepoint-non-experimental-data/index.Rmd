---
title: Regression models for 2-timepoint non-experimental data
author: A. Solomon Kurz
date: '2020-12-29'
slug: ''
categories: []
tags:
  - Bayesian
  - brms
  - longitudinal
  - multilevel
  - R
  - robust
  - tidyverse
  - tutorial
subtitle: ''
summary: ''
authors: []
lastmod: '2021-04-22T10:56:23-07:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: /Users/solomonkurz/Dropbox/blogdown/content/post/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown/content/post/apa.csl  
link-citations: yes
---

```{r, echo = F, cache = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

## Purpose

In the contemporary longitudinal data analysis literature, 2-timepoint data (a.k.a. pre/post data) get a bad wrap. Singer and Willett [-@singerAppliedLongitudinalData2003, p. 10] described 2-timepoint data as only "marginally better" than cross-sectional data and @rogosaGrowthCurveApproach1982 give a technical overview on the limitations of 2-timepoint data. Limitations aside, sometimes two timepoints are all you have. In those cases, researchers should have a good sense of which data analysis options they have at their disposal. I recently came across [Jeffrey Walker](https://twitter.com/jwalkrunski)'s free [-@walkerElementsOfStatisticalModeling2018] text, [*Elements of statistical modeling for experimental biology*](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/), which contains a [nice chapter](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/models-for-longitudinal-experiments-pre-post-designs.html) on 2-timepoint experimental designs. Inspired by his work, this post aims to explore how one might analyze *non-experimental* 2-timepoint data within a regression model paradigm.

### I make assumptions.

In this post, I'm presuming you are familiar with longitudinal data analysis with conventional and multilevel regression. Though I don't emphasize it much, it will also help if you're familiar with Bayesian statistics. All code is in **R** [@R-base], with healthy doses of the **tidyverse** [@R-tidyverse; @wickhamWelcomeTidyverse2019]. The statistical models will be fit with **brms** [@R-brms; @burknerBrmsPackageBayesian2017; @burknerAdvancedBayesianMultilevel2018] and we'll also make some use of the **tidybayes** [@R-tidybayes] and **patchwork** [@R-patchwork] packages. If you need to shore up, I list some educational resources at the [end of the post][Next steps].

Load the primary packages.

```{r, warning = F, message = F}
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
```

## Warm-up

Before we jump into 2-timepoint data, we'll first explore how one might analyze a fuller data set of 6 timepoints. We will then reduce the data set to two different 2-timepoint versions for use in the remainder of the post.

### We need data.

We will simulate the data based on a conventional multilevel growth model of the kind you can learn about in @singerAppliedLongitudinalData2003, @hoffmanLongitudinalAnalysisModeling2015, or Kurz [-@kurzStatisticalRethinkingSecondEd2020, Chapter 14]. We'll have one criterion variable $y$ which will vary across participants $i$ and over time $t$. For simplicity, the systemic change over time will be linear. We might express it in statistical notation[^1] as

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} & = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i} + u_{1i} \text{time}_{ti} \\
\sigma & = \sigma_\epsilon \\
\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} & \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma & = \mathbf{SRS} \\
\mathbf S & = \begin{bmatrix} \sigma_0 & 0 \\ 0 & \sigma_1 \end{bmatrix} \\
\mathbf R & = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix},
\end{align*}
$$

where $\beta_0$ is the population-level intercept (initial status) and $\beta_1$ is the population-level slope (change over time). The $u_{0i}$ and $u_{1i}$ terms are the participant-level deviations around the population-level intercept and slope. Those $u$ deviations follow a bivariate normal distribution centered on zero (they are deviations, after all) and including a covariance matrix, $\mathbf \Sigma$. As is typical within the **brms** framework [see @burknerBrmsPackageBayesian2017], $\mathbf \Sigma$ is decomposed into a matrix of standard deviations ($\mathbf S$) and a correlation matrix ($\mathbf R$). Also notice we renamed our original $\sigma$ parameter as $\sigma_\epsilon$ to help distinguish it from the multilevel standard deviations in the $\mathbf S$ matrix ($\sigma_0$ and $\sigma_1$). In this way, $\sigma_0$ and $\sigma_1$ capture differences *between* participants in their intercepts and slopes, whereas $\sigma_\epsilon$ captures the differences *within* participants over time that occur apart from their linear trajectories.

To simulate data of this kind, we'll first set the true values for $\beta_0, \beta_1, \sigma_0, \sigma_1, \rho,$ and $\sigma_\epsilon$.

```{r}
b0     <- 0      # starting point (average intercept)
b1     <- 1      # growth over time (average slope)
sigma0 <- 1      # std dev in intercepts
sigma1 <- 1      # std dev in slopes
rho    <- -.5    # correlation between intercepts and slopes
sigma_e <- 0.75  # std dev within participants
```

Now combine several of those values to define the $\mathbf S, \mathbf R,$ and $\mathbf \Sigma$ matrices. Then simulate $N = 100$ participant-level intercepts and slopes.

```{r}
mu     <- c(b0, b1)          # combine the means in a vector
sigmas <- c(sigma0, sigma1)  # combine the std devs in a vector

s <- diag(sigmas)      # standard deviation matrix
r <- matrix(c(1, rho,  # correlation matrix
             rho, 1), nrow = 2)

# now matrix multiply s and r to get a covariance matrix
sigma <- s %*% r %*% s

# how many participants would you like?
n_id <- 100

# make the simulation reproducible
set.seed(1)

vary_effects <- 
  MASS::mvrnorm(n_id, mu = mu, Sigma = sigma) %>% 
  data.frame() %>% 
  set_names("intercepts", "slopes") %>% 
  mutate(id = 1:n_id) %>% 
  select(id, everything())

head(vary_effects)
```

Now we have our random intercepts and slopes, we're almost ready to simulate our $y_{ti}$ values. We just need to decide on how many values we'd like to collect over time and how we'd like to structure those assessment periods. To keep things simple, I'm going to specify six evenly-spaced timepoints. The first timepoint will be set to 0, the last timepoint will be set to 1, and the four timepoints in the middle will be the corresponding fractions.

```{r}
# how many timepoints?
time_points <- 6

d <-
  vary_effects %>% 
  # add in time
  expand(nesting(id, intercepts, slopes),
         time = seq(from = 0, to = 1, length.out = time_points)) %>% 
  # now use the model formula to compute y
  mutate(y = rnorm(n(), mean = intercepts + slopes * time, sd = sigma_e))

head(d, n = 10)
```

Before we move on, I should acknowledge that this simulation workflow is heavily influenced by McElreath [-@mcelreathStatisticalRethinkingBayesian2020, Chapter 14]. You can find a similar workflow in the [-@debruineUnderstandingMixedEffects2020] preprint by DeBruine and Barr, [*Understanding mixed effects models through data simulation*](https://psyarxiv.com/xp5cy/).

### Explore the data.

Before fitting the model, it might help if we look at what we've done. Here's a scatter plot of the random intercepts and slopes.

```{r, fig.width = 3, fig.height = 3}
# set the global plotting theme
theme_set(theme_linedraw() +
            theme(text = element_text(family = "Times"),
                  panel.grid = element_blank()))

p1 <-
  vary_effects %>% 
  ggplot(aes(x = intercepts, y = slopes)) +
  geom_point() +
  stat_ellipse(color = "grey50")

p1
```

The 95%-interval ellipse helps point out the negative correlation between the intercepts and slopes. Here's the Pearson's correlation.

```{r}
vary_effects %>% 
  summarise(rho = cor(intercepts, slopes))
```

It's no coincidence that value is very close to our data-generating `rho` value.

```{r}
rho
```

Now check the sample means and standard deviations of our `intercepts` and `slopes`.

```{r}
vary_effects %>% 
  summarise(b0 = mean(intercepts),
            b1 = mean(slopes),
            sigma0 = sd(intercepts),
            sigma1 = sd(slopes)) %>% 
  mutate_all(round, digits = 2)
```

Those aren't quite the true data-generating values for `b0` through `sigma1`, from above. But they're pretty decent sample approximations. With only $N = 100$ and $T = 6$, this is about as close as we should expect.

To get a sense of the `time` and `y` values, we'll plot them in two ways. First we'll plot a random subset from nine of our simulated participants. Then we'll plot the linear trajectories from all 100 participants, along with the grand mean trajectory.

```{r, fig.width = 7, fig.height = 3.75}
set.seed(1)

p2 <-
  d %>% 
  nest(data = c(intercepts, slopes, time, y)) %>% 
  slice_sample(n = 9) %>% 
  unnest(data) %>% 
  
  ggplot(aes(x = time, y = y)) +
  geom_point() +
  geom_abline(aes(intercept = intercepts, slope = slopes),
              color = "blue") +
  labs(subtitle = "random subset of 9 participants") +
  theme(strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~slopes)

p3 <-
  d %>% 
  ggplot(aes(x = time, y = y)) +
  geom_point(color = "transparent") +
  geom_abline(aes(intercept = intercepts, slope = slopes, group = id),
              color = "blue", size = 1/10, alpha = 1/2) +
  geom_abline(intercept = b0, slope = b1,
              color = "blue", size = 2) +
  labs(subtitle = "All participant-level trajectories, along\nwith the grand mean")

# combine
(p2 + p3) &
  scale_x_continuous(breaks = 0:5 / 5, labels = c(0, "", "", "", "", 1)) &
  coord_cartesian(ylim = c(-2.5, 3.5))
```

See how the points in the plots on the left deviate quite a bit from their linear trajectories? That's the result of $\sigma_\epsilon$.

### Fit the multilevel growth model.

Now we'll use **brms** to fit the multilevel growth model. If all goes well, we should largely reproduce our data-generating values in the posterior. Before fitting the model, we should consider a few things about the `brm()` syntax.

In this model and in most of the models to follow, we're relying on the default `brm()` priors. When fitting real-world models, you are much better off going beyond the defaults. However, I will generally deemphasize priors, in this post, to help keep the focus on the conceptual models.

Note how we set the `seed` argument. Though you don't need to do this, setting the `seed` makes the results more reproducible.

Also, note the custom settings for `iter` and `warmup`. Often times, the default settings are fine. But since we'll be comparing a lot of models, I want to make sure we have enough posterior draws from each to ensure stable estimates.

Okay, fit the model.

```{r m0, cache = T, results = "hide", message = F}
m0 <-
  brm(data = d,
      y ~ 1 + time + (1 + time | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

Check the model summary.

```{r}
print(m0)
```

A series of plots might help show how well our model captured the data-generating values.

```{r, fig.width = 5, fig.height = 3}
# name the parameters with the Greek terms
names <- c("beta[0]", "beta[1]", "sigma[0]", "sigma[1]", "rho", "sigma[epsilon]")

# for the vertical lines marking off the true values
vline <-
  tibble(name = names,
         true_value = c(b0, b1, sigma0, sigma1, rho, sigma_e))

# wrangle
posterior_samples(m0) %>% 
  select(b_Intercept:sigma) %>% 
  set_names(names) %>% 
  pivot_longer(everything()) %>% 
  
  # plot
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(.width = .95, normalize = "panels", size = 1/2) +
  geom_vline(data = vline,
             aes(xintercept = true_value),
             size = 1/4, linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("marginal posterior") +
  facet_wrap(~name, scales = "free", labeller = label_parsed)
```

The marginal posterior distribution for all the major summary parameters is summarized by the median (dot)  and percentile-based 95% interval (horizontal line). The true values are shown in the dashed vertical lines. Overall, we did okay.

As fun as this has all been, we've just been warming up.

### Make the 2-timepoint data.

Before fitting the 2-timepoint longitudinal models, we'll need to adjust the data, which currently contains values over six timepoints. Since it's easy to think of 2-timepoint data in terms of pre and post, we'll keep the data points for which `time == 0` and `time == 1`.

```{r}
small_data_long <-
  d %>% 
  filter(time == 0 | time == 1) %>% 
  select(-intercepts, -slopes) %>% 
  mutate(`pre/post` = factor(if_else(time == 0, "pre", "post"),
                             levels = c("pre", "post"))) 

head(small_data_long)
```

As the name implies, the `small_data_long` data are still in the long format. @hoffmanLongitudinalAnalysisModeling2015 described this as the *stacked format* and Singer and Willett [-@singerAppliedLongitudinalData2003] called this a *person-period data set*. Each level of `id` has two rows, one for each level of `time`, which is an explicit variable. In this formulation, `time == 0` is the same as the "pre" timepoint and `time == 1` is the same as "post." To help clarify that, we added a `pre/post` column.

We'll need a second variant of this data set, this time in the wide format.

```{r}
small_data_wide <-
  small_data_long %>% 
  select(-time) %>% 
  pivot_wider(names_from = `pre/post`, values_from = y) %>% 
  mutate(change = post - pre)

head(small_data_wide)
```

With our `small_data_wide` data, each level of `id` only has one row. The time-structured `y` column was broken up into a `pre` and `post` column, and we no longer have a variable explicitly defining *time*. We have a new column, `change`, which is the result of subtracting `pre` from `post`. In her text, Hoffman referred to this type of data structure as the *multivariate format* and Singer and Willett called it a *person-level data set*.

The information is essentially the same in these two data sets, `small_data_long` and `small_data_wide`. Yet, the models supported by them will provide different insights.

## 2-timepoint longitudinal models

Before we start fitting and interpreting models, we should prepare ourselves with an overview.

### Overview.

We will consider 20 ways to fit models based on 2-timepoint data. It seems like there multiple ways to categorize these. Here we'll break them up into four groupings.

The first four model types will take `post` as the criterion:

* $\mathcal M_1 \colon$ The unconditional post model (`post ~ 1`),
* $\mathcal M_2 \colon$ The simple autoregressive model (`post ~ 1 + pre`),
* $\mathcal M_3 \colon$ The bivariate autoregressive model (`bf(post ~ 1 + pre) + bf(pre ~ 1) + set_rescor(rescor = FALSE)`), and
* $\mathcal M_4 \colon$ The bivariate correlational pre/post model (`bf(post ~ 1) + bf(pre ~ 1) + set_rescor(rescor = TRUE)`).

The next four model types will take `change` as the criterion:

* $\mathcal M_5 \colon$ The unconditional change-score model (`change ~ 1`) and
* $\mathcal M_6 \colon$ The conditional change-score model (`change ~ 1 + pre`).
* $\mathcal M_7 \colon$ The bivariate conditional change-score model (`bf(change ~ 1 + pre) + bf(pre ~ 1) + set_rescor(rescor = FALSE)`), and
* $\mathcal M_8 \colon$ The bivariate correlational pre/change model (`bf(change ~ 1) + bf(pre ~ 1) + set_rescor(rescor = TRUE)`).

The next eight model types will take `y` as the criterion:

* $\mathcal M_9 \colon$ The grand-mean model (`y ~ 1`),
* $\mathcal M_{10} \colon$ The random-intercept model (`y ~ 1 + (1 | id)`),
* $\mathcal M_{11} \colon$ The cross-classified model (`y ~ 1 + (1 | id) + (1 | time)`),
* $\mathcal M_{12} \colon$ The simple liner model (`y ~ 1 + time`),
* $\mathcal M_{13} \colon$ The liner model with a random intercept (`y ~ 1 + time + (1 | id)`),
* $\mathcal M_{14} \colon$ The liner model with a random slope (`y ~ 1 + time + (0 + time | id)`),
* $\mathcal M_{15} \colon$ The multilevel growth model with regularizing priors (`y ~ 1 + time + (1 + time | id)`), and
* $\mathcal M_{16} \colon$ The fixed effects with correlated error model (`y ~ 1 + time + ar(time = time, p = 1, gr = id`).

The final four model types will expand previous ones with robust variance parameters:

* $\mathcal M_{17} \colon$ The cross-classified model with robust variances for discrete time (`bf(y ~ 1 + (1 | id) + (1 | time), sigma ~ 0 + factor(time))`),
* $\mathcal M_{18} \colon$ The simple liner model with robust variance for linear time (`bf(y ~ 1 + time, sigma ~ 1 + time)`),
* $\mathcal M_{19} \colon$ The liner model with correlated random intercepts for $\mu$ and $\sigma$ (`bf(y ~ 1 + time + (1 |x| id), sigma ~ 1 + (1 |x| id))`), and
* $\mathcal M_{20} \colon$ The liner model with a random slope for $\mu$ and uncorrelated random intercept for $\sigma$ (`bf(y ~ 1 + time + (0 + time | id), sigma ~ 1 + (1 | id))`).

As far as these model names go, I'm making no claim they are canonical. Call them what you want. My goal, here, is to use names that are minimally descriptive and similar to the terms you might find used by other authors.

### Models focusing on the second timepoint, `post`.

#### $\mathcal M_1 \colon$ The unconditional post model.

We can write the unconditional post model as

$$
\begin{align*}
\text{post}_i & \sim \operatorname{Normal}(\mu, \sigma) \\
\mu & = \beta_0,
\end{align*}
$$

where $\beta_0$ is both the model intercept and the estimate for the mean value of `post`. The focus this model places on `post` comes at the cost of any contextual information on what earlier values we might compare `post` to. Also, since the only variable in the model is `post`, this technically is *not* a 2-timepoint model. But given its connection to the models to follow, it's worth working through.

Here's how to fit the model with **brms**.

```{r m1, cache = T, results = "hide", message = F}
m1 <-
  brm(data = small_data_wide,
      post ~ 1,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m1)
```

We might compare those parameters with their sample values.

```{r}
small_data_wide %>% 
  summarise(mean = mean(post),
            sd = sd(post))
```

If you only care about computing the population estimates for $\mu_\text{post}$ and $\sigma_\text{post}$, this model does a great job. With no other variables in the model, this approach does a poor job telling us about growth processes.

#### $\mathcal M_2 \colon$ The simple autoregressive model.

The simple model with the `pre` scores predicting `post` is a substantial improvement from the previous one. It follows the form

$$
\begin{align*}
\text{post}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{pre}_i,
\end{align*}
$$

where the intercept $\beta_0$ is the expected value of `post` when `pre` is at zero. As with many other regression contexts, centering the predictor `pre` at the mean or some other meaningful value can help make $\beta_0$ more interpretable. Of greater interest is the $\beta_1$ coefficient, which is the expected deviation from $\beta_0$ for a one-unit increase in `pre`. But since `pre` and `post` are really the same variable `y` measured at two timepoints, it might be helpful if we express this model in another way. In perhaps more technical form, the simple model with `pre` predicting `post` is really an autoregressive model following the form

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \phi y_{t - 1,i},
\end{align*}
$$

where the criterion $y$ varies across persons $i$ and timepoints $t$. Here we only have two timepoints, $\text{post} = t$ and $\text{pre} = t - 1$. The strength of association between $y_t$ and $y_{t - 1}$ captured by the autoregressive parameter $\phi$, which is often expressed in a correlation metric.

Here's how to fit the model with `brm()`.

```{r m2, cache = T, results = "hide", message = F}
m2 <-
  brm(data = small_data_wide,
      post ~ 1 + pre,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m2)
```

Let's compare the `pre` coefficient with the Pearson's correlation between `pre` and `post`.

```{r}
small_data_wide %>% 
  summarise(correlation = cor(pre, post))
```

Well look at that. Recall that the $\beta_0$ parameter is the expected value in `post` when the predictor `pre` is at zero. Though the sample mean for `pre` is very close to zero, it's not exactly so.

```{r}
small_data_wide %>% 
  summarise(pre_mean = mean(pre))
```

Here's how to use that information to predict the mean value for `post`.

```{r}
fixef(m2)[1, 1] + fixef(m2)[2, 1] * mean(small_data_wide$pre)
```

We can get a full posterior summary with aid from `fitted()`.

```{r}
nd <- tibble(pre = mean(small_data_wide$pre))

fitted(m2, newdata = nd)
```

#### $\mathcal M_3 \colon$ The bivariate autoregressive model.

Though the simple autoregressive model gives us a sense of the strength of association between `pre` and `post`--and thus a sense of the stability in $y$ over time--, it still lacks an explicit parameter for mean value of $y$ at $t - 1$. Enter the bivariate autoregressive model,

$$
\begin{align*}
\text{post}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\text{pre}_i & \sim \operatorname{Normal}(\nu, \tau) \\
\mu_i & = \beta_0 + \beta_1 \text{pre}_i \\
\nu   & = \gamma_0,
\end{align*}
$$

where $\text{post}_i$ is still modeled as a simple linear function of $\text{pre}_i$, but now we also include an unconditional model for $\text{pre}_i$. This will give us an explicit comparison for where we started at the outset ($\nu$) and where we ended up ($\mu_i$). We can fit this model using the **brms** multivariate syntax where the two submodels are encased in `bf()` statements [@Bürkner2021Multivariate]. Also, be careful to use `set_rescor(rescor = FALSE)` to omit a residual correlation between the two. Their association is already handled with the $\beta_1$ parameter.

```{r m3, cache = T, results = "hide", message = F}
m3 <-
  brm(data = small_data_wide,
      bf(post ~ 1 + pre) +
        bf(pre ~ 1) +
        set_rescor(rescor = FALSE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m3)
```

Now that we've broken out the multivariate syntax, we might consider a second bivariate model.

#### $\mathcal M_4 \colon$ The bivariate correlational model.

The bivariate correlational model follows the form

$$
\begin{align*}
\begin{bmatrix} \text{post}_i \\ \text{pre}_i \end{bmatrix} & \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu \\ \nu \end{bmatrix}, \mathbf \Sigma \right) \\
\mu & = \beta_0 \\
\nu & = \gamma_0 \\
\mathbf \Sigma & = \mathbf{SRS} \\
\mathbf S & = \begin{bmatrix} \sigma & 0 \\ 0 & \tau \end{bmatrix} \\
\mathbf R & = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix},
\end{align*}
$$

where means of both `pre` and `post` are modeled in intercept-only models. However, the association between the two timepoints is captured in the residual correlation $\rho$. Yet because we have no predictors in for either variable, the "residual" correlation is really just a correlation. We might also gain some insights if we re-express this model in terms of $y_{ti}$ and $y_{t - 1,i}$:

$$
\begin{align*}
\begin{bmatrix} y_{ti} \\ y_{t - 1,i} \end{bmatrix} & \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu_t \\ \mu_{t - 1} \end{bmatrix}, \mathbf \Sigma \right) \\
\mu_t & = \beta_t \\
\mu_{t - 1} & = \beta_{t - 1} \\
\mathbf \Sigma & = \mathbf{SRS} \\
\mathbf S & = \begin{bmatrix} \sigma_t & 0 \\ 0 & \sigma_{t - 1} \end{bmatrix} \\
\mathbf R & = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix},
\end{align*}
$$

where what we formerly called an autoregressive coefficient in $\mathcal M_2$, we're now calling a correlation. Note also that this model freely estimates $\sigma_t$ and $\sigma_{t - 1}$. In some contexts, these are presumed to be equal. Though we won't be imposing that constraint, here, I believe it is possible with the **brms** [non-linear syntax](https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html) [@Bürkner2021Non_linear]. Anyway, here's how to fit the model with the `brm()` function .

```{r m4, cache = T, results = "hide", message = F}
m4 <-
  brm(data = small_data_wide,
      bf(post ~ 1) +
        bf(pre ~ 1) +
        set_rescor(rescor = TRUE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

Note out use of the `set_rescor(rescor = TRUE)` syntax in the model `formula`. This explicitly told `brm()` to include the residual correlation. Here's the summary.

```{r}
print(m4)
```

Now the intercept and sigma parameters do a good job capturing the sample statistics.

```{r, message = F}
small_data_wide %>% 
  pivot_longer(pre:post) %>% 
  group_by(name) %>% 
  summarise(mean = mean(value),
            sd = sd(value)) %>% 
  mutate_if(is.double, round, digits = 2)
```

The new 'rescor' line at the bottom of the `print()` summary approximates the Pearson's correlation of the two variables, much like the autoregressive parameter did two models up.

```{r}
small_data_wide %>% 
  summarise(correlation = cor(pre, post))
```

Another nice quality of this model is if you subtract $\gamma_0$ from $\beta_0$ (i.e., $\beta_t - \beta_{t - 1}$), you'd end up with the posterior mean of the change score.

```{r}
posterior_samples(m4) %>% 
  mutate(change = b_post_Intercept - b_pre_Intercept) %>% 
  summarise(mu_change = mean(change))
```

Keeping that in mind, let's switch gears to the first of the change-score models.

### Change-score models.

Instead of modeling `post`, $y_{ti}$, we might instead want to focus on the change from `pre` to `post`, $y_{ti} - y_{t - 1,i}$. When you subtract `pre` from `post` in your data set--like we did to make the `change` variable--, the product is often referred to as a change score or difference score, $y_\Delta$. Though they're conceptually intuitive and simple to compute, change scores have a long history of criticisms in the methodological literature, particularly around issues of reliability [see @lordStatisticalTheoriesMental1968; @rogosaGrowthCurveApproach1982; cf. @kisbu2013monte]. Here we consider four change-score models simply as options.

#### $\mathcal M_5 \colon$ The unconditional change-score model.

 We've already saved that in our `small_data_wide` data as `change`. Here's what the unconditional change-score model[^2] looks like:

$$
\begin{align*}
\text{change}_i & \sim \operatorname{Normal}(\mu, \sigma) \\
\mu & = \beta_0,
\end{align*}
$$

where $\beta_0$ is the expected value for $\text{change}_i$. In the terms of the last model, $\text{change}_i = \text{post}_i - \text{pre}_i$ or, in the terms of the simple autoregressive model, $y_{\Delta i} = y_{ti} - y_{t - 1,i}$.

```{r m5, cache = T, results = "hide", message = F}
m5 <-
  brm(data = small_data_wide,
      change ~ 1,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m5)
```

Thus, this model suggests the average change from `pre` to `post` was about 1.2 units. We can compute the sample statistics for that in two ways.

```{r}
small_data_wide %>% 
  summarise(change = mean(change),
            `post - pre` = mean(post - pre))
```

The major deficit in the unconditional change model is that change is disconnected from any reference points. We have no explicit way of knowing what number we changed from or what number we changed to. The next model offers a solution.

#### $\mathcal M_6 \colon$ The conditional change-score model.

Instead of fitting an unconditional model of `change`, why not condition on the initial `pre` value? We might express this as

$$
\begin{align*}
\text{change}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{pre}_i,
\end{align*}
$$

where $\beta_0$ is now the expected value for `change` when `pre` is at zero. As with the simple autoregressive model, centering the predictor `pre` at the mean or some other meaningful value can help make $\beta_0$ more interpretable. Perhaps of greater interest, the $\beta_1$ coefficient allows us to predict different levels of `change`, conditional in the initial values at `pre`.

```{r m6, cache = T, results = "hide", message = F}
m6 <-
  brm(data = small_data_wide,
      change ~ 1 + pre,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m6)
```

Since the intercept in this model is the expected `change` value based on when `pre == 0`, it might be easiest to interpret that value when using the mean of `pre`.

```{r}
fixef(m6)[1, 1] + fixef(m6)[2, 1] * mean(small_data_wide$pre)
```

That is the point prediction for the mean of `change`. Let's compare that to the sample value.

```{r}
small_data_wide %>% 
  summarise(mean = mean(change))
```

Of course, we can get a fuller summary using the `fitted()` method.

```{r}
nd <- tibble(pre = mean(small_data_wide$pre))

fitted(m6, newdata = nd)
```

Note how the coefficient for `pre` is about `r round(fixef(m6)[2, 1], 2)`. This is a rough analogue of the negative correlation among the intercepts and slopes in the original data-generating model.

```{r}
rho
```

It tells us something very similar; participants with higher values at `pre` tended to have lower `change` values. Much like with the simple autoregressive model, a deficit of this model is there is no explicit parameter for the expected value of `pre`, which we can amend by fitting a bivariate model.

#### $\mathcal M_7 \colon$ The bivariate conditional change-score model.

The bivariate conditional change-score model follows the form

$$
\begin{align*}
\text{change}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\text{pre}_i & \sim \operatorname{Normal}(\nu, \tau) \\
\mu_i & = \beta_0 + \beta_1 \text{pre}_i \\
\nu   & = \gamma_0,
\end{align*}
$$

where the simple linear model of $\text{change}_i$ conditional on $\text{pre}_i$ is coupled with an unconditional intercept-only model for $\text{pre}_i$. We can fit this model with **brms** by way of the multivariate syntax, where the two submodels are encased in `bf()` statements and we set `set_rescor(rescor = FALSE)` to omit a residual correlation between the two.

```{r m7, cache = T, results = "hide", message = F}
m7 <-
  brm(data = small_data_wide,
      bf(change ~ 1 + pre) +
        bf(pre ~ 1) +
        set_rescor(rescor = FALSE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m7)
```

Now we have an intercept for `pre`, we can use the model parameters to compute the expected values for `pre`, `change`, and `post`.

```{r}
posterior_samples(m7) %>% 
  mutate(pre    = b_pre_Intercept,
         change = b_change_Intercept + b_change_pre * b_pre_Intercept) %>% 
  mutate(post = pre + change) %>% 
  pivot_longer(pre:post) %>% 
  group_by(name) %>% 
  mean_qi(value) %>% 
  mutate_if(is.double, round, digits = 2)
```

The posterior means are in the `value` column and the lower- and upper-levels of the percentile-based 95% intervals are in the `.lower` and `.upper` columns. Now compare those mean estimates with the sample means.

```{r, message = F}
small_data_wide %>% 
  pivot_longer(-id) %>% 
  group_by(name) %>% 
  summarise(mean = mean(value)) %>% 
  mutate_if(is.double, round, digits = 2)
```

As handy as this model is, the $\beta_1$ coefficient might not be in the most intuitive metric. Let's reparameterize.

#### $\mathcal M_8 \colon$ The bivariate correlational pre/change model.

The bivariate correlational pre/change model follows the form

$$
\begin{align*}
\begin{bmatrix} \text{change}_i \\ \text{pre}_i \end{bmatrix} & \sim \operatorname{MVNormal} \left (\begin{bmatrix} \mu \\ \nu \end{bmatrix}, \mathbf \Sigma \right) \\
\mu & = \beta_0 \\
\nu & = \gamma_0 \\
\mathbf \Sigma & = \mathbf{SRS} \\
\mathbf S & = \begin{bmatrix} \sigma & 0 \\ 0 & \tau \end{bmatrix} \\
\mathbf R & = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix},
\end{align*}
$$

where means of both `pre` and `change` are modeled in intercept-only models and the association between the two is captured by the correlation $\rho$. And again, because we have no predictors in for either variable, the "residual" correlation is really just a correlation. Here's how to fit the model with **brms**.

```{r m8, cache = T, results = "hide", message = F}
m8 <-
  brm(data = small_data_wide,
      bf(change ~ 1) +
        bf(pre ~ 1) +
        set_rescor(rescor = TRUE),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m8)
```

Now the parameter in the 'Residual Correlations' section of the summary output is a close analogue to our original data-generating `rho` parameter.

```{r}
rho
```

Those with higher `pre` values tended to have lower `change` values. We can look at that with a plot of the data.

```{r, fig.width = 6, fig.height = 3}
p2 <-
  small_data_wide %>% 
  ggplot(aes(x = pre, y = change)) +
  geom_point() +
  stat_ellipse(color = "grey50")

(p1 + p2) &
  coord_cartesian(xlim = range(small_data_wide$pre),
                  ylim = range(small_data_wide$change))
```

Here we've placed the scatter plot of the data-generating `id`-level `intercepts` and `slopes` next to the scatter plot of the `pre` and `change` scores. They are not exactly the same, but the latter are a partial consequence of the former. This is why the correlation parameter in our `m8` model closely, but not exactly, resembled the data-generating `rho` parameter.

Okay, let's switch gears again.

### Models modeling the criterion $y_{ti}$ directly.

All of the models focusing on `post` or `change` used the wide version of the data, `small_data_wide`. The remaining models will all take advantage of the long data set, `small_data_long`, and take $y_{ti}$ as the criterion. Consequently, most of these models will use some version of the multilevel model.

#### $\mathcal M_9 \colon$ The grand-mean model.

The simplest model we might fit using the long version of the 2-timepoint data, `small_data_long`, is what we might call the grand-mean model, or what @hoffmanLongitudinalAnalysisModeling2015 called the *between-person empty model*. It follows the form

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu, \sigma) \\
\mu & = \beta_0,
\end{align*}
$$

where the intercept $\beta_0$ is the expected value value for $y$ across all $i$ participants and $t$ timepoints. We fit this with `brm()` much like we fit the unconditional post model and the unconditional change-score model.

```{r m9, cache = T, results = "hide", message = F}
m9 <-
  brm(data = small_data_long,
      y ~ 1 ,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m9)
```

We might check these with the sample statistics for `y`.

```{r}
small_data_long %>% 
  summarise(mean = mean(y),
            sd = sd(y))
```

Though this model did to a good job describing the population values for the mean and standard deviation for `y`, it did a terrible job telling us about change in `y`, about individual differences in that change, or about anything else of interest we might have as longitudinal researchers.

#### $\mathcal M_{10} \colon$ The random-intercept model.

Now that we have a grand mean, we might want to ask what kinds of variables would help explain the variation around the grand mean. From a multilevel perspective, the first source of variation of interest will be across participants, which we can express by allowing the mean to vary by participant. This is what Hoffman called the *within-person empty model*  and the *empty means, random intercept model*. It follows the form

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \beta_0 + u_{\text{id},i} \\
u_\text{id} & \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
\sigma & = \sigma_\epsilon,
\end{align*}
$$

where $\beta_0$ is now the grand mean among the participant-level means in $y_{ti}$. The participant-level deviations from the grand mean are expressed as $u_{\text{id},i}$, which is normally distributed with a mean at zero (these are *deviations*, after all) and a standard deviation of $\sigma_\text{id}$. The $\sigma_\epsilon$ parameter is a mixture of the variation within participants and over time. With **brms**, we can fit the random-intercept model model like so.

```{r m10, cache = T, results = "hide", message = F}
m10 <-
  brm(data = small_data_long,
      y ~ 1 + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .99))
```

```{r}
print(m10)
```

Our $\beta_0$ intercept returned a very similar estimate for the grand mean, but we now interpret it as the grand mean for the within-person means for `y`. The variation in the `id`-level deviations around the grand mean, which we called $u_{\text{id},i}$, is summarized by $\sigma_\text{id}$ in the 'Group-Level Effects' section of the summary output.

Authors of many longitudinal text books [e.g., @hoffmanLongitudinalAnalysisModeling2015; @singerAppliedLongitudinalData2003] typically present this model as a way to directly compare the between- and within-person variation in the data by way of the intraclass correlation coefficient (ICC),

$$\text{ICC} = \frac{\text{between-person variance}}{\text{total variance}} = \frac{\sigma_\text{id}^2}{\sigma_\text{id}^2 + \sigma_\epsilon^2}.$$

When using frequentist methods, the ICC is typically expressed with a point estimate. When working with all our posterior draws, we can get full posterior distributions within the Bayesian framework.

```{r, fig.width = 4, fig.height = 2.5}
posterior_samples(m10) %>% 
  mutate(icc = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %>% 
  ggplot(aes(x = icc, y = 0)) +
  stat_halfeye(.width = .95) +
  scale_x_continuous("Intraclass correlation coefficient (ICC)", 
                     breaks = 0:5 / 5, expand = c(0, 0), limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL)
```

The ICC is a proportion, which limits it to the range of zero to one. Here it suggests that 0--20% of the variation in our data is due to differences *between* participants; the remaining variation occurs within them. Given that our data were collected across time, it might make sense to fit a model that explicitly accounts for time.

#### $\mathcal M_{11} \colon$ The cross-classified model.

A direct extension of the random-intercept model is the cross-classified multilevel model [@mcelreathStatisticalRethinkingBayesian2015, Chapter 12], which we might express as

$$
\begin{align*}
y_{ti}   & \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} & = \beta_0 + u_{\text{id},i} + u_{\text{time},i} \\
u_\text{id} & \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
u_\text{time} & \sim \operatorname{Normal}(0, \sigma_\text{time}) \\
\sigma & = \sigma_\epsilon,
\end{align*}
$$

where $\sigma_\text{id}$ captures systemic differences between participants, $\sigma_\text{time}$ captures systemic variation across the two timepoints, and $\sigma_\epsilon$ captures the variation within participants over time. Another way to think of this model is as a Bayesian multilevel version of the repeated-measures ANOVA[^3], where variance is partitioned into a between level ($\sigma_\text{id} \approx \text{SS}_\text{between}$), a model level ($\sigma_\text{time} \approx \text{SS}_\text{model}$), and error ($\sigma_\epsilon \approx \text{SS}_\text{error}$).

```{r m11, cache = T, results = "hide", message = F}
m11 <-
  brm(data = small_data_long,
      y ~ 1 + (1 | time) + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9999,
                     max_treedepth = 11))
```

```{r}
print(m11)
```

The intercept is the grand mean across all measures of `y`. The first row in the 'Group-Level Effects' section is our summary for $\sigma_\text{id}$, which gives us a sense of the variation between participants in their overall tendencies in the criterion `y`. If we use the `posterior_samples()`, we can even look at the posteriors for the $u_{\text{id},i}$ parameters, themselves.

```{r, fig.width = 3.5, fig.height = 4.25}
posterior_samples(m11) %>% 
  pivot_longer(starts_with("r_id")) %>% 
  
  ggplot(aes(x = value, y = reorder(name, value))) +
  stat_pointinterval(point_interval = mean_qi, .width = .95, size = 1/6) +
  scale_y_discrete(expression(italic(i)), breaks = NULL) +
  labs(subtitle = expression(sigma[id]~is~the~summary~of~the~variation~across~these),
       x = expression(italic(u)[id][','*italic(i)]))
```

Given that each of the $u_{\text{id},i}$ parameters is based primarily on two data points (the two data points per participant), it should be no surprise they are fairly wide. Even a few more measurement occasions within participants will narrow them substantially. If you fit the same model using the original 6-timepoint data, you'll see the 95% intervals are almost half as wide.

```{r, eval = F, echo = F}
m11b <-
  brm(data = d,
      y ~ 1 + (1 | time) + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .99,
                     max_treedepth = 10))

ranef(m11b)$id[, , "Intercept"] %>% 
  data.frame() %>% 
  mutate(width = Q97.5 - Q2.5) %>% 
  summarise(mean_width = mean(width)) %>% 
  pull() /
  
  ranef(m11)$id[, , "Intercept"] %>% 
  data.frame() %>% 
  mutate(width = Q97.5 - Q2.5) %>% 
  summarise(mean_width = mean(width)) %>% 
  pull()
```

Perhaps of greater interest are the $u_{\text{time},i}$ parameters. If you combine them with the intercept, you'll get the model-based expected values at both timepoints.

```{r}
posterior_samples(m11) %>% 
  transmute(pre  = b_Intercept + `r_time[0,Intercept]`,
            post = b_Intercept + `r_time[1,Intercept]`) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mean_qi(value) %>% 
  mutate_if(is.double, round, digits = 2)
```

The final variance parameter, $\sigma_\text{time}$, captures the within-participant variation over time. With a model like this, it seems natural to directly compare the magnitudes of the three variance parameters, which answers the question: *Where's the variance at*? Here we'll do so with a plot.

```{r, fig.width = 4, fig.height = 2.75}
posterior_samples(m11) %>% 
  select(sd_id__Intercept:sigma) %>% 
  set_names("sigma[id]", "sigma[time]", "sigma[epsilon]") %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("sigma[epsilon]", "sigma[time]", "sigma[id]"))) %>%
  
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_halfeye(.width = .95, size = 1, normalize = "xy") +
  scale_x_continuous("marginal posterior", expand = expansion(mult = c(0, 0.05)), breaks = c(0, 1, 2, 5)) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(xlim = c(0, 5.25),
                  ylim = c(1.5, 3.5)) +
  theme(axis.text.y = element_text(hjust = 0))
```

At first glance, it might be surprising how wide the $\sigma_\text{time}$ posterior is compared to the other two. Yet recall this parameter is summarizing the standard deviation of only two levels. If you have experience with multilevel models, you'll know that it can be difficult to estimate a variance parameter with few levels--two levels is the extreme lower limit. This is why we had to fiddle with the `adapt_delta` and `max_treedepth` parameters within the `brm()` function to get the model to sample properly. Though we pulled this off using default priors, don't be surprised if you have to use tighter priors when fitting a model like this.

#### $\mathcal M_{12} \colon$ The simple liner model.

The last three models focused on the grand mean and sources of variance around that grand mean. A more familiar looking approach might be to fit a simple linear model with `y` conditional on `time`,

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} & = \beta_0 + \beta_1 \text{time}_{ti},
\end{align*}
$$

where the intercept $\beta_0$ is the expected value at the first timepoint and $\beta_1$ captures the change in $y_{ti}$ for the final timepoint.

```{r m12, cache = T, results = "hide", message = F}
m12 <-
  brm(data = small_data_long,
      y ~ 1 + time,
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m12)
```

Now our $\beta_0$ and $\beta_1$ parameters are the direct pre/post single-level analogues to the population-level $\beta_0$ and $\beta_1$ parameters from our original multilevel model based on the full 6-timepoint data set.

```{r}
fixef(m0) %>% round(digits = 2)
```

The major deficit in this model, which is the reason you'll see it criticized in the methodological literature, is it ignores how the `y` values are nested within levels of  `id`. The only variance parameter, $\sigma$, was estimated under the typical assumption that the residuals are all independent of one another. Sure, the model formula accounted for the overall trend in `time`, but it ignored the insights revealed from many of the other models the capture between-participant correlations in intercepts and slopes. This means that if you know something about the value of one's residual for when `time == 0`, you'll also know something about where to expect their residual for when `time == 1`. The two are not independent. As long as we're working with the data in the long format, we'll want to account for this, somehow.

#### $\mathcal M_{13} \colon$ The liner model with a random intercept.

A natural first step to accounting for how the `y` values are nested within levels of `id` is to fit a random-intercept model, or what @hoffmanLongitudinalAnalysisModeling2015 called the *fixed linear time, random intercept model*. It follows the form

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} & = \beta_{0i} + \beta_1 \text{time}_{ti} \\
\beta_{0i} & = \gamma_0 + u_{0i} \\
u_{0i}  & \sim \operatorname{Normal}(0, \sigma_0) \\
\sigma & = \sigma_\epsilon,
\end{align*}
$$

where $\beta_{0i}$ is the intercept and $\beta_1$ is the time slope. Although this parameterization holds the variation in slopes constant across participants, between-participant variation is at least captured in $\beta_{0i}$, which is decomposed into a grand mean, $\gamma_0$, and participant-level deviations around that grand mean, $u_{0i}$. Those participant-level deviations are summarized by the $\sigma_0$ parameter. In this model, $\sigma_\epsilon$ is a mixture of within-participant variation and between-participant variation in slopes.

```{r m13, cache = T, results = "hide", message = F}
m13 <-
  brm(data = small_data_long,
      y ~ 1 + time + (1 | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m13)
```

The $\beta$ parameters are very similar to those from the simple linear model, above.

```{r}
fixef(m12) %>% round(digits = 2)
```

But now look at the size of $\sigma_0$, which suggests substantial differences in staring points. To get a sense of what this means, we'll plot all 100 participant-level trajectories with a little help from `fitted()`.

```{r, fig.width = 3, fig.height = 3}
nd <- distinct(small_data_long, id, time)

fitted(m13, 
       newdata = nd) %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m13)[1, 1],
              slope = fixef(m13)[2, 1],
              size = 3, color = "blue") +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = "Random intercepts, fixed slope",
       y = "y") +
  coord_cartesian(ylim = c(-1, 2))
```

The bold blue line in the middle is based on the population-level intercept and slope, whereas the thinner black lines are the participant-level trajectories. To keep from [overplotting](https://www.data-to-viz.com/caveat/overplotting.html), we're only showing the posterior means, here. Because we only allowed the intercept to vary across participants, all the slopes are identical. And indeed, look at all the variation we see in the intercepts--an insight lacking in the simple linear model.

#### $\mathcal M_{14} \colon$ The liner model with a random slope.

The counterpoint to the last model is to allow the time slopes, but not the intercepts, vary across participants:

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma) \\
\mu_{ti} & = \beta_0 + \beta_{1i} \text{time}_{ti} \\
\beta_{1i} & = \gamma_1 + u_{1i} \\
u_{1i} & \sim \operatorname{Normal}(0, \sigma_1) \\
\sigma   & = \sigma_\epsilon,
\end{align*}
$$

where $\beta_0$ is the intercept for all participants. Now $\beta_{1i}$ is the population mean for the distribution of slopes, which vary across participants, the standard deviation for which is measured by $\sigma_1$.

```{r m14, cache = T, results = "hide", message = F}
m14 <-
  brm(data = small_data_long,
      y ~ 1 + time + (0 + time | id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .95))
```

```{r}
print(m14)
```

Here's random-slopes alternative to the random-intercepts plot from the last section.

```{r, fig.width = 3, fig.height = 3}
fitted(m14, 
       newdata = nd) %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m14)[1, 1],
              slope = fixef(m14)[2, 1],
              size = 3, color = "blue") +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = "Fixed intercept, random slopes",
       y = "y") +
  coord_cartesian(ylim = c(-1, 2))
```

But why choose between random intercepts or random slopes?

#### $\mathcal M_{15} \colon$ The multilevel growth model with regularizing priors.

If you were modeling 2-timepoint data with conventional frequentist estimators (e.g., maximum likelihood), you can have random intercepts or random slopes, but you can't have both; that would require data from three timepoints or more. But because Bayesian models bring in extra information by way of the priors, you can actually fit a full multilevel growth model with both random intercepts and slopes:

$$
\begin{align*}
y_{ti}   & \sim \operatorname{Normal}(\mu_{ti}, \sigma_\epsilon ) \\
\mu_{ti} & = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i} + u_{1i} \text{time}_{ti} \\
\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} & \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma & = \mathbf{SRS} \\
\mathbf S & = \begin{bmatrix} \sigma_0 & 0 \\ 0 & \sigma_1 \end{bmatrix} \\
\mathbf R & = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}.
\end{align*}
$$

The trick is you have to go beyond the diffuse **brms** default settings for the priors for $\sigma_0$ and $\sigma_1$. If you have high-quality information from theory or previous studies, you can base the priors on those. Another approach is to use regularizing priors. Given standardized data, members of the Stan team like either $\operatorname{Normal}^+(0, 1)$ or $\operatorname{Student-t}^+(3, 0, 1)$ for variance parameters (see the [Generic prior for anything](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#generic-prior-for-anything) section from the [*Prior choice recommendations* wiki](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)). In the second edition of his text, McElreath generally favored the $\operatorname{Exponential}(1)$ prior [@mcelreathStatisticalRethinkingBayesian2020], which is the approach we'll experiment with, here. It'll also help if we use a regularizing prior on the correlation among the intercepts and slopes, $\rho$.

```{r m15, cache = T, results = "hide", message = F}
m15 <-
  brm(data = small_data_long,
      y ~ 1 + time + (1 + time | id),
      seed = 1,
      prior = prior(exponential(1), class = sd) +
        prior(lkj(4), class = cor),
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9995))
```

Even with our tighter priors, we still had to adjust the `adapt_delta` parameter to improve the quality of the MCMC sampling. Take a look at the model summary.

```{r}
print(m15)
```

Though the posteriors, particularly for the $\sigma$ and $\rho$ parameters, are not as precise as with the 6-timepoint data, we now have a model with a summary mirroring the structure of the data-generating model. Yet compared to the data-generating values, the estimates for $\sigma_1$ and $\rho$ are particularly biased toward zero. Here's a look at the trajectories.

```{r, fig.width = 3, fig.height = 3.2}
fitted(m15,
       newdata = nd) %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m15)[1, 1],
              slope = fixef(m15)[2, 1],
              size = 3, color = "blue") +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = "Random intercepts AND random slopes\n(2-timepoint data)",
       y = "y") +
  coord_cartesian(ylim = c(-1, 2))
```

For comparision, here's the plot for the original 6-timepoint model.

```{r, fig.width = 3, fig.height = 3.2}
fitted(m0,
       newdata = nd) %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  
  ggplot(aes(x = time, y = Estimate, group = id)) +
  geom_abline(intercept = fixef(m0)[1, 1],
              slope = fixef(m0)[2, 1],
              size = 3, color = "blue") +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_x_continuous(breaks = 0:1) +
  labs(subtitle = "Random intercepts AND random slopes\n(6-timepoint data)",
       y = "y") +
  coord_cartesian(ylim = c(-1, 2))
```

There wasn't enough information in the 2-timepoint data set to capture the complexity in the full 6-timepoint data set.

#### $\mathcal M_{16} \colon$ The fixed effects with correlated error model.

Though our Bayesian 2-timepoint version of the full multilevel growth model was exciting, it's not generally used in the wild. Even with our tighter regularizing priors, there just wasn't enough information in the data to do the model justice. A very different and humbler approach is to combine the simple linear model with the autoregressive model,

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \mathbf \Sigma) \\
\mu_{ti} & = \beta_0 + \beta_1 \text{time}_{ti} \\
\mathbf \Sigma & = \mathbf{SRS} \\
\mathbf S & = \begin{bmatrix} \sigma & 0 \\ 0 & \sigma \end{bmatrix} \\
\mathbf R & = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix},
\end{align*}
$$

where $\rho$ captures the correlation between the responses in the two timepoints, $t$ and $t - 1$, which is an alternative to the way the mixed model from above handles the dependencies (a.k.a [heteroskedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity)) inherent in longitudinal data. Note how $\mathbf \Sigma$ in this model is defined very differently from the full multilevel growth model from above. To fit this model with **brms**, we use the `ar()` syntax.

```{r m16, cache = T, results = "hide", message = F}
m16 <-
  brm(data = small_data_long,
      y ~ time + ar(time = time, p = 1, gr = id),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m16)
```

This summary suggests that, after you account for the linear trend, the correlation between $y_{ti}$ and $y_{t - 1,i}$ is about `r round(posterior_summary(m16)[3, 1], 2)`. Though we don't get `id`-specific variance parameters, this model does account for the nonindependence of the data over time. If you scroll back up, notice how similar this is to the correlation from the bivariate correlational model, `m4`.

### The models with robust variance parameters.

In the social sciences, many of our theories and statistical models are comparisons of or changes in group means. Every model in this blog post uses the normal likelihood, which parameterizes the criterion in terms of $\mu$ and $\sigma$. Every time we added some kind of linear model, we focused that model around the $\mu$. But contemporary Bayesian software allows us to model the $\sigma$ parameter, too. Within the **brms** framework, @Bürkner2021Distributional calls these [distributional models](https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html). The final four models under consideration all use some form of the distributional modeling syntax to relax unnecessarily restrictive assumptions on the variance parameters. Though this section is not exhaustive, it should give a sense of how flexible this approach can be.

#### $\mathcal M_{17} \colon$ The cross-classified model with robust variances for discrete time.

One of the criticisms of the conventional repeated measures ANOVA approach is how it presumes the variances in $y$ are constant over time. However, we can relax that constraint with a model like

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} & = \beta_0 + u_{\text{id},i} + u_{\text{time},t} \\
\log (\sigma_{ti}) & = \eta_{\text{time},t} \\
u_\text{id} & \sim \operatorname{Normal}(0, \sigma_\text{id}) \\
u_\text{time} & \sim \operatorname{Normal}(0, \sigma_\text{time}),
\end{align*}
$$

where we are now modeling both parameters in the likelihood, $\mu$ AND $\sigma$. The second line shows a typical-looking model for $\mu_{ti}$. All the excitement lies in the third line, which contains the linear model for $\log (\sigma_{ti})$. The reason we are modeling $\log (\sigma_{ti})$ rather than directly modeling $\sigma$ is to avoid solutions that predict negative values for $\sigma$. For this model, it's unlikely we'd run into that problem. But since the **brms** default is to use the log link anytime we model $\sigma$ within the distributional modeling syntax, we'll just get used to the log link right from the start. If you are unfamiliar with link functions, they're widely used within the generalized linear modeling framework. Logistic regression with the logit link and Poisson regression with the log link are two widely-used examples. For more on link functions and the generalized linear model, check out the texts by @agrestiFoundationsLinearGeneralized2015; Gelman, Hill, and Vehtari [-@gelmanRegressionOtherStories2020]; and McElreath [-@mcelreathStatisticalRethinkingBayesian2015; -@mcelreathStatisticalRethinkingBayesian2020].

Anyway, the linear model for $\mu_{ti}$ is exactly the same as with the original cross-classified model, `m11`; it includes a grand mean ($\beta_0$) and two kinds of deviations around that grand mean ($u_{\text{id},i}$ and $u_{\text{time},t}$). The model for $\log (\sigma_{ti})$ contains an intercept, which varies across the two levels of time, $\eta_{\text{time},t}$. Here's how to fit the model with `brms::brm()`.

```{r m17, cache = T, results = "hide", message = F}
m17 <-
  brm(data = small_data_long,
      bf(y ~ 1 + (1 | time) + (1 | id),
         sigma ~ 0 + factor(time)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .999,
                     max_treedepth = 12))
```

```{r}
print(m17)
```

Even though we didn't explicitly ask to use the log link in our `brm()` syntax, you can look at the second line in the `print()` output to see that it was automatically used. Though I won't explore how to do so, here, one can fit this model without the log link. Anyway, the primary focus in this model is the `sigma_factortime0` and `sigma_factortime1` lines in the 'Population-Level Effects' section of the output. Those are the summaries for the $\sigma$ parameters, conditional on whether `time == 0` or `time == 1`. Though is might be difficult to evaluate parameters on the log scale, we can simply exponentiate them to convert them back to their natural metric.

```{r}
fixef(m17)[2:3, c(1, 3:4)] %>% exp()
```

In this case, it looks like the two parameters are largely overlapping. If we work directly with the posterior draws, we can compute a formal difference score and plot the results.

```{r, fig.width = 4, fig.height = 2.75}
posterior_samples(m17) %>% 
  mutate(`sigma[time==0]` = exp(b_sigma_factortime0),
         `sigma[time==1]` = exp(b_sigma_factortime1),
         `sigma[time==1]-sigma[time==0]` = exp(b_sigma_factortime1) - exp(b_sigma_factortime0)) %>% 
  pivot_longer(`sigma[time==0]`:`sigma[time==1]-sigma[time==0]`) %>% 
  
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye(.width = .95) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 3.1)) +
  xlab("marginal posterior") +
  theme(axis.text.y = element_text(hjust = 0))
```

In this case, it looks like there was little difference between $\sigma_{\text{time} = 0}$ and $\sigma_{\text{time} = 1}$. This shouldn't be a surprise; we simulated that data that way. However, it won't always be like this in real-world data.

#### $\mathcal M_{18} \colon$ The simple liner model with robust variance for linear time.

Our cross-classified approach treated `time` as a factor. Here we'll treat it as a continuous variable in the models of both $\mu$ and $\sigma$. This will be a straight extension of the simple linear model, `m12`, following the form

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} & = \beta_0 + \beta_1 \text{time}_{ti} \\
\log(\sigma_{ti}) & = \eta_0 + \eta_1 \text{time}_{ti},
\end{align*}
$$

where the intercept $\beta_0$ is the expected value at the first timepoint and $\beta_1$ captures the change in $y_i$ for the final timepoint. Now $\log(\sigma_{ti})$ has a similar linear model, where $\eta_0$ is the expected log of the standard deviation at the first timepoint and $\eta_1$ captures the change standard deviation for the final timepoint.

```{r m18, cache = T, results = "hide", message = F}
m18 <-
  brm(data = small_data_long,
      bf(y ~ 1 + time, 
         sigma ~ 1 + time),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

```{r}
print(m18)
```

Even though this model looks very different from the last one, we can wrangle the posterior draws a little to make a similar plot comparing $\sigma$ at the two timepoints.

```{r, fig.width = 4, fig.height = 2.75}
posterior_samples(m18) %>% 
  mutate(`sigma[time==0]` = exp(b_sigma_Intercept),
         `sigma[time==1]` = exp(b_sigma_Intercept + b_sigma_time * 1),
         `sigma[time==1]-sigma[time==0]` = exp(b_sigma_Intercept) - exp(b_sigma_Intercept + b_sigma_time)) %>% 
  pivot_longer(`sigma[time==0]`:`sigma[time==1]-sigma[time==0]`) %>% 
  
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye(.width = .95) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 3.1)) +
  xlab("marginal posterior") +
  theme(axis.text.y = element_text(hjust = 0))
```

Though this model is robust to differences in $\sigma$ based on timepoint, it still ignores systemic differences across participants. The next model tackles that that limitation in spades.

#### $\mathcal M_{19} \colon$ The liner model with correlated random intercepts for $\mu$ and $\sigma.$

Here we return to the multilevel model framework to accommodate participant-level differences for both $\mu$ and $\sigma$:

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} & = \beta_0 + \beta_1 \text{time}_{ti} + u_{0i}  \\
\log(\sigma_{ti}) & = \eta_0  + u_{2i} \\
\begin{bmatrix} u_{0i} \\ u_{2i} \end{bmatrix} & \sim \operatorname{MVNormal} \left (\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf \Sigma \right) \\
\mathbf \Sigma & = \mathbf{SRS} \\
\mathbf S & = \begin{bmatrix} \sigma_0 & 0 \\ 0 & \sigma_2 \end{bmatrix} \\
\mathbf R & = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix},
\end{align*}
$$

where $\beta_0$ is the grand mean for the intercepts and $u_{0i}$ are the participant-level deviations around that grand mean. $\beta_1$ is the time slope, which is invariant across all participants in this model. The $\eta_0$ parameter is the grand mean for the log standard deviations and $u_{2i}$ captures the participant-level deviations around that grand mean. In the fourth line, we learn that $u_{0i}$ and $u_{2i}$ are multivariate normal, with a mean vector of two zeros and a variance/covariance matrix $\mathbf \Sigma$. As is typical within the **brms** framework, we decompose $\mathbf \Sigma$ into a variance matrix $\mathbf S$ and correlation matrix $\mathbf R$. Of particular interest is the $\rho$ parameter, which captures the correlation in the participant-level intercepts and participant-level standard deviations. Here's how to fit the model with **brms**.

```{r m19, cache = T, results = "hide", message = F}
m19 <-
  brm(data = small_data_long,
      bf(y ~ 1 + time + (1 |x| id),
         sigma ~ 1 + (1 |x| id)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000,
      control = list(adapt_delta = .9))
```

You may have noticed the `|x|` parts in the `formula` lines for `y` and `sigma`. What that did was tell **brms** we wanted those parameters to be correlated. That is, that's how we estimated the $\rho$ parameter. There was nothing special about including `x` between the vertical lines. We could have used any other character. The important thing is that we used the same character in both. Anyway, here's the model summary.

```{r}
print(m19)
```

Once again, **brms** used the log link for $\sigma$. If you want to see $\eta_0$ in its natural $\sigma$ metric, exponentiate.

```{r}
fixef(m19)["sigma_Intercept", c(1, 3:4)] %>% exp()
```

The `sd(sigma_Intercept)` row in the 'Group-Level Effects' section shows the variation in those $\log \sigma$'s. It might be easier to appreciate them in a plot.

```{r, fig.width = 3.5, fig.height = 4.25}
posterior_samples(m19) %>% 
  pivot_longer(starts_with("r_id__sigma")) %>% 
  mutate(sigma_i = exp(sd_id__sigma_Intercept + value)) %>% 
  
  ggplot(aes(x = sigma_i, y = reorder(name, sigma_i))) +
  stat_pointinterval(point_interval = mean_qi, .width = .95, size = 1/6) +
  scale_y_discrete(expression(italic(i)), breaks = NULL) +
  labs(subtitle = expression(sigma[2]~summarizes~the~variation~across~these),
       x = expression(sigma[italic(i)]))
```

In this case, there was not a lot of variation in $\sigma$ across participants. This is because we simulated the data that way. Though it may be hard to model participant-level variances with 2-timepoint data, I have found it comes in handy in real-world data sets based on more measurement occasions.

Finally, it might be useful to consider our $\rho$ parameter, which suggested a mild negative correlation between the $u_{0i}$ and $u_{2i}$ deviations. Here's how you might visualize that in a plot.

```{r, fig.width = 3.5, fig.height = 3.25}
bind_rows(
  posterior_samples(m19) %>% select(starts_with("r_id[")) %>% set_names(1:100),
  posterior_samples(m19) %>% select(starts_with("r_id__sigma") %>% set_names(1:100))
) %>% 
  mutate(iter = rep(1:c(n() / 2), times = 2),
         type = rep(c("intercept", "log_sigma"), each = n() / 2)) %>% 
  pivot_longer(-c(iter, type)) %>% 
  pivot_wider(names_from = type, values_from = value) %>% 
  
  ggplot(aes(x = intercept, y = log_sigma, group = name)) +
  stat_ellipse(geom = "polygon", level = .01, alpha = 1/4) +
  labs(x = expression(italic(u)[0][italic(i)]),
       y = expression(log(italic(u)[2][italic(i)])))
```

Each of the ovals is a 1% ellipse of the bivariate posterior for $u_{0i}$ and $\log(u_{2i})$. Notice how using ellipses helps reveal the differences in the between- and within-person patterns.

This approach where residual variance parameters vary across participants has its origins in the work of [Donald Hedeker](https://health.uchicago.edu/faculty/donald-hedeker-phd) and colleagues [@hedekerApplicationMixedeffectsLocation2008; @hedekerModelingWithinsubjectVariance2012]. More recently, [Philippe Rast](https://twitter.com/rastlab) and colleagues (particularly graduate student, [Donald Williams](wdonald_1985)) have adapted this approach for use within the Stan/**brms** ecosystem [@williamsBayesianNonlinearMixedeffects2019a; @williamsSurfaceUnearthingWithinperson2019; @williamsBayesianMultivariateMixedeffects2019a; @williamsPuttingIndividualReliability2019].

#### $\mathcal M_{20} \colon$ The liner model with a random slope for $\mu$ and uncorrelated random intercept for $\sigma$.

Though we can find interesting things when we allow the random components in the $\mu$ and $\sigma$ models, we don't have to think of them as covarying. Here we fit an extension of the linear model with a random time slope, where we add an orthogonal random intercept for $\sigma$:

$$
\begin{align*}
y_{ti} & \sim \operatorname{Normal}(\mu_{ti}, \sigma_{ti}) \\
\mu_{ti} & = \beta_0 + \beta_1 \text{time}_{ti} + u_{1i} \\
\log(\sigma_{ti}) & = \eta_0  + u_{2i}  \\
u_{1i} & \sim \operatorname{Normal}(0, \sigma_1) \\
u_{2i} & \sim \operatorname{Normal}(0, \sigma_2),
\end{align*}
$$

where the two random components, $u_{1i}$ and $u_{2i}$, are now modeled with separate normal distributions, $\operatorname{Normal}(0, \sigma_1)$ and $\operatorname{Normal}(0, \sigma_2)$.

```{r m20, cache = T, results = "hide", message = F}
m20 <-
  brm(data = small_data_long,
      bf(y ~ 1 + time + (0 + time | id),
         sigma ~ 1 + (1 | id)),
      seed = 1,
      cores = 4, chains = 4, iter = 3500, warmup = 1000)
```

Note that in sharp contrast with our syntax for the previous model, this time we did not employ the `|x|` syntax in the `formula` lines for `y` and `sigma`. By omitting the `|x|` syntax, we omitted the correlation among those two random effects. Here's the summary.

```{r}
print(m20)
```

We can see the $\sigma_1$ and $\sigma_2$ summaries in the 'Group-Level Effects' section. It's hard to compare them directly, because one is based on parameters in the log metric. But we can at least get a sense of what these parameters are summarizing by plotting the bivariate posterior for $u_{1i}$ and $\log(u_{2i})$.

```{r, fig.width = 3.5, fig.height = 3.25}
bind_rows(
  posterior_samples(m20) %>% select(starts_with("r_id[")) %>% set_names(1:100),
  posterior_samples(m20) %>% select(starts_with("r_id__sigma") %>% set_names(1:100))
) %>% 
  mutate(iter = rep(1:c(n() / 2), times = 2),
         type = rep(c("slope", "log_sigma"), each = n() / 2)) %>% 
  pivot_longer(-c(iter, type)) %>% 
  pivot_wider(names_from = type, values_from = value) %>% 
  
  ggplot(aes(x = slope, y = log_sigma, group = name)) +
  stat_ellipse(geom = "polygon", level = .01, alpha = 1/4) +
  labs(x = expression(italic(u)[1][italic(i)]),
       y = expression(log(italic(u)[2][italic(i)])))
```

Notice how, this time, the 1% ellipses suggest no clear association between these two dimensions.

## Next steps

As promised, here I recommend some resources for understanding the models in this post.

### Books focusing on longutidinal data analysis.

* My introduction to longitudinal data analysis was through Singer and Willett [-@singerAppliedLongitudinalData2003], [*Applied longitudinal data analysis: Modeling change and event occurrence*](https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968). Their focus was on the multilevel growth model and on survival analysis, primary from a maximum-likelihood frequentist framework. However, they generally avoided 2-timepoint data analysis.

* Hoffman's [-@hoffmanLongitudinalAnalysisModeling2015] text, [*Longitudinal analysis: Modeling within-person fluctuation and change*](https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025) is another thorough introduction to the multilevel growth model, from a frequentist perspective. Hoffman covered 2-timepoint data analysis and variants from the ANOVA family. The text comes with a companion website, [https://www.pilesofvariance.com/](https://www.pilesofvariance.com/), which contains extensive data and code files for reproducing the material. 

* Newsom's [-@newsom2015longitudinal] text, [*Longitudinal structural equation modeling: A comprehensive introduction*](http://www.longitudinalsem.com/), covers longitudinal data analysis from a structural equation modeling (SEM) perspective. Even for those not interested in SEM, his Chapter 4 does a nice job introducing the autoregressive and change-score models. The companion website, [http://www.longitudinalsem.com/](http://www.longitudinalsem.com/), contains data and script files for most of the problems in the text.

### Books introducing regression.

* Gelman, Hill, and Vehtari's [-@gelmanRegressionOtherStories2020] [*Regression and other stories*](https://www.cambridge.org/core/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C) contains a thorough introduction to single-level regression, primarily from a Bayesian framework. The text is not oriented around longitudinal analyses, per se, but it does include several chapters on causal inference. Vehtari hosts a GitHub repo, [https://github.com/avehtari/ROS-Examples](https://github.com/avehtari/ROS-Examples), where you can download the data files and **R** scripts for many of the examples.

* Both editions of McElreath's [-@mcelreathStatisticalRethinkingBayesian2015; -@mcelreathStatisticalRethinkingBayesian2020] [*Statistical rethinking: A Bayesian course with examples in R and Stan*](https://xcelab.net/rm/statistical-rethinking/) provide a thorough introduction to Bayesian regression, both single-level and multilevel. McElreath also touched on causal inference and included a few examples of longitudinal data analysis. His text includes extensive examples of **R** code and his website, [https://xcelab.net/rm/statistical-rethinking/](https://xcelab.net/rm/statistical-rethinking/), contains information about the accompanying statistical software.

## Session info

```{r}
sessionInfo()
```

## References

[^1]: Throughout this post, my statistical notation will be a blend of sensibilities from @singerAppliedLongitudinalData2003, @hoffmanLongitudinalAnalysisModeling2015, and @mcelreathStatisticalRethinkingBayesian2020.

[^2]: Though I'm no fan of the null-hypothesis significance testing paradigm, it might be helpful to point out if one were to focus on whether zero is within the 95% interval bounds of our $\beta_0$ parameter, you be viewing this model through the lens of the repeated-measures $t$-test. For more on that connection, see Chapter 3 in @newsom2015longitudinal.

[^3]: There's some debate over how to think about the repeated measures ANOVA and what its closest multilevel analogue might be. For a nice collection of perspectives, check out [this Twitter thread](https://twitter.com/SolomonKurz/status/1342645143082594304).
