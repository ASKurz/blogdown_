---
title: Example power analysis report, II
author: A. Solomon Kurz
date: '2022-03-05'
slug: ''
categories: []
tags:
  - multilevel
  - power
  - lme4
  - R
  - tidyverse
  - tutorial
subtitle: ''
summary: ''
authors: []
lastmod: '2022-03-05T08:45:58-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: /Users/solomonkurz/Dropbox/blogdown/content/post/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown/content/post/apa.csl  
link-citations: yes

disable_codefolding: false
codefolding_show: hide
codefolding_nobutton: false
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="preamble" class="section level2">
<h2>Preamble</h2>
<p>In <a href="https://solomonkurz.netlify.app/post/2021-07-02-example-power-analysis-report/">an earlier post</a>, I gave an example of what a power analysis report could look like for a multilevel model. At my day job, I was recently asked for a rush-job power analysis that required a multilevel model of a different kind and it seemed like a good opportunity to share another example.</p>
<p>For the sake of confidentiality, some of the original content will be omitted or slightly altered. But the overall workflow is about <span class="math inline">\(95\%\)</span> faithful to the original report I submitted to my boss. To understand this report, you should know:</p>
<ul>
<li>my boss has some experience fitting multilevel models, but they’re not a stats jock;</li>
<li>we had good pilot data; and</li>
<li>this document was meant for internal purposes only.</li>
</ul>
<p>The pilot data were from <span class="math inline">\(N = 50\)</span> persons, collected on pre- and post-treatment assessment. We had a <span class="math inline">\(4\%\)</span> missing data rate a pre-treatment and <span class="math inline">\(10\%\)</span> missing data a post-treatment, which became an important factor in my analytic strategy. The larger study this power analysis is designed to inform would follow the same basic structure.</p>
<p>Okay, we’re ready for the report. I’ll wrap this post up with a few comments in an <a href="#afterward">afterward</a> section.</p>
</div>
<div id="executive-summary" class="section level2">
<h2>Executive summary</h2>
<p>A total sample size of <span class="math inline">\(N = \mathbf{218}\)</span> is the minimum number to reliably detect a <em>small</em> size effect size (i.e., Cohen’s <span class="math inline">\(d = 0.2\)</span>). This recommendation assumes</p>
<ul>
<li>a pre-post single-group design, and</li>
<li>a <span class="math inline">\(7\%\)</span> overall missing data rate.</li>
</ul>
<p>The small effect size estimates and the missing data rate are based on pilot data collected in early <span class="math inline">\(2022\)</span>.</p>
<p>The remainder of this report details how I came to these conclusions. For full transparency, I will supplement prose with the statistical code used for all computations. By default, the code is hidden in this document. However, if you are interested in the code, you should be able to make it appear by selecting “Show All Code” in the dropdown menu from the “Code” button on the upper-right corner.</p>
</div>
<div id="statistical-framework-and-cohens-d" class="section level2">
<h2>Statistical framework and Cohen’s <span class="math inline">\(d\)</span></h2>
<p>In this report, Cohen’s <span class="math inline">\(d\)</span> is meant to indicate a standardized mean difference. The <span class="math inline">\(d = 0.2\)</span> is the conventional cutt-off value for a <em>small</em> effect size <span class="citation">(see <a href="#ref-cohenStatisticalPowerAnalysis1988a" role="doc-biblioref">Cohen, 1988</a>)</span>. In a conventional single-group pre-post study, the formula for <span class="math inline">\(d\)</span> is</p>
<p><span class="math display">\[
d = \frac{\bar y_\text{post} - \bar y_\text{pre}}{s_\text{pre}},
\]</span></p>
<p>where <span class="math inline">\(\bar y_\text{pre}\)</span> and <span class="math inline">\(\bar y_\text{post}\)</span> are the sample means at each time point and <span class="math inline">\(s_\text{pre}\)</span> is the sample standard deviation at the baseline pre-intervention assessment. The major limitation of this approach is it does not accommodate missing data and our pilot data indicated we should expect about <span class="math inline">\(7\%\)</span> of values will be missing. However, analyzing the data within a multilevel model using a full-information estimator (e.g., maximum likelihood) will allow us to estimate the effect size in the presence of missing values. Under this strategy, the model would follow the equation</p>
<p><span class="math display">\[
\begin{align*}
y_{it} &amp; \sim \mathcal N(\mu_{it}, \sigma_\epsilon) \\
\mu_{it} &amp; = \beta_0 + \beta_1 \text{time}_{it} + u_{0i} \\
u_{0i} &amp; \sim \mathcal N(0, \sigma_0),
\end{align*}
\]</span></p>
<p>where the outcome variable <span class="math inline">\(y\)</span> varies across <span class="math inline">\(i\)</span> persons and over <span class="math inline">\(t\)</span> time. Presuming the data are approximately continuous, they are modeled as normally distributed with a conditional mean <span class="math inline">\(\mu_{it}\)</span>. The <span class="math inline">\(\beta_0\)</span> parameter is the mean value at the pre-treatment baseline assessment and the <span class="math inline">\(\beta_1\)</span> parameter is the difference at the post-treatment assessment. Person-specific differences are captured in the model by <span class="math inline">\(u_{0i}\)</span>, which is itself modeled as normally distributed with a mean of zero and standard deviation <span class="math inline">\(\sigma_0\)</span>. The remaining variance is captured by <span class="math inline">\(\sigma_\epsilon\)</span>, the within-person variation term.</p>
<p>Following the methodological framework developed by Feingold <span class="citation">(<a href="#ref-feingoldEffectSizeForGMA2009" role="doc-biblioref">2009</a>, <a href="#ref-feingoldARegressionFramework2013" role="doc-biblioref">2013</a>)</span>, we can use the results from this model to compute the pre-post standardized mean difference effect size as</p>
<p><span class="math display">\[
d = \frac{\beta_1}{s_\text{pre}}.
\]</span></p>
<p>A major advantage of this approach is it also returns model-based <span class="math inline">\(95\%\)</span> confidence intervals for the estimate of <span class="math inline">\(d\)</span>.</p>
</div>
<div id="power-from-simulation" class="section level2">
<h2>Power from simulation</h2>
<p>For studies following simple designs, there are known formulas for deriving statistical power. However, these formulas are not robust to missing data, which our pilot trial suggests we will have in a larger-scale study. In such a case, one can determine the statistical power for a study with a simulation-based power analysis. With this approach, one must:</p>
<ol style="list-style-type: decimal">
<li>Determine the characteristics of the data you anticipate collecting.</li>
<li>Define the primary statistical model and parameter(s) of interest.</li>
<li>Select a sample size.</li>
<li>Build a custom simulation function that will iteratively
<ol style="list-style-type: lower-alpha">
<li>simulate data resembling the real data you intend to collect,</li>
<li>fit a statistical model to each data simulation, and</li>
<li>summarize the parameter(s) of interest in each model.</li>
</ol></li>
<li>Iterate a large number of times with the chosen settings to ensure sable results.</li>
<li>Summarize the results.</li>
<li>Iterate across different sample sizes and effect sizes, as needed.</li>
</ol>
<p>In our case, the parameter of interest is <span class="math inline">\(\beta_1\)</span> from the statistical model outlined in the previous section and the smallest effect size we would reliably like to detect is <span class="math inline">\(d = 0.2\)</span>.</p>
<p>Here is the <strong>R</strong> code for the simulation:</p>
<pre class="r"><code># load the necessary packages
library(faux)
library(tidyverse)
library(lme4)
library(broom.mixed)

# define the simulation function
sim_lmer &lt;- function(seed = 1, n_group = 20, d = 0.2) {
  
  # set the seed 
  set.seed(seed)
  
  # simulate the data
  d_sim &lt;- rnorm_multi(
    n = n_group,
    mu = c(0, d),
    sd = c(1, 1), 
    r = .5, 
    varnames = list(&quot;pre&quot;, &quot;post&quot;)
  ) %&gt;% 
    mutate(id = 1:n(),
           # simulate missingness
           pre  = ifelse(rbinom(n = n(), size = 1, prob = .04) == 1, NA, pre),
           post = ifelse(rbinom(n = n(), size = 1, prob = .10) == 1, NA, post)) %&gt;% 
    pivot_longer(-id, values_to = &quot;y&quot;) %&gt;% 
    mutate(time = ifelse(name == &quot;pre&quot;, 0, 1))
  
  # fit the model
  fit_sim &lt;- lmer(
    data = d_sim,
    y ~ 1 + time + (1 | id)
  )
  
  # summarize the results for beta_1
  tidy(fit_sim, conf.int = TRUE) %&gt;% 
    filter(term == &quot;time&quot;)
  
}

# run the simulation 1000 times with a sample size of 218
sim218 &lt;- tibble(seed = 1:1000) %&gt;% 
  mutate(tidy = map(seed, sim_lmer, n_group = 218)) %&gt;% 
  unnest(tidy)

# summarize the overall power results
z_critical &lt;- qnorm(p = .975, mean = 0, sd = 1)

sim218 %&gt;% 
  summarise(p = mean(statistic &gt; z_critical))</code></pre>
<p>After running the simulation several times with different values of <span class="math inline">\(N\)</span>, we determined a total sample size of <span class="math inline">\(N = 218\)</span> was sufficient to meet the conventional <span class="math inline">\(.8\)</span> power threshold.</p>
</div>
<div id="session-information" class="section level2">
<h2>Session information</h2>
<p>The make these analyses more reproducible, here is the session information on the software used to make this report and the analyses herein.</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.1.2 (2021-11-01)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## loaded via a namespace (and not attached):
##  [1] bookdown_0.23   digest_0.6.29   R6_2.5.1        jsonlite_1.7.3 
##  [5] magrittr_2.0.2  evaluate_0.14   blogdown_1.5    stringi_1.7.4  
##  [9] rlang_1.0.1     cli_3.1.1       rstudioapi_0.13 jquerylib_0.1.4
## [13] bslib_0.3.0     rmarkdown_2.10  tools_4.1.2     stringr_1.4.0  
## [17] xfun_0.25       yaml_2.2.1      fastmap_1.1.0   compiler_4.1.2 
## [21] htmltools_0.5.2 knitr_1.33      sass_0.4.0</code></pre>
</div>
<div id="afterward" class="section level2">
<h2>Afterward</h2>
<p>I presented the original report with an HTML document, which used the R Markdown <a href="https://community.rstudio.com/t/notebook-with-code-folding-hide-by-default/55845">code folding</a> option, which hid my code, by default. Since I’m not aware of a good way to use code folding with <strong>blogdown</strong> blog posts, here you see the code in all its glory.</p>
<p>This is <em>another</em> example of a frequentist power analysis. Had this not been a rush job, I would have considered running a Bayesian power analysis where the model used weakly-regularizing priors. So it goes…</p>
<p>As to the statistical model, had there not been missing data at the pre-treatment assessment, I might have opted for a simpler single-level model. Another consideration, though, is my boss likes multilevel models and since it seemed like a good option here, why not? As to the simulation code, you may have noticed I used the <code>rnorm_multi()</code> function from the <a href="https://github.com/debruine/faux"><strong>faux</strong> package</a> <span class="citation">(<a href="#ref-R-faux" role="doc-biblioref">DeBruine, 2021</a>)</span>, which I find handy for data with small numbers of time points. In <code>rnorm_multi()</code>, I set the correlation between the two time points at <span class="math inline">\(\rho = .5\)</span>, which was based on the simple correlation from the pilot data.</p>
<p>In retrospect, it might have been a good idea to have considered simulating the data with a slightly larger standard deviation for the post-treatment assessment. My experience is it’s not uncommon for variation to increase over time. I could have used the sample standard deviations from the pilot data as a guide. Now that I’ve written this in a blog post, perhaps it’ll occur to me more readily next time.</p>
<p>The analysis from the pilot data suggested a post-treatment <span class="math inline">\(d = 0.35\)</span>, <span class="math inline">\(95\% \text{ CI} [0.05, 0.65]\)</span>. In the power analysis, we used <span class="math inline">\(d = 0.2\)</span> because (a) we wanted a conservative estimate and (b) it was the smallest effect size we were interested in reliably detecting.</p>
<p>Had I intended to share a report like this for a broader audience, possibly as supplemental material for a paper, I might have explained my custom <code>sim_lmer()</code> code. Since this was originally meant for internal use, my main goal was to present the results with an extra bit of transparency and so that even if I somehow lost my original files, I would always have the basics of the code documented in the report I emailed to my boss.</p>
<p>I should also add that even though it isn’t spelled out in the code, I came to the <span class="math inline">\(N = 218\)</span> figure by iterating over several different sample size options. It seemed unnecessary to clutter up the file with those details.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-cohenStatisticalPowerAnalysis1988a" class="csl-entry">
Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em>. <span>L. Erlbaum Associates</span>. <a href="https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467">https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467</a>
</div>
<div id="ref-R-faux" class="csl-entry">
DeBruine, L. (2021). <em><span class="nocase">faux</span>: <span>Simulation</span> for factorial designs</em> [Manual]. <a href="https://github.com/debruine/faux">https://github.com/debruine/faux</a>
</div>
<div id="ref-feingoldEffectSizeForGMA2009" class="csl-entry">
Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. <em>Psychological Methods</em>, <em>14</em>(1), 43. <a href="https://doi.org/10.1037/a0014699">https://doi.org/10.1037/a0014699</a>
</div>
<div id="ref-feingoldARegressionFramework2013" class="csl-entry">
Feingold, A. (2013). A regression framework for effect size assessments in longitudinal modeling of group differences. <em>Review of General Psychology</em>, <em>17</em>(1), 111–121. <a href="https://doi.org/10.1037/a0030048">https://doi.org/10.1037/a0030048</a>
</div>
</div>
</div>
