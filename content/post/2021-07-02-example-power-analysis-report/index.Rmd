---
title: Example power analysis report
author: A. Solomon Kurz
date: '2021-07-02'
slug: ''
categories: []
tags:
  - multilevel
  - power
  - powerlmm
  - R
  - tidyverse
  - tutorial
subtitle: ''
summary: ''
authors: []
lastmod: '2021-07-02T11:15:08-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: /Users/solomonkurz/Dropbox/blogdown/content/post/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown/content/post/apa.csl  
link-citations: yes

disable_codefolding: false
codefolding_show: hide
codefolding_nobutton: false
---

## Context

In one of my recent Twitter posts, I got pissy and complained about a vague power-analysis statement I saw while reviewing a manuscript submitted to a scientific journal.

```{r echo = FALSE}
blogdown::shortcode('tweet', '1409626961161371648')
```

It wasn't my best moment and I ended up apologizing for my tone.

```{r echo = FALSE}
blogdown::shortcode('tweet', '1409634560485199876')
```

However, the broader issue remains. If you plan to analyze your data with anything more complicated than a $t$-test, the power analysis phase gets tricky. The manuscript I was complaining about used a complicated multilevel model as its primary analysis. I'm willing to bet that most applied researchers (including the authors of that manuscript) have never done a power analysis for a multilevel model and probably have never seen what one might look like, either. The purpose of this post is to give a real-world example of just such an analysis.

Over the past couple years, I've done a few multilevel power analyses as part of my day job. In this post, I will reproduce one of them. For the sake of confidentiality, some of the original content will be omitted or slightly altered. But the overall workflow will be about 90% faithful to the original report I submitted to my boss. To understand this report, you should know:

* my boss has some experience fitting multilevel models, but they're not a stats jock;
* we had pilot data from two different sources, each with its strengths and weaknesses; and
* this document was meant for internal purposes only, though I believe some of its contents did make it into other materials.

At the end, I'll wrap this post up with a few comments. Here's the report:

## Executive summary

A total sample size of **164** is the minimum number to detect an effect size similar to that in the pilot data (i.e., Cohen's $d = 0.3$). This recommendation assumes 

* a study design of three time points, 
* random assignment of participants into two equal groups, and 
* 20% dropout on the second time point and another 20% dropout by the third time point.

If we presume a more conservative effect size of $0.2$ and a larger dropout rate of 30% the second and third time points, the minimum recommended total sample size is **486**.

The remainder of this report details how I came to these conclusions. For full transparency, I will supplement prose with figures, tables, and the statistical code used used for all computations. By default, the code is hidden is this document. However, if you are interested in the code, you should be able to make it appear by selecting "Show All Code" in the dropdown menu from the "Code" button on the upper-right corner.

## Cohen's $d$

In this report, Cohen's $d$ is meant to indicate a standardized mean difference. The $d = 0.3$ from above is based on the `some_file.docx` file you shared with me last week. In Table 1, you provided the following summary information for the intervention group:

```{r, warning = F, message = F}
library(tidyverse)

tibble(summary = c("mean", "sd"),
       baseline = c(1.29, 1.13),
       followup = c(0.95, 1.09)) %>% 
  knitr::kable()
```

With that information, we can compute a within-subject's $d$ by hand. With this formula, we will be using the average of the two standard deviations in the denominator.

```{r}
d <- (1.29 - .95) / ((1.13 + 1.09) / 2)
d
```

However, 0.306 is just a point estimate. We can express the uncertainty in that point estimate with 95% confidence intervals.

```{r}
ci <-
  MBESS::ci.smd(smd = d,
                n.1 = 50, 
                n.2 = 26)

ci %>% 
  data.frame() %>% 
  glimpse()
```

In this output, `smd` refers to "standardized mean difference," what what we have been referring to as Cohen's $d$. The output indicates the effect size for the experimental group from the pilot study was $d$ of 0.31 [-0.17, .78]. The data look promising for a small/moderate effect. But those confidence intervals swing from small negative to large. 

For reference, here are the 50% intervals.

```{r}
MBESS::ci.smd(smd = d,
              n.1 = 50, 
              n.2 = 26,
              conf.level = .5) %>% 
  data.frame() %>% 
  glimpse()
```

The 50% CIs range from 0.14 to 0.47.


