---
title: Example power analysis report
author: A. Solomon Kurz
date: '2021-07-02'
slug: ''
categories: []
tags:
  - multilevel
  - power
  - powerlmm
  - R
  - tidyverse
  - tutorial
subtitle: ''
summary: ''
authors: []
lastmod: '2021-07-02T11:15:08-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: /Users/solomonkurz/Dropbox/blogdown/content/post/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown/content/post/apa.csl  
link-citations: yes

disable_codefolding: false
codefolding_show: hide
codefolding_nobutton: false
---

## Context

In one of my recent Twitter posts, I got pissy and complained about a vague power-analysis statement I saw while reviewing a manuscript submitted to a scientific journal.

```{r echo = FALSE}
blogdown::shortcode('tweet', '1409626961161371648')
```

It wasn't my best moment and I ended up apologizing for my tone.

```{r echo = FALSE}
blogdown::shortcode('tweet', '1409634560485199876')
```

However, the broader issue remains. If you plan to analyze your data with anything more complicated than a $t$-test, the power analysis phase gets tricky. The manuscript I was complaining about used a complicated multilevel model as its primary analysis. I'm willing to bet that most applied researchers (including the authors of that manuscript) have never done a power analysis for a multilevel model and probably have never seen what one might look like, either. The purpose of this post is to give a real-world example of just such an analysis.

Over the past couple years, I've done a few multilevel power analyses as part of my day job. In this post, I will reproduce one of them. For the sake of confidentiality, some of the original content will be omitted or slightly altered. But the overall workflow will be about 90% faithful to the original report I submitted to my boss. To understand this report, you should know:

* my boss has some experience fitting multilevel models, but they're not a stats jock;
* we had pilot data from two different sources, each with its strengths and weaknesses; and
* this document was meant for internal purposes only, though I believe some of its contents did make it into other materials.

At the end, I'll wrap this post up with a few comments. Here's the report:

## Executive summary

A total sample size of **164** is the minimum number to detect an effect size similar to that in the pilot data (i.e., Cohen's $d = 0.3$). This recommendation assumes 

* a study design of three time points, 
* random assignment of participants into two equal groups, and 
* 20% dropout on the second time point and another 20% dropout by the third time point.

If we presume a more conservative effect size of $0.2$ and a larger dropout rate of 30% the second and third time points, the minimum recommended total sample size is **486**.

The remainder of this report details how I came to these conclusions. For full transparency, I will supplement prose with figures, tables, and the statistical code used used for all computations. By default, the code is hidden is this document. However, if you are interested in the code, you should be able to make it appear by selecting "Show All Code" in the dropdown menu from the "Code" button on the upper-right corner.

## Cohen's $d$

In this report, Cohen's $d$ is meant to indicate a standardized mean difference. The $d = 0.3$ from above is based on the `some_file.docx` file you shared with me last week. In Table 1, you provided the following summary information for the intervention group:

```{r, warning = F, message = F}
library(tidyverse)

tibble(summary = c("mean", "sd"),
       baseline = c(1.29, 1.13),
       followup = c(0.95, 1.09)) %>% 
  knitr::kable()
```

With that information, we can compute a within-subject's $d$ by hand. With this formula, we will be using the average of the two standard deviations in the denominator.

```{r}
d <- (1.29 - .95) / ((1.13 + 1.09) / 2)
d
```

However, 0.306 is just a point estimate. We can express the uncertainty in that point estimate with 95% confidence intervals.

```{r}
ci <-
  MBESS::ci.smd(smd = d,
                n.1 = 50, 
                n.2 = 26)

ci %>% 
  data.frame() %>% 
  glimpse()
```

In this output, `smd` refers to "standardized mean difference," what what we have been referring to as Cohen's $d$. The output indicates the effect size for the experimental group from the pilot study was $d$ of 0.31 [-0.17, .78]. The data look promising for a small/moderate effect. But those confidence intervals swing from small negative to large. 

For reference, here are the 50% intervals.

```{r}
MBESS::ci.smd(smd = d,
              n.1 = 50, 
              n.2 = 26,
              conf.level = .5) %>% 
  data.frame() %>% 
  glimpse()
```

The 50% CIs range from 0.14 to 0.47.

### Power analyses can be tailor made.

Whenever possible, it is preferable to tailor a power analysis to the statistical models researchers plan to use to analyze the data they intend to collect. Based on your previous analyses, I suspect you intend to fit a series of hierarchical models. I would have done the same thing with those data and I further recommend you analyze the data you intend to collect within a hierarchical growth model paradigm. With that in mind, the power analyses in the report are all based on the following model:

\begin{align*}
y_{ij} & = \beta_{0i} + \beta_{1i} \text{time}_{ij} + \epsilon_{ij} \\
\beta_{0i} & = \gamma_{00} + \gamma_{01} \text{treatment}_i +  u_{0i} \\
\beta_{1i} & = \gamma_{10} + \gamma_{11} \text{treatment}_i +  u_{1i}, 
\end{align*}

where $y$ is the dependent variable of interest, which varies across $i$ participants and $j$ measurement occasions. The model is linear with an intercept $\beta_{0i}$ and slope $\beta_{1i}$. As indicated by the $i$ subscripts, both intercepts and slopes vary across participants with grand means $\gamma_{00}$ and $\gamma_{10}$, respectively, and participant-specific deviations around those means $u_{0i}$ and $u_{1i}$, respectively. There is a focal between-participant predictor in the model, $\text{treatment}_i$, which is coded 0 = *control* 1 = *treatment*. Rearranging the the formulas into the composite form will make it clear this is an interaction model:

\begin{align*}
y_{ij} & = \gamma_{00} + \gamma_{01} \text{treatment}_i \\
       & \;\;\; + \gamma_{10} \text{time}_{ij} + \gamma_{11} \text{treatment}_i \times \text{time}_{ij} \\
       & \;\;\; + u_{0i} +  u_{1i} \text{time}_{ij} + \epsilon_{ij},
\end{align*}

where the parameter of primary interest for the study is $\gamma_{11} \text{treatment}_i \times \text{time}_{ij}$, the difference between the two $\text{treatment}$ conditions in their change in $y$ over $\text{time}$. As such, the focus of the power analyses reported above are on the power to reject the null hypothesis the $\text{treatment}$ conditions do not differ in their change over $\text{time}$,

$$H_0: \gamma_{11} = 0.$$

To finish out the equations, this approach makes the typical assumptions the within-participant residual term, $\epsilon_{ij}$, is normally distributed around zero, 

$$\epsilon_{ij} \sim \operatorname{Normal} (0, \sigma_\epsilon^2),$$

and the between-participant variances $u_{0i}$ and $u_{1i}$ have a multivariate normal distribution with a mean vector of zeros,

$$\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} \sim \operatorname{Normal} \Bigg ( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \sigma_0^2 & \sigma_{01} \\ \sigma_{01} & \sigma_1^2 \end{bmatrix} \Bigg ).$$

Following convention, the within-participant residuals $\epsilon_{ij}$ are orthogonal to the between-participant variances $u_{0i}$ and $u_{1i}$.

For simplicity, another assumption of this model that the control condition will remain constant over time.

