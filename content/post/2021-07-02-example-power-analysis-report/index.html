---
title: Example power analysis report
author: A. Solomon Kurz
date: '2021-07-02'
slug: ''
categories: []
tags:
  - multilevel
  - power
  - powerlmm
  - R
  - tidyverse
  - tutorial
subtitle: ''
summary: ''
authors: []
lastmod: '2021-07-02T11:15:08-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: /Users/solomonkurz/Dropbox/blogdown/content/post/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown/content/post/apa.csl  
link-citations: yes

disable_codefolding: false
codefolding_show: hide
codefolding_nobutton: false
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="context" class="section level2">
<h2>Context</h2>
<p>In one of my recent Twitter posts, I got pissy and complained about a vague power-analysis statement I saw while reviewing a manuscript submitted to a scientific journal.</p>
<p>{{% tweet "1409626961161371648" %}}</p>
<p>It wasn’t my best moment and I ended up apologizing for my tone.</p>
<p>{{% tweet "1409634560485199876" %}}</p>
<p>However, the broader issue remains. If you plan to analyze your data with anything more complicated than a <span class="math inline">\(t\)</span>-test, the power analysis phase gets tricky. The manuscript I was complaining about used a complicated multilevel model as its primary analysis. I’m willing to bet that most applied researchers (including the authors of that manuscript) have never done a power analysis for a multilevel model and probably have never seen what one might look like, either. The purpose of this post is to give a real-world example of just such an analysis.</p>
<p>Over the past couple years, I’ve done a few multilevel power analyses as part of my day job. In this post, I will reproduce one of them. For the sake of confidentiality, some of the original content will be omitted or slightly altered. But the overall workflow will be about 90% faithful to the original report I submitted to my boss. To understand this report, you should know:</p>
<ul>
<li>my boss has some experience fitting multilevel models, but they’re not a stats jock;</li>
<li>we had pilot data from two different sources, each with its strengths and weaknesses; and</li>
<li>this document was meant for internal purposes only, though I believe some of its contents did make it into other materials.</li>
</ul>
<p>At the end, I’ll wrap this post up with a few comments. Here’s the report:</p>
</div>
<div id="executive-summary" class="section level2">
<h2>Executive summary</h2>
<p>A total sample size of <strong>164</strong> is the minimum number to detect an effect size similar to that in the pilot data (i.e., Cohen’s <span class="math inline">\(d = 0.3\)</span>). This recommendation assumes</p>
<ul>
<li>a study design of three time points,</li>
<li>random assignment of participants into two equal groups, and</li>
<li>20% dropout on the second time point and another 20% dropout by the third time point.</li>
</ul>
<p>If we presume a more conservative effect size of <span class="math inline">\(0.2\)</span> and a larger dropout rate of 30% the second and third time points, the minimum recommended total sample size is <strong>486</strong>.</p>
<p>The remainder of this report details how I came to these conclusions. For full transparency, I will supplement prose with figures, tables, and the statistical code used used for all computations. By default, the code is hidden is this document. However, if you are interested in the code, you should be able to make it appear by selecting “Show All Code” in the dropdown menu from the “Code” button on the upper-right corner.</p>
</div>
<div id="cohens-d" class="section level2">
<h2>Cohen’s <span class="math inline">\(d\)</span></h2>
<p>In this report, Cohen’s <span class="math inline">\(d\)</span> is meant to indicate a standardized mean difference. The <span class="math inline">\(d = 0.3\)</span> from above is based on the <code>some_file.docx</code> file you shared with me last week. In Table 1, you provided the following summary information for the intervention group:</p>
<pre class="r"><code>library(tidyverse)

tibble(summary = c(&quot;mean&quot;, &quot;sd&quot;),
       baseline = c(1.29, 1.13),
       followup = c(0.95, 1.09)) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">summary</th>
<th align="right">baseline</th>
<th align="right">followup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mean</td>
<td align="right">1.29</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">sd</td>
<td align="right">1.13</td>
<td align="right">1.09</td>
</tr>
</tbody>
</table>
<p>With that information, we can compute a within-subject’s <span class="math inline">\(d\)</span> by hand. With this formula, we will be using the average of the two standard deviations in the denominator.</p>
<pre class="r"><code>d &lt;- (1.29 - .95) / ((1.13 + 1.09) / 2)
d</code></pre>
<pre><code>## [1] 0.3063063</code></pre>
<p>However, 0.306 is just a point estimate. We can express the uncertainty in that point estimate with 95% confidence intervals.</p>
<pre class="r"><code>ci &lt;-
  MBESS::ci.smd(smd = d,
                n.1 = 50, 
                n.2 = 26)

ci %&gt;% 
  data.frame() %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 1
## Columns: 3
## $ Lower.Conf.Limit.smd &lt;dbl&gt; -0.1711662
## $ smd                  &lt;dbl&gt; 0.3063063
## $ Upper.Conf.Limit.smd &lt;dbl&gt; 0.7817338</code></pre>
<p>In this output, <code>smd</code> refers to “standardized mean difference,” what what we have been referring to as Cohen’s <span class="math inline">\(d\)</span>. The output indicates the effect size for the experimental group from the pilot study was <span class="math inline">\(d\)</span> of 0.31 [-0.17, .78]. The data look promising for a small/moderate effect. But those confidence intervals swing from small negative to large.</p>
<p>For reference, here are the 50% intervals.</p>
<pre class="r"><code>MBESS::ci.smd(smd = d,
              n.1 = 50, 
              n.2 = 26,
              conf.level = .5) %&gt;% 
  data.frame() %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 1
## Columns: 3
## $ Lower.Conf.Limit.smd &lt;dbl&gt; 0.1413087
## $ smd                  &lt;dbl&gt; 0.3063063
## $ Upper.Conf.Limit.smd &lt;dbl&gt; 0.4692338</code></pre>
<p>The 50% CIs range from 0.14 to 0.47.</p>
<div id="power-analyses-can-be-tailor-made." class="section level3">
<h3>Power analyses can be tailor made.</h3>
<p>Whenever possible, it is preferable to tailor a power analysis to the statistical models researchers plan to use to analyze the data they intend to collect. Based on your previous analyses, I suspect you intend to fit a series of hierarchical models. I would have done the same thing with those data and I further recommend you analyze the data you intend to collect within a hierarchical growth model paradigm. With that in mind, the power analyses in the report are all based on the following model:</p>
<p><span class="math display">\[\begin{align*}
y_{ij} &amp; = \beta_{0i} + \beta_{1i} \text{time}_{ij} + \epsilon_{ij} \\
\beta_{0i} &amp; = \gamma_{00} + \gamma_{01} \text{treatment}_i +  u_{0i} \\
\beta_{1i} &amp; = \gamma_{10} + \gamma_{11} \text{treatment}_i +  u_{1i}, 
\end{align*}\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the dependent variable of interest, which varies across <span class="math inline">\(i\)</span> participants and <span class="math inline">\(j\)</span> measurement occasions. The model is linear with an intercept <span class="math inline">\(\beta_{0i}\)</span> and slope <span class="math inline">\(\beta_{1i}\)</span>. As indicated by the <span class="math inline">\(i\)</span> subscripts, both intercepts and slopes vary across participants with grand means <span class="math inline">\(\gamma_{00}\)</span> and <span class="math inline">\(\gamma_{10}\)</span>, respectively, and participant-specific deviations around those means <span class="math inline">\(u_{0i}\)</span> and <span class="math inline">\(u_{1i}\)</span>, respectively. There is a focal between-participant predictor in the model, <span class="math inline">\(\text{treatment}_i\)</span>, which is coded 0 = <em>control</em> 1 = <em>treatment</em>. Rearranging the the formulas into the composite form will make it clear this is an interaction model:</p>
<p><span class="math display">\[\begin{align*}
y_{ij} &amp; = \gamma_{00} + \gamma_{01} \text{treatment}_i \\
       &amp; \;\;\; + \gamma_{10} \text{time}_{ij} + \gamma_{11} \text{treatment}_i \times \text{time}_{ij} \\
       &amp; \;\;\; + u_{0i} +  u_{1i} \text{time}_{ij} + \epsilon_{ij},
\end{align*}\]</span></p>
<p>where the parameter of primary interest for the study is <span class="math inline">\(\gamma_{11} \text{treatment}_i \times \text{time}_{ij}\)</span>, the difference between the two <span class="math inline">\(\text{treatment}\)</span> conditions in their change in <span class="math inline">\(y\)</span> over <span class="math inline">\(\text{time}\)</span>. As such, the focus of the power analyses reported above are on the power to reject the null hypothesis the <span class="math inline">\(\text{treatment}\)</span> conditions do not differ in their change over <span class="math inline">\(\text{time}\)</span>,</p>
<p><span class="math display">\[H_0: \gamma_{11} = 0.\]</span></p>
<p>To finish out the equations, this approach makes the typical assumptions the within-participant residual term, <span class="math inline">\(\epsilon_{ij}\)</span>, is normally distributed around zero,</p>
<p><span class="math display">\[\epsilon_{ij} \sim \operatorname{Normal} (0, \sigma_\epsilon^2),\]</span></p>
<p>and the between-participant variances <span class="math inline">\(u_{0i}\)</span> and <span class="math inline">\(u_{1i}\)</span> have a multivariate normal distribution with a mean vector of zeros,</p>
<p><span class="math display">\[\begin{bmatrix} u_{0i} \\ u_{1i} \end{bmatrix} \sim \operatorname{Normal} \Bigg ( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \sigma_0^2 &amp; \sigma_{01} \\ \sigma_{01} &amp; \sigma_1^2 \end{bmatrix} \Bigg ).\]</span></p>
<p>Following convention, the within-participant residuals <span class="math inline">\(\epsilon_{ij}\)</span> are orthogonal to the between-participant variances <span class="math inline">\(u_{0i}\)</span> and <span class="math inline">\(u_{1i}\)</span>.</p>
<p>For simplicity, another assumption of this model that the control condition will remain constant over time.</p>
</div>
<div id="main-results-power-curves." class="section level3">
<h3>Main results: Power curves.</h3>
<p>I computed a series of power curves to examine the necessary sample size given different assumptions. Due to the uncertainty in the effect size from the pilot data, <span class="math inline">\(d = 0.31 [-0.17, .78]\)</span>, varied the effect size from 0.1 to 0.3. I also examined different levels of missing data via dropout. These followed four patterns of dropout and were extensions of the missing data pattern described in the <code>some_other_file.docx</code> file. They were:</p>
<pre class="r"><code>tibble(`dropout rate` = str_c(c(0, 10, 20, 30), &quot;%&quot;),
       baseline = &quot;100%&quot;,
       `1st followup` = str_c(c(100, 90, 80, 70), &quot;%&quot;),
       `2nd followup` = str_c(c(100, 80, 60, 40), &quot;%&quot;)) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">dropout rate</th>
<th align="left">baseline</th>
<th align="left">1st followup</th>
<th align="left">2nd followup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0%</td>
<td align="left">100%</td>
<td align="left">100%</td>
<td align="left">100%</td>
</tr>
<tr class="even">
<td align="left">10%</td>
<td align="left">100%</td>
<td align="left">90%</td>
<td align="left">80%</td>
</tr>
<tr class="odd">
<td align="left">20%</td>
<td align="left">100%</td>
<td align="left">80%</td>
<td align="left">60%</td>
</tr>
<tr class="even">
<td align="left">30%</td>
<td align="left">100%</td>
<td align="left">70%</td>
<td align="left">40%</td>
</tr>
</tbody>
</table>
<p>The row with the 20% dropout rate, for example, corresponds directly to the dropout rate entertained in the <code>some_other_file.docx</code> file.</p>
<p>The power simulations of this kind required two more bits of information. The first was that we specify an expected intraclass correlation coefficient (ICC). I used ICC = .9, which is the ICC value you reported in your previous work (p. 41).</p>
<p>The second value needed is the ratio of <span class="math inline">\(u_{1i}/ \epsilon_{ij}\)</span>, sometimes called the “variance ratio.” I was not able to determine that value from the <code>some_file.docx</code> or the <code>some_other_file.docx</code>. However, I was able to compute one based on data from a different project on participants from a similar population. The data are from several hundred participants in a longitudinal survey study. The data do not include your primary variable of interest. Instead, I took the <span class="math inline">\(u_{1i}/ \epsilon_{ij}\)</span> from recent hierarchical analyses of two related measures. These left me with two values: 0.018 on the low end and 0.281 on the high end. Thus, I performed the power curves using both.</p>
<p>Here is the code for the simulations:</p>
<pre class="r"><code>library(powerlmm)

t &lt;- 3
n &lt;- 100

# variance ratio 0.018
icc0.9_vr_0.018_d0.1 &lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.018,
                  effect_size = cohend(-0.2, 
                                       standardizer = &quot;pretest_SD&quot;),
                 dropout = dropout_manual(0, 0.1, 0.2)) %&gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &quot;pretest_SD&quot;))

icc0.9_vr_0.018_d0.2 &lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.018,
                  effect_size = cohend(-0.2, 
                                       standardizer = &quot;pretest_SD&quot;),
                 dropout = dropout_manual(0, 0.2, 0.4)) %&gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &quot;pretest_SD&quot;))

icc0.9_vr_0.018_d0.3 &lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.018,
                  effect_size = cohend(-0.2, 
                                       standardizer = &quot;pretest_SD&quot;),
                 dropout = dropout_manual(0, 0.3, 0.6)) %&gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &quot;pretest_SD&quot;))

# variance ratio 0.281
icc0.9_vr_0.281_d0.1 &lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.281,
                  effect_size = cohend(-0.2, 
                                       standardizer = &quot;pretest_SD&quot;),
                 dropout = dropout_manual(0, 0.1, 0.2)) %&gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &quot;pretest_SD&quot;))

icc0.9_vr_0.281_d0.2 &lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.281,
                  effect_size = cohend(-0.2, 
                                       standardizer = &quot;pretest_SD&quot;),
                 dropout = dropout_manual(0, 0.2, 0.4)) %&gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &quot;pretest_SD&quot;))

icc0.9_vr_0.281_d0.3 &lt;-
  study_parameters(n1 = t,
                 n2 = n,
                 icc_pre_subject = 0.9,
                  var_ratio = 0.281,
                  effect_size = cohend(-0.2, 
                                       standardizer = &quot;pretest_SD&quot;),
                 dropout = dropout_manual(0, 0.3, 0.6)) %&gt;% 
  get_power_table(n2 = 25:500,
                  effect_size = cohend(c(.1, .15, .2, .25, .3), 
                                       standardizer = &quot;pretest_SD&quot;))</code></pre>
<p>Here are the power curve plots, beginning with the plot for the smaller variance ratio of 0.018.</p>
<pre class="r"><code>bind_rows(
  icc0.9_vr_0.018_d0.1 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.018_d0.2 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.018_d0.3
) %&gt;% 
  mutate(missing = c(rep(str_c(c(10, 20, 30, 00), &quot;% missing per time point after baseline&quot;), each = n() / 4))) %&gt;% 
  
  mutate(d = factor(effect_size,
                    levels = c(&quot;0.1&quot;, &quot;0.15&quot;, &quot;0.2&quot;, &quot;0.25&quot;, &quot;0.3&quot;),
                    labels = c(&quot;.10&quot;, &quot;.15&quot;, &quot;.20&quot;, &quot;.25&quot;, &quot;.30&quot;))) %&gt;% 
  mutate(d = fct_rev(d)) %&gt;% 
  
  ggplot(aes(x = tot_n, y = power, color = d)) +
  geom_vline(xintercept = 500, color = &quot;white&quot;, size = 1) +
  geom_hline(yintercept = .8, color = &quot;white&quot;, size = 1) +
  geom_line(size = 1.5) +
  scale_color_viridis_d(expression(paste(&quot;Cohen&#39;s &quot;, italic(d))),
                        option = &quot;A&quot;, end = .67, direction = -1) +
  scale_x_continuous(expression(paste(italic(N), &quot; (i.e., the total sample size)&quot;)), 
                     breaks = seq(from = 0, to = 1000, by = 100), limits = c(0, 1000)) +
  scale_y_continuous(breaks = c(0, .2, .4, .6, .8, 1), limits = c(0, 1)) +
  ggtitle(&quot;Power curves based on a variance ratio of 0.018&quot;) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~missing)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="816" /></p>
<p>Here is the power curve plot for the larger variance ratio of 0.281.</p>
<pre class="r"><code>bind_rows(
  icc0.9_vr_0.281_d0.1 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.281_d0.2 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.281_d0.3
) %&gt;% 
  mutate(missing = c(rep(str_c(c(10, 20, 30, 00), &quot;% missing per time point after baseline&quot;), each = n() / 4))) %&gt;% 
  
  mutate(d = factor(effect_size,
                    levels = c(&quot;0.1&quot;, &quot;0.15&quot;, &quot;0.2&quot;, &quot;0.25&quot;, &quot;0.3&quot;),
                    labels = c(&quot;.10&quot;, &quot;.15&quot;, &quot;.20&quot;, &quot;.25&quot;, &quot;.30&quot;))) %&gt;% 
  mutate(d = fct_rev(d)) %&gt;% 
  
  ggplot(aes(x = tot_n, y = power, color = d)) +
  geom_vline(xintercept = 500, color = &quot;white&quot;, size = 1) +
  geom_hline(yintercept = .8, color = &quot;white&quot;, size = 1) +
  geom_line(size = 1.5) +
  scale_color_viridis_d(expression(paste(&quot;Cohen&#39;s &quot;, italic(d))),
                        option = &quot;A&quot;, end = .67, direction = -1) +
  scale_x_continuous(expression(paste(italic(N), &quot; (i.e., the total sample size)&quot;)), 
                     breaks = seq(from = 0, to = 1000, by = 100), limits = c(0, 1000)) +
  scale_y_continuous(breaks = c(0, .2, .4, .6, .8, 1), limits = c(0, 1)) +
  ggtitle(&quot;Power curves based on a variance ratio of 0.281&quot;) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~missing)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="816" /></p>
<p>The upshot of the variance ratio issue is that a higher variance ratio led to lower power. To be on the safe side, <em>I recommend leaning on the more conservative power curve estimates from the simulations based on the larger variance ratio</em>, <strong>0.281</strong>.</p>
<p>A more succinct way to summarize the information in the power curves in with two tables. Here is the minimum total sample size required to reach a power of .8 based on the smaller evidence ratio of 0.018 and the various combinations of Cohen’s <span class="math inline">\(d\)</span> and dropout:</p>
<pre class="r"><code>bind_rows(
  icc0.9_vr_0.018_d0.1 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.018_d0.2 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.018_d0.3
) %&gt;% 
  mutate(missing = c(rep(c(10, 20, 30, 00), each = n() / 4))) %&gt;% 

  filter(power &gt; .8) %&gt;% 
  group_by(missing, effect_size) %&gt;% 
  top_n(-1, power) %&gt;% 
  select(-n2, -power, -dropout) %&gt;% 
  ungroup() %&gt;% 
  mutate(`Cohen&#39;s d` = effect_size) %&gt;% 
  
  ggplot(aes(x = `Cohen&#39;s d`, y = missing)) +
  geom_tile(aes(fill = tot_n),
            show.legend = F) +
  geom_text(aes(label = tot_n, color = tot_n &lt; 700),
            show.legend = F) +
  scale_fill_viridis_c(option = &quot;B&quot;, begin = .1, end = .70 ,limits = c(0, 1000)) +
  scale_color_manual(values = c(&quot;black&quot;, &quot;white&quot;)) +
  labs(title = expression(paste(&quot;Total &quot;, italic(N), &quot; required for .8 power, based on a variance ratio of 0.018&quot;)),
       subtitle = expression(paste(&quot;The power simulations only considered up to &quot;, italic(N), &quot; = 1,000.&quot;)),
       x = expression(paste(&quot;Cohen&#39;s &quot;, italic(d))),
       y = &quot;% missing\nper follow-up&quot;) +
  theme(axis.ticks = element_blank(),
        panel.grid = element_blank())</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="624" /></p>
<p>Here is an alternative version of that plot, this time based on the more conservative variance ratio of 0.281.</p>
<pre class="r"><code>bind_rows(
  icc0.9_vr_0.281_d0.1 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.281_d0.2 %&gt;% filter(dropout == &quot;with missing&quot;),
  icc0.9_vr_0.281_d0.3
) %&gt;% 
  mutate(missing = c(rep(c(10, 20, 30, 00), each = n() / 4))) %&gt;% 

  filter(power &gt; .8) %&gt;% 
  group_by(missing, effect_size) %&gt;% 
  top_n(-1, power) %&gt;% 
  select(-n2, -power, -dropout) %&gt;% 
  ungroup() %&gt;% 
  mutate(`Cohen&#39;s d` = effect_size) %&gt;% 
  
  ggplot(aes(x = `Cohen&#39;s d`, y = missing)) +
  geom_tile(aes(fill = tot_n),
            show.legend = F) +
  geom_text(aes(label = tot_n, color = tot_n &lt; 700),
            show.legend = F) +
  scale_fill_viridis_c(option = &quot;B&quot;, begin = .1, end = .70 ,limits = c(0, 1000)) +
  scale_color_manual(values = c(&quot;black&quot;, &quot;white&quot;)) +
  labs(title = expression(paste(&quot;Total &quot;, italic(N), &quot; required for .8 power, based on variance ratio of 0.281&quot;)),
       subtitle = expression(paste(&quot;The power simulations only considered up to &quot;, italic(N), &quot; = 1,000.&quot;)),
       x = expression(paste(&quot;Cohen&#39;s &quot;, italic(d))),
       y = &quot;% missing\nper follow-up&quot;) +
  theme(axis.ticks = element_blank(),
        panel.grid = element_blank())</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="624" /></p>
<p>Again, I recommend playing it safe and relying on the power estimates based on the larger variance ratio of 0.281. Those power curves indicate that even with rather large dropout (i.e., 30% at the second time point and another 30% at the final time point), <span class="math inline">\(N = 486\)</span> is sufficient to detect a small effect size (i.e., <span class="math inline">\(d = 0.2\)</span>) at the conventional .8 power threshold. Note that because we cut off the power simulations at <span class="math inline">\(N = 1{,}000\)</span>, we never reached .8 power in the conditions where <span class="math inline">\(d = 0.1\)</span> and there was missingness at or greater than 0% dropout at each follow-up time point.</p>
<p>To clarify, <span class="math inline">\(N\)</span> in each cell is the total sample size presuming both the control and experimental conditions have equal numbers in each. Thus, <span class="math inline">\(n_\text{control} = n_\text{experimental} = N/2\)</span>.</p>
</div>
</div>
