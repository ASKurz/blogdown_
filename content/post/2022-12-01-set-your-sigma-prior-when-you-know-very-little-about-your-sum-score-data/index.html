---
title: Set your sigma prior when you know very little about your sum-score data
author: A. Solomon Kurz
date: '2022-12-01'
slug: ''
categories: []
tags:
  - Bayesian
  - beta-binomial
  - brms
  - Likert
  - Popoviciu's inequality
  - prior
  - R
  - sigma
  - sum score
  - tidyverse
  - tutorial
  - uniform
subtitle: ''
summary: ''
authors: []
lastmod: '2022-12-01T11:21:32-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: /Users/solomonkurz/Dropbox/blogdown/content/post/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown/content/post/apa.csl  
link-citations: yes
---



<div id="what" class="section level2">
<h2>What?</h2>
<p>We psychologists analyze a lot of sum-score data. Even though it’s not the best, we usually use the Gaussian likelihood<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> for these analyses. I was recently in a situation were I wanted to model sum-scores from a new questionnaire and there was no good prior research on the distribution of the sum-scores. Like, there wasn’t a single published paper reporting the sample statistics. Crazy, I know… Anyway, I spent some time trying to reason through how I would set a justifiable prior for my <span class="math inline">\(\sigma\)</span> parameter, and in this blog post we’ll cover how I came up with a solution.</p>
<div id="i-make-assumptions." class="section level3">
<h3>I make assumptions.</h3>
<ul>
<li><p>For this post, I’m presuming you are familiar with Bayesian regression. For an introduction, I recommend either edition of McElreath’s text <span class="citation">(<a href="#ref-mcelreathStatisticalRethinkingBayesian2020" role="doc-biblioref">2020</a>, <a href="#ref-mcelreathStatisticalRethinkingBayesian2015" role="doc-biblioref">2015</a>)</span>; Kruschke’s <span class="citation">(<a href="#ref-kruschkeDoingBayesianData2015" role="doc-biblioref">2015</a>)</span> text; or Gelman, Hill, and Vehtari’s <span class="citation">(<a href="#ref-gelmanRegressionOtherStories2020" role="doc-biblioref">2020</a>)</span> text.</p></li>
<li><p>All code is in <strong>R</strong>. Data wrangling and plotting were done with help from the <strong>tidyverse</strong> <span class="citation">(<a href="#ref-wickhamWelcomeTidyverse2019" role="doc-biblioref">Wickham et al., 2019</a>; <a href="#ref-R-tidyverse" role="doc-biblioref">Wickham, 2022</a>)</span>, <strong>tidybayes</strong> <span class="citation">(<a href="#ref-R-tidybayes" role="doc-biblioref">Kay, 2022</a>)</span>, and <strong>patchwork</strong> <span class="citation">(<a href="#ref-R-patchwork" role="doc-biblioref">Pedersen, 2022</a>)</span>. The Bayesian model will be fit with <a href="https://github.com/paul-buerkner/brms"><strong>brms</strong></a> <span class="citation">(<a href="#ref-burknerBrmsPackageBayesian2017" role="doc-biblioref">Bürkner, 2017</a>, <a href="#ref-burknerAdvancedBayesianMultilevel2018" role="doc-biblioref">2018</a>, <a href="#ref-R-brms" role="doc-biblioref">2022</a>)</span>. We will also use the <a href="https://CRAN.R-project.org/package=MetBrewer"><strong>MetBrewer</strong> package</a> <span class="citation">(<a href="#ref-R-MetBrewer" role="doc-biblioref">Mills, 2022</a>)</span> to select the color palette for our figures.</p></li>
</ul>
<p>Here we load the packages and adjust the global plotting theme.</p>
<pre class="r"><code># load
library(tidyverse)
library(tidybayes)
library(patchwork)
library(brms)
library(MetBrewer)

# save a color vector
h &lt;- met.brewer(colorblind_palettes[8])  # &quot;Hiroshige&quot;

# adjust the global plotting theme
theme_set(
  theme_linedraw(base_size = 13) +
    theme(panel.background = element_rect(fill = alpha(h[5], 0.33)),
          panel.border = element_rect(linewidth = 0.33),
          panel.grid = element_blank(),
          plot.background = element_rect(fill = alpha(h[5], 0.2)),
          strip.background = element_rect(fill = alpha(h[5], 0.67), linewidth = 0),
          strip.text = element_text(color = &quot;black&quot;))
)</code></pre>
<p>The color palette in this post is inspired by Utagawa Hiroshige’s <em>Sailing Boats Returning to Yabase, Lake Biwa</em>, about which you can learn more from <a href="https://www.metmuseum.org/art/collection/search/36534">The Met’s website</a>.</p>
</div>
</div>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>I came up with three broad strategies for settling in on a prior for <span class="math inline">\(\sigma\)</span> when in a state of relative ignorance:</p>
<ul>
<li>Compute the maximum value with Popoviciu’s inequality.</li>
<li>Compute an approximate standard deviation presuming a uniform distribution.</li>
<li>Compute standard deviations based on plausible beta-binomial distributions.</li>
</ul>
<p>We’ll explore each in turn.</p>
<div id="popovicius-inequality." class="section level3">
<h3>Popoviciu’s inequality.</h3>
<p>If you have continuous data with clear lower and upper boundaries, you can use Popoviciu’s <span class="citation">(<a href="#ref-popoviciu1935equations" role="doc-biblioref">1935</a>)</span> <a href="https://en.wikipedia.org/wiki/Popoviciu%27s_inequality_on_variances">inequality on variances</a> to compute the maximum variance of the distribution. If we let <span class="math inline">\(U\)</span> be the upper bound and <span class="math inline">\(L\)</span> be the lower bound, Popoviciu’s inequality states</p>
<p><span class="math display">\[
\sigma^2 \leq \frac{1}{4} (U - L)^2.
\]</span></p>
<p>For example, let’s say you’re using a questionnaire like the PHQ-9 <span class="citation">(<a href="#ref-spitzer1999validation" role="doc-biblioref">Spitzer et al., 1999</a>)</span>, which is a widely-used depression questionnaire<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> composed of 9 Likert-type items ranging from 0 to 3. The PHQ-9 sum score has a possible range of 0 to 27, with higher scores indicating deeper depression. Using Popoviciu’s inequality, we can compute the maximum variance for the PHQ-9 sum score like so.</p>
<pre class="r"><code>u &lt;- 27  # upper limit
l &lt;- 0   # lower limit

(u - l)^2 / 4  # maximum variance</code></pre>
<pre><code>## [1] 182.25</code></pre>
<p>Since variances are the same as standard deviations squared, we can take the square root of that value to compute the maximum standard deviation possible for the PHQ-9.</p>
<pre class="r"><code>sqrt((u - l)^2 / 4)  # maximum standard deviation</code></pre>
<pre><code>## [1] 13.5</code></pre>
<p>We might check this out with a little simulation. If we had a data set composed of <span class="math inline">\(N = 10{,}000\)</span> PHQ-9 sum-score values which were evenly split between all zero’s and all 27’s, here’s what the sample standard deviation would be.</p>
<pre class="r"><code>n &lt;- 10000

tibble(y = rep(c(0, 27), each = n / 2)) %&gt;% 
  summarise(var = var(y),
            sd = sd(y))</code></pre>
<pre><code>## # A tibble: 1 × 2
##     var    sd
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  182.  13.5</code></pre>
<p>The displayed output values are rounded, but the sample statistics are just a hair above the population values from Popoviciu’s inequality<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
<p>When in a state of complete ignorance, you know your <span class="math inline">\(\sigma\)</span> prior should be centered somewhere between zero and the value returned by Popoviciu’s inequality. Though I wouldn’t recommend it, one way to use this information would be to set a uniform prior ranging from zero to the upper-limit based on Popoviciu’s inequality.</p>
</div>
<div id="take-the-standard-deviation-from-the-uniform-distribution." class="section level3">
<h3>Take the standard deviation from the uniform distribution.</h3>
<p>Though Popoviciu’s inequality gives us a sense of the range of possible values, it doesn’t do a great job helping us decide which parts of that range are more likely than not. As a next stab, you might assume the sum-score data were uniformly distributed. This would be something of a null hypothesis stating: <em>All values are equally likely.</em></p>
<p>If you have a uniform distribution with a lower limit of <span class="math inline">\(L\)</span> and an upper limit of <span class="math inline">\(U\)</span>, you can compute the variance with the equation</p>
<p><span class="math display">\[
\sigma^2 = \frac{1}{12} (U - L)^2,
\]</span></p>
<p>with the standard deviation being the square root of that value. Continuing on with our PHQ-9 data as an example, here are the variance and standard deviation presuming they were uniformly distributed.</p>
<pre class="r"><code>u &lt;- 27  # upper limit
l &lt;- 0   # lower limit

(u - l)^2 / 12  # variance</code></pre>
<pre><code>## [1] 60.75</code></pre>
<pre class="r"><code>sqrt((u - l)^2 / 12)  # sd</code></pre>
<pre><code>## [1] 7.794229</code></pre>
<p>Again, let’s run a little simulation to see how this shakes out.</p>
<pre class="r"><code>n_option &lt;- 1000

tibble(y = rep(0:27, each = n_option)) %&gt;% 
  summarise(var = var(y),
            sd = sd(y))</code></pre>
<pre><code>## # A tibble: 1 × 2
##     var    sd
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  65.3  8.08</code></pre>
<p>In this case, the sample statistics from our simulation are a little bit off from the statistics we’d expect based on the formula from the uniform distribution. Here, though, keep in mind that the uniform distribution presumes truly continuous data, not integer values like those you’d get from a sum score. Thus the uniform distribution is only an approximation, not a true analogue. However, this isn’t a bad approach if you just want to get a sense of where to start.</p>
<p>Thus Popoviciu’s inequality gave us a possible range for <span class="math inline">\(\sigma\)</span>, and the uniform distribution gave us an approximate value given the null assumption all sum-scores are equally plausible.</p>
<p>But do you really want to use the null assumption that all sum-scores are equally plausible? If not, we’ll want to work with a new distribution.</p>
</div>
<div id="sum-scores-might-look-beta-binomial." class="section level3">
<h3>Sum scores might look beta-binomial.</h3>
<p>So even though we tend to use the Gaussian likelihood to model sum-score data for <em>reasons</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>, it’s not the only distribution in town. If you search around in the backwater of applied statistics, you’ll find papers on the beta-binomial likelihood for sum-score-type data. For example, Lord discussed the beta-binomial distribution for psychological test data in <span class="citation">(<a href="#ref-lord1962estimating" role="doc-biblioref">1962</a>)</span>, Wilcox later reviewed it for the same purpose in <span class="citation">(<a href="#ref-wilcox1981review" role="doc-biblioref">1981</a>)</span>, and Carlin and Rubin kept the conversation rolling in <span class="citation">(<a href="#ref-carlin1991summarizing" role="doc-biblioref">1991</a>)</span>. Though these authors didn’t focus on Likert-type data per se, Greenleaf used the beta-binomial to describe a distribution of sums of Likert-type items in <span class="citation">(<a href="#ref-greenleaf1992measuring" role="doc-biblioref">1992</a>)</span>. I’d like to extrapolate further and suggest the beta-binomial distribution can help us understand sum-score data in general, and the standard deviations of sum-score data in particular.</p>
<p>Given some count variable <span class="math inline">\(y\)</span> with a clear upper bound <span class="math inline">\(n\)</span>, we can describe the beta-binomial likelihood as</p>
<p><span class="math display">\[
f(y | n, \alpha, \beta) = \binom{n}{y} \frac{\operatorname B (y + \alpha, n - y + \beta)}{\operatorname B(\alpha, \beta)},
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are the two parameters from the canonical version of the beta likelihood, <span class="math inline">\(\operatorname B(\cdot)\)</span> is the <a href="https://en.wikipedia.org/wiki/Beta_function">beta function</a>, and <span class="math inline">\(\tbinom{n}{y}\)</span> is a shorthand factorial notation. The <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameters aren’t the most intuitive, but for our purposes the main thing to understand is when the mean of the distribution is in the middle of the range, <span class="math inline">\(\alpha = \beta\)</span>. I propose that when you are in a state of ignorance about a new sum-score, your go-to assumption should be the mean is in the middle. <em>Why?</em> Well, you have to start somewhere and the middle is at least as good a place as any other. But also keep in mind that most questionnaire developers have had at least <em>some</em><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> training in psychometrics, and they know that it’s generally a good idea to develop measures with approximately bell-shaped and symmetric distributions. Thus even of you’ve never seen prior examples of a given sum score, a good initial guess is the sample mean is somewhere in the middle of the possible range.</p>
<p>In the case of our PHQ-9 example where the lower limit is zero and the upper limit is 27, we could presume the data were uniformly distributed with <span class="math inline">\(\operatorname{BetaBinomial}(27, 1, 1)\)</span>. We can simulate data of that kind with the <code>rbetabinom.ab()</code> function from the <strong>VGAM</strong> package <span class="citation">(<a href="#ref-R-VGAM" role="doc-biblioref">Yee, 2022</a>)</span>. Here’s a sample of <span class="math inline">\(N = 10{,}000\)</span>.</p>
<pre class="r"><code>set.seed(9)

d &lt;- tibble(phq = VGAM::rbetabinom.ab(n = 1e4, size = 27, shape1 = 1, shape2 = 1))

# what?
head(d)</code></pre>
<pre><code>## # A tibble: 6 × 1
##     phq
##   &lt;dbl&gt;
## 1    20
## 2    23
## 3    16
## 4    17
## 5    10
## 6    22</code></pre>
<p>Here’s what our simulated <code>phq</code> values look like in a plot.</p>
<pre class="r"><code>d %&gt;% 
  ggplot(aes(x = phq)) +
  geom_bar(fill = h[7]) +
  scale_x_continuous(breaks = 0:3 * 9) +
  ggtitle(&quot;10,000 draws from BetaBinomial(27, 1, 1)&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="456" /></p>
<p>Here are the sample statistics.</p>
<pre class="r"><code>d %&gt;% 
  summarise(m = mean(phq),
            v = var(phq),
            s = sd(phq))</code></pre>
<pre><code>## # A tibble: 1 × 3
##       m     v     s
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  13.4  64.6  8.04</code></pre>
<p>Now we’ve shifted from the exponential distribution to the beta-binomial, we might point out the variance of the beta-binomial is defined as</p>
<p><span class="math display">\[
\sigma^2 = \frac{n \alpha \beta (\alpha + \beta + n)}{(\alpha + \beta)^2 (\alpha + \beta + 1)},
\]</span></p>
<p>and the standard deviation of the beta-binomial is just the square root of that value. Here’s what that looks like in the case of <span class="math inline">\(\operatorname{BetaBinomial}(27, 1, 1)\)</span>.</p>
<pre class="r"><code>n &lt;- 27
a &lt;- 1
b &lt;- a

# population variance
((n * a * b) * (a + b + n)) / ((a + b)^2 * (a + b + 1))</code></pre>
<pre><code>## [1] 65.25</code></pre>
<pre class="r"><code># population sd
sqrt(((n * a * b) * (a + b + n)) / ((a + b)^2 * (a + b + 1)))</code></pre>
<pre><code>## [1] 8.077747</code></pre>
<p>So if we’re willing to describe the sum scores of our Likert-type items as integers, the flat beta-binomial may give us a better first stab at the standard deviation than the exponential distribution approach, above.</p>
<p>But we can go further. The beta-binomial <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameters can take on any positive real values. For simplicity, consider what the beta-binomial distribution looks like if we hold <span class="math inline">\(n = 27\)</span> and serially increase both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> from 1 to 9.</p>
<pre class="r"><code>tibble(a = 1:9) %&gt;% 
  mutate(b = a) %&gt;% 
  expand(nesting(a, b),
         phq = 0:27) %&gt;% 
  mutate(d = VGAM::dbetabinom.ab(x = phq, size = 27, shape1 = a, shape2 = b)) %&gt;% 
  mutate(label = str_c(&quot;BetaBinomial(27, &quot;, a, &quot;, &quot;, b, &quot;)&quot;)) %&gt;% 
  
  ggplot(aes(x = phq, y = d)) +
  geom_col(fill = h[7]) +
  scale_x_continuous(breaks = 0:3 * 9) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~ label)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="624" /></p>
<p>As the <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameters increase, the distribution becomes more concentrated around the mean. I don’t know about you, but my experience has been that most roughly-symmetric sum-score data tend to look about like the distributions in <span class="math inline">\(\operatorname{BetaBinomial}(27, 3, 3)\)</span> through <span class="math inline">\(\operatorname{BetaBinomial}(27, 7, 7)\)</span>.</p>
<p>Using the formula from above, here are the expected standard deviations from each of the 9 distributions.</p>
<pre class="r"><code>tibble(n = 27,
       a = 1:9) %&gt;% 
  mutate(b = a) %&gt;% 
  mutate(variance = (n * a * b * (a + b + n)) / ((a + b)^2 * (a + b + 1))) %&gt;% 
  mutate(standard_deviation = sqrt(variance))</code></pre>
<pre><code>## # A tibble: 9 × 5
##       n     a     b variance standard_deviation
##   &lt;dbl&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;              &lt;dbl&gt;
## 1    27     1     1     65.2               8.08
## 2    27     2     2     41.8               6.47
## 3    27     3     3     31.8               5.64
## 4    27     4     4     26.2               5.12
## 5    27     5     5     22.7               4.76
## 6    27     6     6     20.2               4.5 
## 7    27     7     7     18.4               4.30
## 8    27     8     8     17.1               4.13
## 9    27     9     9     16.0               4.00</code></pre>
<p>If you’re in a total state of ignorance, it’s hard to know which one’s best. But I assert that in the right ballpark, here.</p>
</div>
<div id="recap." class="section level3">
<h3>Recap.</h3>
<ul>
<li>Popoviciu’s inequality gave us a possible range for <span class="math inline">\(\sigma\)</span>, which was between 0 and 13.5 for our PHQ-9 example.</li>
<li>When we used the uniform distribution as a rough null default, the expected standard deviation was about 7.8.</li>
<li>When we used plausible candidate beta-binomial distributions, we got <span class="math inline">\(\sigma\)</span> values ranging from 6 to 4, depending on which seems like the best default.</li>
</ul>
<p>Thus, if I were to try modeling actual PHQ-9 data for the first time, and I had no access to all the previous studies using the PHQ-9, I’d set something like</p>
<p><span class="math display">\[
\sigma \sim \operatorname{Exponential}(1 / 5.641049),
\]</span></p>
<p>based of the expected standard deviation from <span class="math inline">\(\operatorname{BetaBinomial}(27, 3, 3)\)</span>. Here’s what that prior looks like:</p>
<pre class="r"><code># 1 / 5.641049 is about 0.177272
prior(exponential(0.177272)) %&gt;% 
  parse_dist() %&gt;% 
  ggplot(aes(y = 0, dist = .dist, args = .args)) +
  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95),
               fill = h[6]) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = &quot;Exponential(1 / 5.641049)&quot;,
       x = NULL)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="456" /></p>
<p>Such a distribution puts the inner 50% of the prior mass between 1.6 and 7.8, which is centered where I want it. The exponential prior, by default, holds the standard deviation constant with the mean. If you wanted a stronger prior with a smaller standard deviation, you could switch to a 2-parameter distribution like the gamma or lognormal.</p>
<pre class="r"><code># to compute the 50% and 95% ranges, execute this code
qexp(p = c(0.25, 0.75), rate = 1 / 5.641049)    # 50% interval
qexp(p = c(0.025, 0.975), rate = 1 / 5.641049)  # 95% interval</code></pre>
<p>If you’re not familiar with using the exponential distribution for <span class="math inline">\(\sigma\)</span> priors, check out the second edition of McElreath’s <a href="https://xcelab.net/rm/statistical-rethinking/">textbook</a>. In chapter 4, he made the case the exponential distribution can be a good option when you have a sense of where the mean of the prior should be, but you’re unsure how large you want its spread. I’ve found it pretty handy over the past couple years.</p>
</div>
</div>
<div id="applied-example" class="section level2">
<h2>Applied example</h2>
<p>Okay, that’s enough theory. Let’s see how this works in practice.</p>
<div id="we-need-data." class="section level3">
<h3>We need data.</h3>
<p>In response to a <a href="https://twitter.com/SolomonKurz/status/1597718777797705730">call on twitter</a>, neuroscience graduate student <a href="https://szorowi1.github.io/">Sam Zorowitz</a> kindly shared some de-identified PHQ-9 data collected by the good folks in the Niv Lab. I’ve saved the file on GitHub. Here we’ll load the data and save them as <code>phq9</code>.</p>
<pre class="r"><code># load
phq9 &lt;- read_csv(&quot;data/phq9.csv&quot;)

# what is this?
glimpse(phq9)</code></pre>
<pre><code>## Rows: 4,500
## Columns: 4
## $ subject  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3…
## $ item     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3…
## $ order    &lt;dbl&gt; 1, 6, 10, 3, 9, 8, 4, 5, 7, 3, 2, 8, 10, 4, 5, 1, 9, 6, 3, 5,…
## $ response &lt;dbl&gt; 1, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 2, 3, 3…</code></pre>
<p>These data are a subset of <span class="math inline">\(n = 500\)</span> cases, which Zorowitz randomly drew from a larger pool of <span class="math inline">\(N \approx 10{,}000\)</span> cases. In a personal communication (11-30-2022), Zorowitz reported the full parent data set will eventually be made public. In the meantime, this <span class="math inline">\(n = 500\)</span> subset is more than enough for our purposes. In this file:</p>
<ul>
<li><code>subject</code>: anonymized participant ID</li>
<li><code>item</code>: the unique item on the PHQ-9</li>
<li><code>order</code>: the position of the item on the webpage as presented to that participant</li>
<li><code>response</code>: the participant’s response</li>
</ul>
<p>As we’d expect, the <code>response</code> values range from <code>0</code> to <code>3</code>.</p>
<pre class="r"><code>phq9 %&gt;% 
  count(response)</code></pre>
<pre><code>## # A tibble: 4 × 2
##   response     n
##      &lt;dbl&gt; &lt;int&gt;
## 1        0  1939
## 2        1  1392
## 3        2   726
## 4        3   443</code></pre>
<p>Happily, there are no missing values to contend with. Since our focus is on the sum scores, rather than the items, we’ll need to wrangle the data a bit.</p>
<pre class="r"><code>phq9 &lt;- phq9 %&gt;% 
  group_by(subject) %&gt;% 
  summarise(phq = sum(response)) %&gt;% 
  ungroup()

# what?
glimpse(phq9)</code></pre>
<pre><code>## Rows: 500
## Columns: 2
## $ subject &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…
## $ phq     &lt;dbl&gt; 6, 5, 16, 3, 8, 9, 13, 13, 7, 4, 2, 4, 3, 13, 3, 7, 8, 0, 10, …</code></pre>
<p>Now the sum scores are in the <code>phq</code> column. Though we don’t technically need to, it will help me make my point if we subset the data further to <span class="math inline">\(n = 200\)</span>. We can take a random subset with the <code>slice_sample()</code> function.</p>
<pre class="r"><code>set.seed(9)

phq9 &lt;- phq9 %&gt;% 
  slice_sample(n = 200)

# confirm the sample size
nrow(phq9)</code></pre>
<pre><code>## [1] 200</code></pre>
<p>Here’s what the data look like.</p>
<pre class="r"><code>phq9 %&gt;% 
  ggplot(aes(x = phq)) +
  geom_bar(fill = h[3]) +
  scale_x_continuous(breaks = 0:3 * 9, limits = c(-0.5, 27.5))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="456" /></p>
<p>Based on my experience, this is a pretty typical distribution for the PHQ-9.</p>
</div>
<div id="we-need-a-model." class="section level3">
<h3>We need a model.</h3>
<p>For the sake of the blog, let’s keep things simple and fit an intercept-only model of the <code>phq</code> scores. In the real world, I actually know a lot about the PHQ-9 and there’s a mountain of prior studies reporting sample statistics for its sum score. But if this weren’t the case and we were trying to fit a model in a state of near-complete ignorance, I think a pretty great place to start would be</p>
<p><span class="math display">\[
\begin{align*}
\text{PHQ-9}_i &amp; \sim \operatorname{Normal}(\mu, \sigma) \\
\mu &amp; = \beta_0 \\
\beta_0 &amp; \sim \operatorname{Normal}(12, 5.641049) \\
\sigma &amp; \sim \operatorname{Exponential}(1 / 5.641049),
\end{align*}
\]</span></p>
<p>where the top line indicates the PHQ-9 scores vary across the <span class="math inline">\(i\)</span> cases, and the scores are described as following the normal distribution. The second line of the equation indicates this is an intercept-only model, with <span class="math inline">\(\beta_0\)</span> defining the intercept. Skipping the third line for a moment, the fourth line shows we’ve chosen an exponential prior for <span class="math inline">\(\sigma\)</span> for which the rate is <span class="math inline">\(1 / 5.641049\)</span> and thus the mean is <span class="math inline">\(5.641049\)</span><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. Where did that value come from, again? After exploring some nice candidate beta-binomial distributions, we said <span class="math inline">\(\operatorname{BetaBinomial}(27, 3, 3)\)</span> might be a good default option, and the expected standard deviation for that distribution is 5.641049. Here those are, again:</p>
<pre class="r"><code># define
n &lt;- 27
a &lt;- 3
b &lt;- a

tibble(phq = 0:27,
       a   = a,
       b   = b) %&gt;% 
  mutate(d = VGAM::dbetabinom.ab(x = phq, size = n, shape1 = a, shape2 = b)) %&gt;%

  # plot
  ggplot(aes(x = phq, y = d)) +
  geom_col(fill = h[7]) +
  scale_x_continuous(breaks = 0:3 * 9) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle(&quot;BetaBinomial(27, 3, 3)&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="456" /></p>
<pre class="r"><code># compute the population sd
sqrt(((n * a * b) * (a + b + n)) / ((a + b)^2 * (a + b + 1)))</code></pre>
<pre><code>## [1] 5.641049</code></pre>
<p>Now back to the third line in the model equation. As per usual, we have used a Gaussian prior for <span class="math inline">\(\beta_0\)</span>. We’ve centered the prior on the middle of the possible range of the sum score, which is 13.5. Since we’ve decided 5.641049 is a pretty good value for <span class="math inline">\(\sigma\)</span>, it’s not a bad idea to use that value for the scale of our <span class="math inline">\(\beta_0\)</span> prior. Such a prior would have an inner 95% range of about 2.4 to 22.6, which covers most of the possible sum-score range.</p>
<pre class="r"><code>qnorm(p = c(0.025, 0.975), mean = 13.5, sd = 5.641049)</code></pre>
<pre><code>## [1]  2.443747 24.556253</code></pre>
<p>Otherwise put, the full range of the PHQ-9 sum score is 13.5 plus or minus 2.4 standard deviations, as defined by our <span class="math inline">\(\sigma\)</span> prior.</p>
<pre class="r"><code>13.5 / 5.641049</code></pre>
<pre><code>## [1] 2.393172</code></pre>
</div>
<div id="f-around-and-find-out.7" class="section level3">
<h3>F* around and find out.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></h3>
<p>Here’s how to fit the model with <code>brm()</code>.</p>
<pre class="r"><code>fit1 &lt;- brm(
  data = phq9,
  family = gaussian,
  phq ~ 1,
  prior(normal(13.5, 5.641049), class = Intercept) +
    prior(exponential(1 / 5.641049), class = sigma),
  cores = 4, seed = 1
)</code></pre>
<p>Check the model summary.</p>
<pre class="r"><code>summary(fit1)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: phq ~ 1 
##    Data: phq9 (Number of observations: 200) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     9.04      0.41     8.24     9.83 1.00     3009     2271
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     5.81      0.29     5.27     6.43 1.00     3468     2246
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>At a glance, everything looks fine. It might be more instructive to look the results in a couple plots where we compare the priors with the posteriors.</p>
<pre class="r"><code># intercept
p1 &lt;- as_draws_df(fit1) %&gt;% 
  ggplot(aes(x = b_Intercept, y = after_stat(density))) +
  geom_area(data = tibble(b_Intercept = seq(-5, 30, by = 0.01),
                          density = dnorm(b_Intercept, 13.5, 5.641049)),
            aes(y = density),
            fill = h[10]) +
  geom_histogram(fill = alpha(h[2], 2/3), boundary = 0, binwidth = 0.2) +
  scale_x_continuous(expression(beta[0]), breaks = 0:3 * 9) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 27))

# sigma
p2 &lt;- as_draws_df(fit1) %&gt;% 
  ggplot(aes(x = sigma, y = after_stat(density))) +
  geom_area(data = tibble(sigma = seq(0, 20, by = 0.01),
                          density = dexp(sigma, 1 / 5.641049)),
            aes(y = density),
            fill = h[10]) +
  geom_histogram(fill = alpha(h[2], 2/3), boundary = 0, binwidth = 0.1) +
  geom_vline(xintercept = 13.5, linetype = 2, color = h[1]) +
  annotate(geom = &quot;text&quot;,
           x = 13.3, y = 1.37,
           label = &quot;Upper limit via\nPopoviciu&#39;s inequality&quot;,
           hjust = 1, size = 3) +
  scale_x_continuous(expression(sigma), breaks = 0:3 * 5) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 15))

# combine
(p1 | p2) &amp;
  plot_annotation(title = &quot;Careful priors can express our substantive ignorance, but still\naccount for our methodological knowledge.&quot;,
                  subtitle = &quot;The prior densities lurk in the background. The posteriors are the\nsemitransparent orange histograms in the foreground.&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="624" /></p>
<p>It looks like both priors did their jobs well.</p>
</div>
</div>
<div id="objections" class="section level2">
<h2>Objections</h2>
<div id="but-the-data-were-skewed" class="section level3">
<h3>But the data were skewed!</h3>
<p>Some readers might not like how we centered the prior for <span class="math inline">\(\beta_0\)</span> or how we used a symmetric beta-binomial distribution to determine the prior for <span class="math inline">\(\sigma\)</span> given how the actual PHQ-9 data were skewed to the right, and given how PHQ-9 data are very often skewed to the right in other samples. Keep in mind that our focus is a strategy which will work when we don’t have information like that going in. When you’re completely ignorant of how your sum-score data tend to look, I recommend assuming they’ll be roughly symmetric.</p>
<p>However, it’s possible some of y’all work in fields where your sum-scores are routinely skewed in one direction or the other. If so, great! That means you have useful domain knowledge upon which you can base your priors. One of the nice things about the beta-binomial distribution is it can take on a variety of non-symmetric shapes. For example, take a look at the distribution of <span class="math inline">\(\operatorname{BetaBinomial}(27, 1.5, 3.25)\)</span>.</p>
<pre class="r"><code># define
n &lt;- 27
a &lt;- 1.5
b &lt;- 3.25

tibble(phq = 0:27,
       a   = a,
       b   = b) %&gt;% 
  mutate(d = VGAM::dbetabinom.ab(x = phq, size = n, shape1 = a, shape2 = b)) %&gt;% 

  # plot
  ggplot(aes(x = phq, y = d)) +
  geom_col(fill = h[7]) +
  scale_x_continuous(breaks = 0:3 * 9) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle(&quot;BetaBinomial(27, 1.5, 3.25)&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-25-1.png" width="456" /></p>
<p>This looks a lot like the empirical distribution in our <code>phq9</code> data, doesn’t it? We could then use this skewed version of the beta-binomial to compute the expected standard deviation.</p>
<pre class="r"><code># compute the population sd
sqrt(((n * a * b) * (a + b + n)) / ((a + b)^2 * (a + b + 1)))</code></pre>
<pre><code>## [1] 5.675623</code></pre>
<p>So if you were working with questionnaire data in a domain where you expected the sum-score to have this kind of skewed distribution, you might assume <span class="math inline">\(\sigma \sim \operatorname{Exponential}(1 / 5.675623)\)</span>. It’s worth spending some time plotting beta-binomial distributions with different values for its <span class="math inline">\(n\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\beta\)</span> parameters. Scientists working in different research domains might prefer different default versions of the distribution.</p>
</div>
<div id="just-standardize." class="section level3">
<h3>Just standardize.</h3>
<p>If you wanted to take a different approach, you could just standardize the sum-score data before the analysis and then fit the model to the standardized values. This is a fine practice and it would greatly simplify how you choose your priors. I do this on occasion, myself. However, sometimes I want to fit the model to the un-transformed data and I’d like a workflow that can work on those occasions. This is such a workflow.</p>
<p>Plus, like, do you really care so little about your data and your research questions that you don’t want to put a little thought into what you expect the data will look like? If not, maybe science isn’t a good fit for you.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
</div>
<div id="you-sure-about-that-gauss" class="section level3">
<h3>You sure about that Gauss?</h3>
<p>Some readers might wonder whether it’s such a great idea to model sum-score data with the conventional Gaussian likelihood. The Gauss expects truly continuous data, and it doesn’t have a natural way to handle data with well-defined minimum and maximum values. The Gaussian likelihood might not be the best when dealing with markedly-skewed data, either.</p>
<p>One alternative would be using skew-normal or skew-Student likelihood. For details, see the <span class="citation">(<a href="#ref-martin2017outgrowing" role="doc-biblioref">2017</a>)</span> preprint by Martin and Williams. This approach, however, will not fully solve the problem with the lower and upper boundaries. A more sophisticated approach would be to model the item-level data with a multilevel-ordinal IRT-type model, such as discussed by <span class="citation">Bürkner (<a href="#ref-burknerBayesianItemResponse2020" role="doc-biblioref">2020</a>)</span>. This approach is excellent for respecting the ordinal nature of the questionnaire items and for expressing the data-generating process, but it comes at the cost of a complex, highly-parameterized model which may be difficult to fit.</p>
<p>If you’ve been following along, another option is to model the sum scores as beta-binomial. This approach would account for skew, upper and lower boundaries, and for the integer values. The beta-binomial would not faithfully reproduce the item-level data-generating process, but you might think of it as a pragmatic and simpler alternative to the rigorous item-level multilevel-ordinal IRT approach. This approach is possible with <code>brm()</code> by setting <code>family = beta_binomial</code>. Though it’s beyond the scope of this blog post, I’ve been chipping away at a blog series which will explore the beta-binomial distribution for sum-score data. As a preview, here’s how to fit such a model to these data.</p>
<pre class="r"><code>fit2 &lt;- brm(
  data = phq9,
  family = beta_binomial,
  phq | trials(27) ~ 1,
  prior(normal(0, 1), class = Intercept) +
    prior(gamma(2.25, 0.375), class = phi),
  cores = 4, seed = 1
)</code></pre>
<p>Whichever method you use in your research, I hope this post gave you some helpful ideas. Happy modeling, friends!</p>
</div>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.2.0 (2022-04-22)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur/Monterey 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] MetBrewer_0.2.0 brms_2.18.0     Rcpp_1.0.9      patchwork_1.1.2
##  [5] tidybayes_3.0.2 forcats_0.5.1   stringr_1.4.1   dplyr_1.0.10   
##  [9] purrr_0.3.4     readr_2.1.2     tidyr_1.2.1     tibble_3.1.8   
## [13] ggplot2_3.4.0   tidyverse_1.3.2
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.4.1         backports_1.4.1      VGAM_1.1-7          
##   [4] plyr_1.8.7           igraph_1.3.4         splines_4.2.0       
##   [7] svUnit_1.0.6         crosstalk_1.2.0      TH.data_1.1-1       
##  [10] rstantools_2.2.0     inline_0.3.19        digest_0.6.30       
##  [13] htmltools_0.5.3      fansi_1.0.3          magrittr_2.0.3      
##  [16] checkmate_2.1.0      googlesheets4_1.0.1  tzdb_0.3.0          
##  [19] modelr_0.1.8         RcppParallel_5.1.5   matrixStats_0.62.0  
##  [22] vroom_1.5.7          sandwich_3.0-2       xts_0.12.1          
##  [25] prettyunits_1.1.1    colorspace_2.0-3     rvest_1.0.2         
##  [28] ggdist_3.2.0         haven_2.5.1          xfun_0.35           
##  [31] callr_3.7.3          crayon_1.5.2         jsonlite_1.8.3      
##  [34] lme4_1.1-31          survival_3.4-0       zoo_1.8-10          
##  [37] glue_1.6.2           gtable_0.3.1         gargle_1.2.0        
##  [40] emmeans_1.8.0        distributional_0.3.1 pkgbuild_1.3.1      
##  [43] rstan_2.21.7         abind_1.4-5          scales_1.2.1        
##  [46] mvtnorm_1.1-3        DBI_1.1.3            miniUI_0.1.1.1      
##  [49] xtable_1.8-4         bit_4.0.4            stats4_4.2.0        
##  [52] StanHeaders_2.21.0-7 DT_0.24              htmlwidgets_1.5.4   
##  [55] httr_1.4.4           threejs_0.3.3        arrayhelpers_1.1-0  
##  [58] posterior_1.3.1      ellipsis_0.3.2       pkgconfig_2.0.3     
##  [61] loo_2.5.1            farver_2.1.1         sass_0.4.2          
##  [64] dbplyr_2.2.1         utf8_1.2.2           labeling_0.4.2      
##  [67] tidyselect_1.1.2     rlang_1.0.6          reshape2_1.4.4      
##  [70] later_1.3.0          munsell_0.5.0        cellranger_1.1.0    
##  [73] tools_4.2.0          cachem_1.0.6         cli_3.4.1           
##  [76] generics_0.1.3       broom_1.0.1          ggridges_0.5.3      
##  [79] evaluate_0.18        fastmap_1.1.0        yaml_2.3.5          
##  [82] bit64_4.0.5          processx_3.8.0       knitr_1.40          
##  [85] fs_1.5.2             nlme_3.1-159         projpred_2.2.1      
##  [88] mime_0.12            xml2_1.3.3           compiler_4.2.0      
##  [91] bayesplot_1.9.0      shinythemes_1.2.0    rstudioapi_0.13     
##  [94] gamm4_0.2-6          reprex_2.0.2         bslib_0.4.0         
##  [97] stringi_1.7.8        highr_0.9            ps_1.7.2            
## [100] blogdown_1.15        Brobdingnag_1.2-8    lattice_0.20-45     
## [103] Matrix_1.4-1         nloptr_2.0.3         markdown_1.1        
## [106] shinyjs_2.1.0        tensorA_0.36.2       vctrs_0.5.0         
## [109] pillar_1.8.1         lifecycle_1.0.3      jquerylib_0.1.4     
## [112] bridgesampling_1.1-2 estimability_1.4.1   httpuv_1.6.5        
## [115] R6_2.5.1             bookdown_0.28        promises_1.2.0.1    
## [118] gridExtra_2.3        codetools_0.2-18     boot_1.3-28         
## [121] MASS_7.3-58.1        colourpicker_1.1.1   gtools_3.9.3        
## [124] assertthat_0.2.1     withr_2.5.0          shinystan_2.6.0     
## [127] multcomp_1.4-20      mgcv_1.8-40          parallel_4.2.0      
## [130] hms_1.1.1            grid_4.2.0           minqa_1.2.5         
## [133] coda_0.19-4          rmarkdown_2.16       googledrive_2.0.0   
## [136] numDeriv_2016.8-1.1  shiny_1.7.2          lubridate_1.8.0     
## [139] base64enc_0.1-3      dygraphs_1.1.1.6</code></pre>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-burknerBayesianItemResponse2020" class="csl-entry">
Bürkner, P.-C. (2020). <em>Bayesian item response modeling in <span>R</span> with brms and <span>Stan</span></em>. <a href="http://arxiv.org/abs/1905.09501">http://arxiv.org/abs/1905.09501</a>
</div>
<div id="ref-burknerBrmsPackageBayesian2017" class="csl-entry">
Bürkner, P.-C. (2017). <span class="nocase">brms</span>: <span>An R</span> package for <span>Bayesian</span> multilevel models using <span>Stan</span>. <em>Journal of Statistical Software</em>, <em>80</em>(1), 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>
</div>
<div id="ref-burknerAdvancedBayesianMultilevel2018" class="csl-entry">
Bürkner, P.-C. (2018). Advanced <span>Bayesian</span> multilevel modeling with the <span>R</span> package brms. <em>The R Journal</em>, <em>10</em>(1), 395–411. <a href="https://doi.org/10.32614/RJ-2018-017">https://doi.org/10.32614/RJ-2018-017</a>
</div>
<div id="ref-R-brms" class="csl-entry">
Bürkner, P.-C. (2022). <em><span class="nocase">brms</span>: <span>Bayesian</span> regression models using ’<span>Stan</span>’</em>. <a href="https://CRAN.R-project.org/package=brms">https://CRAN.R-project.org/package=brms</a>
</div>
<div id="ref-carlin1991summarizing" class="csl-entry">
Carlin, J. B., &amp; Rubin, D. B. (1991). Summarizing multiple-choice tests using three informative statistics. <em>Psychological Bulletin</em>, <em>110</em>(2), 338–349. <a href="https://doi.org/10.1037/0033-2909.110.2.338">https://doi.org/10.1037/0033-2909.110.2.338</a>
</div>
<div id="ref-gelmanRegressionOtherStories2020" class="csl-entry">
Gelman, A., Hill, J., &amp; Vehtari, A. (2020). <em>Regression and other stories</em>. <span>Cambridge University Press</span>. <a href="https://doi.org/10.1017/9781139161879">https://doi.org/10.1017/9781139161879</a>
</div>
<div id="ref-greenleaf1992measuring" class="csl-entry">
Greenleaf, E. A. (1992). Measuring extreme response style. <em>Public Opinion Quarterly</em>, <em>56</em>(3), 328–351. <a href="https://doi.org/10.1086/269326">https://doi.org/10.1086/269326</a>
</div>
<div id="ref-R-tidybayes" class="csl-entry">
Kay, M. (2022). <em><span class="nocase">tidybayes</span>: <span>Tidy</span> data and ’geoms’ for <span>Bayesian</span> models</em>. <a href="https://CRAN.R-project.org/package=tidybayes">https://CRAN.R-project.org/package=tidybayes</a>
</div>
<div id="ref-kruschkeDoingBayesianData2015" class="csl-entry">
Kruschke, J. K. (2015). <em>Doing <span>Bayesian</span> data analysis: <span>A</span> tutorial with <span>R</span>, <span>JAGS</span>, and <span>Stan</span></em>. <span>Academic Press</span>. <a href="https://sites.google.com/site/doingbayesiandataanalysis/">https://sites.google.com/site/doingbayesiandataanalysis/</a>
</div>
<div id="ref-lord1962estimating" class="csl-entry">
Lord, F. M. (1962). Estimating true measurements from fallible measurements (binomial case)expansion in a series of beta distributions. <em>ETS Research Bulletin Series</em>, <em>1962</em>(2), i–26. <a href="https://doi.org/10.1002/j.2333-8504.1962.tb00301.x">https://doi.org/10.1002/j.2333-8504.1962.tb00301.x</a>
</div>
<div id="ref-martin2017outgrowing" class="csl-entry">
Martin, S. R., &amp; Williams, D. R. (2017). <em>Outgrowing the <span>Procrustean</span> bed of normality: <span>The</span> utility of <span>Bayesian</span> modeling for asymmetrical data analysis</em>. <a href="https://doi.org/10.31234/osf.io/26m49">https://doi.org/10.31234/osf.io/26m49</a>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2020" class="csl-entry">
McElreath, R. (2020). <em>Statistical rethinking: <span>A Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (Second Edition). <span>CRC Press</span>. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2015" class="csl-entry">
McElreath, R. (2015). <em>Statistical rethinking: <span>A Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em>. <span>CRC press</span>. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a>
</div>
<div id="ref-R-MetBrewer" class="csl-entry">
Mills, B. R. (2022). <em><span>MetBrewer</span>: <span>Color</span> palettes inspired by works at the <span>Metropolitan Museum</span> of <span>Art</span></em>. <a href="https://CRAN.R-project.org/package=MetBrewer">https://CRAN.R-project.org/package=MetBrewer</a>
</div>
<div id="ref-R-patchwork" class="csl-entry">
Pedersen, T. L. (2022). <em><span class="nocase">patchwork</span>: <span>The</span> composer of plots</em>. <a href="https://CRAN.R-project.org/package=patchwork">https://CRAN.R-project.org/package=patchwork</a>
</div>
<div id="ref-popoviciu1935equations" class="csl-entry">
Popoviciu, T. (1935). Sur les équations algébriques ayant toutes leurs racines réelles. <em>Mathematica Cluj</em>, <em>9</em>(129-145).
</div>
<div id="ref-spitzer1999validation" class="csl-entry">
Spitzer, R. L., Kroenke, K., Williams, J. B., Group, P. H. Q. P. C. S., Group, P. H. Q. P. C. S., et al. (1999). Validation and utility of a self-report version of <span>PRIME-MD</span>: The <span>PHQ</span> primary care study. <em>JAMA</em>, <em>282</em>(18), 1737–1744. <a href="https://doi.org/10.1001/jama.282.18.1737">https://doi.org/10.1001/jama.282.18.1737</a>
</div>
<div id="ref-R-tidyverse" class="csl-entry">
Wickham, H. (2022). <em><span class="nocase">tidyverse</span>: <span>Easily</span> install and load the ’tidyverse’</em>. <a href="https://CRAN.R-project.org/package=tidyverse">https://CRAN.R-project.org/package=tidyverse</a>
</div>
<div id="ref-wickhamWelcomeTidyverse2019" class="csl-entry">
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. <em>Journal of Open Source Software</em>, <em>4</em>(43), 1686. <a href="https://doi.org/10.21105/joss.01686">https://doi.org/10.21105/joss.01686</a>
</div>
<div id="ref-wilcox1981review" class="csl-entry">
Wilcox, R. R. (1981). A review of the beta-binomial model and its extensions. <em>Journal of Educational Statistics</em>, <em>6</em>(1), 3–32. <a href="https://doi.org/10.2307/1165046">https://doi.org/10.2307/1165046</a>
</div>
<div id="ref-R-VGAM" class="csl-entry">
Yee, T. (2022). <em><span>VGAM</span>: <span>Vector</span> generalized linear and additive models</em>. <a href="https://www.stat.auckland.ac.nz/~yee/VGAM/">https://www.stat.auckland.ac.nz/~yee/VGAM/</a>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Or if you’re thinking in terms of OLS, presume the residuals are normally distributed.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>We use the PHQ-9 all the time in the VA (i.e., the US Department of Veterans Affairs) both in clinical services and in research settings. It’s reasonably brief and it does a fair job assessing the primary diagnostic criteria for <a href="https://www.mdcalc.com/calc/10195/dsm-5-criteria-major-depressive-disorder">major depressive disorder</a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>If you increase the <code>n</code> of the sample, you’ll see the sample statistics quickly approach the values from Popoviciu’s inequality.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>There’s the whole ‘Fisher liked the normal distribution, so you should too’ bit. Plus we do need defaults and the Gaussian isn’t a bad candidate, all things considered. But yeah, <em>sigh</em><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Yes, I know; <em>some</em> is doing a <em>lot</em> of work, here.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>If you didn’t know, the <em>rate</em> of the exponential distribution is the reciprocal of its mean.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>In case you’re not in on the joke, behold: <a href="https://youtu.be/d6sbPCIEMyI" class="uri">https://youtu.be/d6sbPCIEMyI</a><a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Okay, I admit that was a little snarky. But what are we really doing here? Are we just churning out <span class="math inline">\(p\)</span>-values for pubs or does the science matter at all?<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
