---
title: Bayesian meta-analysis in brms-II
author: A. Solomon Kurz
date: '2020-10-16'
slug: ''
categories: []
tags:
  - Bayesian
  - brms
  - meta-analysis
  - R
  - Statistical Rethinking
  - tutorial
subtitle: ''
summary: ''
authors: []
lastmod: '2021-04-22T10:24:22-07:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: /Users/solomonkurz/Dropbox/blogdown/content/post/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown/content/post/apa.csl  
link-citations: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="preamble" class="section level2">
<h2>Preamble</h2>
<p>In <a href="https://bookdown.org/content/3890/missing-data-and-other-opportunities.html#summary-bonus-meta-analysis">Section 14.3</a> of my <span class="citation">(<a href="#ref-kurzStatisticalRethinkingBrms2020" role="doc-biblioref">2020a</a>)</span> translation of the first edition of McElreath’s <span class="citation">(<a href="#ref-mcelreathStatisticalRethinkingBayesian2015" role="doc-biblioref">2015</a>)</span> <em>Statistical rethinking</em>, I included a bonus section covering Bayesian meta-analysis. For my <span class="citation">(<a href="#ref-kurzStatisticalRethinkingSecondEd2020" role="doc-biblioref">2020b</a>)</span> translation of the second edition of the text <span class="citation">(<a href="#ref-mcelreathStatisticalRethinkingBayesian2020" role="doc-biblioref">McElreath, 2020</a>)</span>, I’d like to include another section on the topic, but from a different perspective. The first time around, we focused on standardized mean differences. This time, I’d like to tackle odds ratios and, while we’re at it, give a little bit of a plug for open science practices.</p>
<p>The purpose of this post is to present a rough draft of the section. I intend to tack this section onto the end of Chapter 15 (<em>Missing Data and Other Opportunities</em>), which covers measurement error. If you have any constrictive criticisms, please pass them along either in the <a href="https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues">GitHub issues for the ebook</a> or on <a href="https://twitter.com/SolomonKurz/status/1317854064839958531">Twitter</a>.</p>
<p>Here’s the rough draft:</p>
</div>
<div id="summary-bonus-bayesian-meta-analysis-with-odds-ratios" class="section level2">
<h2><del>Summary</del> Bonus: Bayesian meta-analysis with odds ratios</h2>
<pre class="r"><code># these packages and setting alterations will already have been 
# opened and made before this section
library(tidyverse)
library(brms)
library(ggdark)
library(viridis)
library(broom)
library(tidybayes)

theme_set(
  dark_theme_bw() +
    theme(legend.position = &quot;none&quot;,
          panel.grid = element_blank())
  )

# to reset the default ggplot2 theme to its default parameters,
# execute `ggplot2::theme_set(theme_gray())` and `ggdark::invert_geom_defaults()`</code></pre>
<p>If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use <code>brms::brm()</code> to fit Bayesian meta-analyses, too.</p>
<p>Before we proceed, I should acknowledge that this section is heavily influenced by <a href="https://mvuorre.github.io/#about">Matti Vourre</a>’s great blog post, <a href="https://mvuorre.github.io/post/2016/09/29/meta-analysis-is-a-special-case-of-bayesian-multilevel-modeling/"><em>Meta-analysis is a special case of Bayesian multilevel modeling</em></a>. Since neither editions of McElreath’s text directly address meta-analyses, we’ll also have to borrow a bit from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s <span class="citation">(<a href="#ref-gelman2013bayesian" role="doc-biblioref">2013</a>)</span> <a href="https://stat.columbia.edu/~gelman/book/"><em>Bayesian data analysis, Third edition</em></a>.</p>
<div id="how-do-meta-analyses-fit-into-the-picture" class="section level3">
<h3>How do meta-analyses fit into the picture?</h3>
<p>Let Gelman and colleagues introduce the topic:</p>
<blockquote>
<p>Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected <em>a priori</em> to have predictable effects favoring one study over another…. this third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…</p>
<p>The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125–126, <em>emphasis</em> in the original)</p>
</blockquote>
<p>The basic version of a Bayesian meta-analysis follows the form</p>
<p><span class="math display">\[y_j \sim \operatorname{Normal}(\theta_j, \sigma_j),\]</span></p>
<p>where <span class="math inline">\(y_j\)</span> = the point estimate for the effect size of a single study, <span class="math inline">\(j\)</span>, which is presumed to have been a draw from a Normal distribution centered on <span class="math inline">\(\theta_j\)</span>. The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. The standard error from study <span class="math inline">\(j\)</span> is specified <span class="math inline">\(\sigma_j\)</span>, which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating <span class="math inline">\(\sigma_j\)</span>, here. Those values we take directly from the original studies.</p>
<p>Building on the model, we further presume that study <span class="math inline">\(j\)</span> is itself just one draw from a population of related studies, each of which have their own effect sizes. As such, we presume <span class="math inline">\(\theta_j\)</span> itself has a distribution following the form</p>
<p><span class="math display">\[\theta_j \sim \operatorname{Normal}(\mu, \tau),\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the meta-analytic effect (i.e., the population mean) and <span class="math inline">\(\tau\)</span> is the variation around that mean, what you might also think of as <span class="math inline">\(\sigma_\tau\)</span>.</p>
</div>
<div id="we-need-some-data." class="section level3">
<h3>We need some data.</h3>
<p>Our data in this section come from the second large-scale replication project by the Many Labs team <span class="citation">(<a href="#ref-kleinManyLabsInvestigating2018" role="doc-biblioref">Klein et al., 2018</a>)</span>. Of the 28 studies replicated in the study, we will focus on the replication of the trolley experiment from <span class="citation"><a href="#ref-hauserDissociationMoralJudgments2007" role="doc-biblioref">Hauser et al.</a> (<a href="#ref-hauserDissociationMoralJudgments2007" role="doc-biblioref">2007</a>)</span>. Here’s how the study was described by Klein and colleagues:</p>
<blockquote>
<p>According to the principle of double effect, an act that harms other people is more morally permissible if the act is a foreseen side effect rather than the means to the greater good. <span class="citation"><a href="#ref-hauserDissociationMoralJudgments2007" role="doc-biblioref">Hauser et al.</a> (<a href="#ref-hauserDissociationMoralJudgments2007" role="doc-biblioref">2007</a>)</span> compared participants’ reactions to two scenarios to test whether their judgments followed this principle. In the <em>foreseen-side-effect</em> scenario, a person on an out-of-control train changed the train’s trajectory so that the train killed one person instead of five. In the <em>greater-good</em> scenario, a person pushed a fat man in front of a train, killing him, to save five people. Whereas <span class="math inline">\(89\%\)</span> of participants judged the action in the foreseen-side-effect scenario as permissible <span class="math inline">\((95 \% \; \text{CI} = [87\%, 91\%]),\)</span> only <span class="math inline">\(11\%\)</span> of participants in the greater-good scenario judged it as permissible <span class="math inline">\((95 \% \; \text{CI} = [9\%, 13\%])\)</span>. The difference between the percentages was significant<span class="math inline">\(, \chi^2(1, N = 2,646) = 1,615.96,\)</span> <span class="math inline">\(p &lt; .001,\)</span> <span class="math inline">\(w = .78,\)</span> <span class="math inline">\(d = 2.50,\)</span> <span class="math inline">\(95 \% \; \text{CI} = [2.22, 2.86]\)</span>. Thus, the results provided evidence for the principle of double effect. (p. 459, <em>emphasis</em> in the original)</p>
</blockquote>
<p>You can find supporting materials for the replication project on the Open Science Framework at <a href="https://osf.io/8cd4r/">https://osf.io/8cd4r/</a>. The relevant subset of the data for the replication of Hauser et al. come from the <code>Trolley Dilemma 1 (Hauser et al., 2007)</code> folder within the <code>OSFdata.zip</code> (<a href="https://osf.io/ag2pd/">https://osf.io/ag2pd/</a>). I’ve downloaded the file and saved it on GitHub.</p>
<p>Here we load the data and call it <code>h</code>.</p>
<pre class="r"><code>h &lt;- 
  readr::read_csv(&quot;https://raw.githubusercontent.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/master/data/Hauser_1_study_by_order_all_CLEAN_CASE.csv&quot;)

h &lt;- 
  h %&gt;% 
  mutate(y   = ifelse(variable == &quot;Yes&quot;, 1, 0),
         loc = factor(Location,
                      levels = distinct(h, Location) %&gt;% pull(Location),
                      labels = 1:59))

glimpse(h)</code></pre>
<pre><code>## Rows: 6,842
## Columns: 29
## $ uID              &lt;dbl&gt; 65, 68, 102, 126, 145, 263, 267, 298, 309, 318, 350, 356, 376, 431, 438, …
## $ variable         &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;…
## $ factor           &lt;chr&gt; &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;Si…
## $ .id              &lt;chr&gt; &quot;ML2_Slate1_Brazil__Portuguese_execution_illegal_r.csv&quot;, &quot;ML2_Slate1_Braz…
## $ source           &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, …
## $ haus1.1          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1…
## $ haus1.1t_1       &lt;dbl&gt; 39.054, 36.792, 56.493, 21.908, 25.635, 50.633, 58.661, 50.137, 51.717, 2…
## $ haus2.1          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ haus2.1t_1       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ Source.Global    &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, …
## $ Source.Primary   &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, …
## $ Source.Secondary &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, …
## $ Country          &lt;chr&gt; &quot;Brazil&quot;, &quot;Brazil&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Ca…
## $ Location         &lt;chr&gt; &quot;Social and Work Psychology Department, University of Brasilia, DF, Brazi…
## $ Language         &lt;chr&gt; &quot;Portuguese&quot;, &quot;Portuguese&quot;, &quot;Portuguese&quot;, &quot;English&quot;, &quot;English&quot;, &quot;English&quot;…
## $ Weird            &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ Execution        &lt;chr&gt; &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illega…
## $ SubjectPool      &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;,…
## $ Setting          &lt;chr&gt; &quot;In a classroom&quot;, &quot;In a classroom&quot;, &quot;In a classroom&quot;, &quot;In a lab&quot;, &quot;In a l…
## $ Tablet           &lt;chr&gt; &quot;Computers&quot;, &quot;Computers&quot;, &quot;Computers&quot;, &quot;Computers&quot;, &quot;Computers&quot;, &quot;Compute…
## $ Pencil           &lt;chr&gt; &quot;No, the whole study was on the computer (except maybe consent/debriefing…
## $ StudyOrderN      &lt;chr&gt; &quot;Hauser|Ross.Slate1|Rottenstrich|Graham|Kay|Inbar|Anderson|VanLange|Huang…
## $ IDiffOrderN      &lt;chr&gt; &quot;ID: Global self-esteem SISE|ID: Mood|ID: Subjective wellbeing|ID: Disgus…
## $ study.order      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ analysis.type    &lt;chr&gt; &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;…
## $ subset           &lt;chr&gt; &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;al…
## $ case.include     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…
## $ y                &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1…
## $ loc              &lt;fct&gt; 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3…</code></pre>
<p>The total sample size is <span class="math inline">\(N = 6,842\)</span>.</p>
<pre class="r"><code>h %&gt;% 
  distinct(uID) %&gt;% 
  count()</code></pre>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1  6842</code></pre>
<p>All cases are to be included.</p>
<pre class="r"><code>h %&gt;% 
  count(case.include)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   case.include     n
##   &lt;lgl&gt;        &lt;int&gt;
## 1 TRUE          6842</code></pre>
<p>The data were collected in 59 locations with sample sizes ranging from 34 to 325.</p>
<pre class="r"><code>h %&gt;% 
  count(Location) %&gt;% 
  arrange(desc(n))</code></pre>
<pre><code>## # A tibble: 59 x 2
##    Location                                                                                        n
##    &lt;chr&gt;                                                                                       &lt;int&gt;
##  1 University of Toronto, Scarborough                                                            325
##  2 MTurk India Workers                                                                           308
##  3 MTurk US Workers                                                                              304
##  4 University of Illinois at Urbana-Champaign, Champaign, IL                                     198
##  5 Eotvos Lorand University, in Budapest, Hungary                                                180
##  6 Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Net…   173
##  7 Department of Psychology, San Diego State University, San Diego, CA 92182                     171
##  8 Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001          166
##  9 American University of Sharjah, United Arab Emirates                                          162
## 10 University of British Columbia, Vancouver, Canada                                             147
## # … with 49 more rows</code></pre>
</div>
<div id="our-effect-size-will-be-an-odds-ratio." class="section level3">
<h3>Our effect size will be an odds ratio.</h3>
<p>Here’s how Klein and colleagues summarized their primary results:</p>
<blockquote>
<p>In the aggregate replication sample <span class="math inline">\((N = 6,842\)</span> after removing participants who responded in less than <span class="math inline">\(4\)</span> s<span class="math inline">\(), 71\%\)</span> of participants judged the action in the foreseen-side-effect scenario as permissible, but only <span class="math inline">\(17\%\)</span> of participants in the greater-good scenario judged it as permissible. The difference between the percentages was significant, <span class="math inline">\(p = 2.2 \text e^{-16},\)</span> <span class="math inline">\(\text{OR} = 11.54,\)</span> <span class="math inline">\(d = 1.35,\)</span> <span class="math inline">\(95\% \; \text{CI} = [1.28, 1.41]\)</span>. The replication results were consistent with the double-effect hypothesis, and the effect was about half the magnitude of the original <span class="math inline">\((d = 1.35,\)</span> <span class="math inline">\(95\% \; \text{CI} = [1.28, 1.41],\)</span> vs. original <span class="math inline">\(d = 2.50)\)</span>. (p. 459)</p>
</blockquote>
<p>Here is the breakdown of the outcome and primary experimental condition, which will confirm the two empirical percentages mentioned, above.</p>
<pre class="r"><code>h %&gt;% 
  count(variable, factor) %&gt;% 
  group_by(factor) %&gt;% 
  mutate(percent = 100 * n / sum(n))</code></pre>
<pre><code>## # A tibble: 4 x 4
## # Groups:   factor [2]
##   variable factor          n percent
##   &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;
## 1 No       GreaterGood  2781    82.8
## 2 No       SideEffect   1026    29.4
## 3 Yes      GreaterGood   577    17.2
## 4 Yes      SideEffect   2458    70.6</code></pre>
<p>Though the authors presented their overall effect size with a <span class="math inline">\(p\)</span>-value, an odds-ratio (OR), and a Cohen’s <span class="math inline">\(d\)</span> (i.e., a kind of standardized mean difference), we will focus on the OR. The primary data are binomial counts, which are well-handled with logistic regression. When you perform a logistic regression where a control condition is compared with some experimental condition, the difference between those conditions may be expressed as an OR. To get a sense of what that is, we’ll first practice fitting a logistic regression model with the frequentist <code>glm()</code> function. Here are the results based on the subset of data from the first location.</p>
<pre class="r"><code>glm0 &lt;- glm(y ~ factor, family = binomial(logit), data = h %&gt;% filter(loc == 1))

summary(glm0)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ factor, family = binomial(logit), data = h %&gt;% 
##     filter(loc == 1))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5227  -0.6231  -0.6231   0.8677   1.8626  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)       -1.5404     0.3673  -4.194 2.74e-05 ***
## factorSideEffect   2.3232     0.4754   4.887 1.02e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 139.47  on 101  degrees of freedom
## Residual deviance: 110.98  on 100  degrees of freedom
## AIC: 114.98
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Just like with <strong>brms</strong>, the base-<strong>R</strong> <code>glm()</code> function returns the results of a logistic regression model in the log-odds metric. The intercept is the log-odds probability of selecting <em>yes</em> in the study for participants in the <code>GreaterGood</code> condition. The ‘factorSideEffect’ parameter is the difference in log-odds probability for participants in the <code>SideEffect</code> condition. Here’s what happens when you exponentiate that coefficient.</p>
<pre class="r"><code>coef(glm0)[2] %&gt;% exp()</code></pre>
<pre><code>## factorSideEffect 
##         10.20833</code></pre>
<p>That, my friends, is an odds ratio (OR). <strong>Odds ratios are simply exponentiated logistic regression coefficients</strong>. The implication of this particular OR is that those in the <code>SideEffect</code> condition have about 10 times the odds of selecting <em>yes</em> compared to those in the <code>GreaterGood</code> condition. In the case of this subset of the data, that’s 18% yeses versus 69%, which seems like a large difference, to me.</p>
<pre class="r"><code>h %&gt;% 
  filter(loc == 1) %&gt;% 
  count(variable, factor) %&gt;% 
  group_by(factor) %&gt;% 
  mutate(percent = 100 * n / sum(n)) %&gt;% 
  filter(variable == &quot;Yes&quot;)</code></pre>
<pre><code>## # A tibble: 2 x 4
## # Groups:   factor [2]
##   variable factor          n percent
##   &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;
## 1 Yes      GreaterGood     9    17.6
## 2 Yes      SideEffect     35    68.6</code></pre>
</div>
<div id="log-odds-odds-ratios-and-modeling-effect-sizes." class="section level3">
<h3>Log-odds, odds ratios, and modeling effect sizes.</h3>
<p>Though it’s common for researchers to express their effect sizes as odds ratios, we don’t want to work directly with odds ratios in a meta-analysis. <em>Why?</em> Well, think back on why we model binomial data with the logit link. The logit link transforms a bounded <span class="math inline">\([0, 1]\)</span> parameter space into an unbounded parameter space ranging from negative to positive infinity. For us Bayesians, it also provides a context in which our <span class="math inline">\(\beta\)</span> parameters are approximately Gaussian. However, when we exponentiate those approximately Gaussian log-odds coefficients, the resulting odds ratios aren’t so Gaussian any more. This is why, even if our ultimate goal is to express a meta-analytic effect as an OR, we want to work with effect sizes in the log-odds metric. It allows us to use the Bayesian meta-analytic framework outlined by Gelman et al, above,</p>
<p><span class="math display">\[\begin{align*}
y_j      &amp; \sim \operatorname{Normal}(\theta_j, \sigma_j) \\
\theta_j &amp; \sim \operatorname{Normal}(\mu, \tau),
\end{align*}\]</span></p>
<p>where <span class="math inline">\(y_j\)</span> is the point estimate in the <span class="math inline">\(j\)</span>th study still in the log-odds scale. After fitting the model, we can then exponentiate the meta-analytic parameter <span class="math inline">\(\mu\)</span> into the OR metric.</p>
</div>
<div id="compute-the-study-specific-effect-sizes." class="section level3">
<h3>Compute the study-specific effect sizes.</h3>
<p>Our <code>h</code> data from the Klein et al replication study includes the un-aggregated data from all of the study locations combined. Before we compute our meta-analysis, we’ll need to compute the study-specific effect sizes and standard errors. Here we do so within a nested tibble.</p>
<pre class="r"><code>glms &lt;-
  h %&gt;% 
  select(loc, y, factor) %&gt;% 
  nest(data = c(y, factor)) %&gt;% 
  mutate(glm = map(data, ~update(glm0, data = .))) %&gt;% 
  mutate(coef = map(glm, tidy)) %&gt;% 
  select(-data, -glm) %&gt;% 
  unnest(coef) %&gt;% 
  filter(term == &quot;factorSideEffect&quot;)

# what did we do?
glms %&gt;% 
  mutate_if(is.double, round, digits = 3)</code></pre>
<pre><code>## # A tibble: 59 x 6
##    loc   term             estimate std.error statistic p.value
##    &lt;fct&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
##  1 1     factorSideEffect     2.32     0.475      4.89       0
##  2 2     factorSideEffect     3.64     0.644      5.64       0
##  3 3     factorSideEffect     2.37     0.399      5.96       0
##  4 4     factorSideEffect     2.24     0.263      8.54       0
##  5 5     factorSideEffect     2.02     0.505      4.00       0
##  6 6     factorSideEffect     2.49     0.571      4.36       0
##  7 7     factorSideEffect     2.53     0.658      3.84       0
##  8 8     factorSideEffect     1.78     0.459      3.87       0
##  9 9     factorSideEffect     1.81     0.378      4.79       0
## 10 10    factorSideEffect     2.37     0.495      4.79       0
## # … with 49 more rows</code></pre>
<p>In the <code>estimate</code> column we have all the <span class="math inline">\(y_j\)</span> values and <code>std.error</code> contains the corresponding <span class="math inline">\(\sigma_j\)</span> values. Here they are in a plot.</p>
<pre class="r"><code>color &lt;- viridis_pal(option = &quot;C&quot;)(7)[5]

glms %&gt;% 
  ggplot(aes(x = std.error, y = estimate)) +
  geom_point(color = color) +
  labs(x = expression(sigma[italic(j)]~(&quot;log-odds&quot;)),
       y = expression(italic(y[j])~(&quot;log-odds&quot;)))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="fit-the-bayesian-meta-analysis." class="section level3">
<h3>Fit the Bayesian meta-analysis.</h3>
<p>Now are data are ready, we can express our first Bayesian meta-analysis with the formula</p>
<p><span class="math display">\[\begin{align*}
\text{estimate}_j &amp; \sim \operatorname{Normal}(\theta_j, \; \text{std.error}_j) \\
\theta_j   &amp; \sim \operatorname{Normal}(\mu, \tau) \\
\mu        &amp; \sim \operatorname{Normal}(0, 1.5) \\
\tau       &amp; \sim \operatorname{Exponential}(1),
\end{align*}\]</span></p>
<p>where the last two lines spell out our priors. As we learned in <a href="https://bookdown.org/content/4857/god-spiked-the-integers.html#binomial-regression">Section 11.1</a>, the <span class="math inline">\(\operatorname{Normal}(0, 1.5)\)</span> prior in the log-odds space is just about flat on the probability space. If you wanted to be more conservative, consider something like <span class="math inline">\(\operatorname{Normal}(0, 1)\)</span>. Here’s how to fit the model with <strong>brms</strong>.</p>
<pre class="r"><code>me0 &lt;- 
  brm(data = glms, 
      family = gaussian,
      estimate | se(std.error) ~ 1 + (1 | loc),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(exponential(1), class = sd)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 15)</code></pre>
<p><code>se()</code> is one of the <strong>brms</strong> helper functions designed to provide additional information about the criterion variable. Here it informs <code>brm()</code> that each <code>estimate</code> value has an associated measurement error defined in the <code>std.error</code> column. Unlike the <code>mi()</code> function, which we used earlier in the chapter to accommodate measurement error and the Bayesian imputation of missing data, the <code>se()</code> function is specially designed to handle meta-analyses. <code>se()</code> contains a <code>sigma</code> argument which is set to <code>FALSE</code> by default. This will return a model with no estimate for sigma, which is what we want. The uncertainty around the <code>estimate</code>-value for each study <span class="math inline">\(j\)</span> has already been encoded in the data as <code>std.error</code>.</p>
<p>Let’s look at the model results.</p>
<pre class="r"><code>print(me0)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: estimate | se(std.error) ~ 1 + (1 | loc) 
##    Data: glms (Number of observations: 59) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~loc (Number of levels: 59) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.43      0.09     0.26     0.62 1.00     1956     2389
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.55      0.09     2.38     2.72 1.00     3443     2631
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Our estimate for heterogeneity across studies, <span class="math inline">\(\tau\)</span>, is about 0.4, suggesting modest differences across the studies. The meta-analytic effect, <span class="math inline">\(\mu\)</span>, is about 2.5. Both, recall, are in the log-odds metric. Here we exponentiate <span class="math inline">\(\mu\)</span> to get our odds ratio.</p>
<pre class="r"><code>fixef(me0) %&gt;% exp()</code></pre>
<pre><code>##           Estimate Est.Error     Q2.5    Q97.5
## Intercept 12.79272  1.091829 10.85899 15.25431</code></pre>
<p>If you look back up to the results reported by Klein and colleagues, you’ll see this is rather close to their OR estimate of 11.54.</p>
</div>
<div id="fit-the-bayesian-muiltilevel-alternative." class="section level3">
<h3>Fit the Bayesian muiltilevel alternative.</h3>
<p>We said earlier that meta-analysis is just a special case of the multilevel model, applied to summary data. We typically perform meta-analyses on data summaries because historically it has not been the norm among researchers to make their data publicly available. So effect size summaries were the best we typically had for aggregating study results. However, times are changing (e.g., <a href="https://www.apa.org/monitor/2017/11/trends-open-science.aspx">here</a>, <a href="https://www.blog.google/products/search/making-it-easier-discover-datasets/">here</a>). In this case, Klein and colleagues engaged in open-science practices and reported all their data. Thus we can just directly fit the model</p>
<p><span class="math display">\[\begin{align*}
\text{y}_{ij} &amp; \sim \operatorname{Binomial}(n = 1, p_{ij}) \\
\operatorname{logit}(p_{ij}) &amp; \sim \alpha + \beta \text{factor}_{ij} + u_{\alpha j} + u_{\beta j} \text{factor}_{ij} \\

\begin{bmatrix} u_{\alpha j} \\ u_{\beta j} \end{bmatrix} &amp; \sim \operatorname{MVNormal} \begin{pmatrix} \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \mathbf{SRS} \end{pmatrix} \\

\mathbf S &amp; = \begin{bmatrix} \sigma_\alpha &amp; 0 \\ 0 &amp; \sigma_\beta \end{bmatrix} \\
\mathbf R &amp; = \begin{bmatrix} 0 &amp; \rho_{\alpha \beta} \\ \rho_{\beta \alpha} &amp; 0 \end{bmatrix} \\

\alpha &amp; \sim \operatorname{Normal}(0, 1.5) \\
\beta  &amp; \sim \operatorname{Normal}(0, 1.5) \\
\sigma_\alpha &amp; \sim \operatorname{Exponential}(1) \\
\sigma_\beta  &amp; \sim \operatorname{Exponential}(1) \\
\mathbf R &amp; \sim \operatorname{LKJ}(2),
\end{align*}\]</span></p>
<p>where the criterion variable, <span class="math inline">\(y\)</span>, is nested in <span class="math inline">\(i\)</span> participants within <span class="math inline">\(j\)</span> locations. The <span class="math inline">\(\beta\)</span> parameter is analogous to the meta-analytic effect (<span class="math inline">\(\mu\)</span>) and <span class="math inline">\(\sigma_\beta\)</span> is analogous to the expression of heterogeneity in the meta-analytic effect (<span class="math inline">\(\tau\)</span>). Here is how to fit the model with <strong>brms</strong>.</p>
<pre class="r"><code>me1 &lt;- 
  brm(data = h, 
      family = binomial,
      y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc),
      prior = c(prior(normal(0, 1.5), class = b),
                prior(exponential(1), class = sd),
                prior(lkj(2), class = cor)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 15)</code></pre>
<p>The results for the focal parameters are very similar to those from <code>me0</code>.</p>
<pre class="r"><code>print(me1)</code></pre>
<pre><code>##  Family: binomial 
##   Links: mu = logit 
## Formula: y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc) 
##    Data: h (Number of observations: 6842) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~loc (Number of levels: 59) 
##                                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)                       0.42      0.07     0.30     0.57 1.00     2120     2870
## sd(factorSideEffect)                0.48      0.09     0.32     0.66 1.01     1107     2010
## cor(Intercept,factorSideEffect)    -0.31      0.19    -0.62     0.08 1.00     1487     2276
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept           -1.66      0.08    -1.82    -1.52 1.00     2012     2675
## factorSideEffect     2.57      0.09     2.39     2.76 1.00     2044     2623
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Here’s the multilevel version of the effect size as an odds ratio.</p>
<pre class="r"><code>fixef(me1)[2, -2] %&gt;% exp()</code></pre>
<pre><code>## Estimate     Q2.5    Q97.5 
## 13.02704 10.93772 15.73129</code></pre>
<p>Here we compare the study specific effect sizes, <span class="math inline">\(\theta_j\)</span>, by our two modeling approaches.</p>
<pre class="r"><code>color &lt;- viridis_pal(option = &quot;C&quot;)(7)[3]

# how many levels are there?
n_loc &lt;- distinct(h, loc) %&gt;% count() %&gt;% pull(n)

# rank by meta-analysis
ranks &lt;-
  tibble(Estimate = coef(me0)$loc[, 1, &quot;Intercept&quot;],
         index    = 1:n_loc) %&gt;% 
  arrange(Estimate) %&gt;% 
  mutate(rank = 1:n_loc)

rbind(coef(me0)$loc[, , &quot;Intercept&quot;],
      coef(me1)$loc[, , &quot;factorSideEffect&quot;]) %&gt;% 
  data.frame() %&gt;% 
  mutate(index = rep(1:n_loc, times = 2),
         type  = rep(c(&quot;meta-analysis&quot;, &quot;multilevel model&quot;), each = n_loc)) %&gt;% 
  left_join(select(ranks, -Estimate), 
            by = &quot;index&quot;) %&gt;% 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_pointrange(fatten = 1, color = color) +
  scale_x_continuous(expression(log-odds~effect~size~(theta[italic(j)])), limits = c(0, 4.5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~type)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The results are very similar. You might be curious how to show these results in a more conventional looking forest plot where the names of the groups (typically studies) for the <span class="math inline">\(\theta_j\)</span> values are listed on the left, the point estimate and 95% interval summaries are listed on the right, and the summary for the population level effect, <span class="math inline">\(\mu\)</span>, is listed beneath all all the <span class="math inline">\(\theta_j\)</span>’s. That’ll require some prep work. First we’ll need to reformat the location names. I’ll save the results in an object called <code>labs</code>.</p>
<pre class="r"><code>labs &lt;-
  h %&gt;% 
  mutate(lab = case_when(
    Location == &quot;Social and Work Psychology Department, University of Brasilia, DF, Brazil&quot; ~ &quot;University of Brasilia&quot;,
    Location == &quot;Wilfrid Laurier University, Waterloo, Ontario, Canada&quot; ~ &quot;Wilfrid Laurier University&quot;,
    Location == &quot;University of British Columbia, Vancouver, Canada&quot; ~ &quot;University of British Columbia&quot;,
    Location == &quot;University of Toronto, Scarborough&quot; ~ &quot;University of Toronto&quot;,
    Location == &quot;Division of Social Science, The Hong Kong University of Science and Technology, Hong Kong, China&quot; ~ &quot;Hong Kong University of Science and Technology&quot;,
    Location == &quot;Chinese Academy of Science, Beijing, China&quot; ~ &quot;Chinese Academy of Science&quot;,
    Location == &quot;Shanghai International Studies University, SISU Intercultural Institute, Shanghai, China&quot; ~ &quot;Shanghai International Studies University&quot;,
    Location == &quot;Guangdong Literature &amp; Art Vocational College, Guangzhou, China&quot; ~ &quot;Guangdong Literature &amp; Art Vocational College&quot;,
    Location == &quot;The University of J. E. Purkyně, Ústí nad Labem, Czech Republic&quot; ~ &quot;The University of J. E. Purkyně&quot;,
    Location == &quot;University of Leuven, Belgium&quot; ~ &quot;University of Leuven&quot;,
    Location == &quot;Department of Experimental and Applied Psychology, VU Amsterdam, 1081BT, Amsterdam, The Netherlands&quot; ~ &quot;VU Amsterdam&quot;,
    Location == &quot;Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&quot; ~ &quot;Department of Social Psychology, Tilburg University&quot;,
    Location == &quot;Eindhoven University of Technology, Eindhoven, Netherlands&quot; ~ &quot;Eindhoven University of Technology&quot;,
    Location == &quot;Department of Communication and Information Sciences, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&quot; ~ &quot;Department of Communication and Information Sciences, Tilburg University&quot;,
    Location == &quot;University of Navarra, Spain&quot; ~ &quot;University of Navarra&quot;,
    Location == &quot;University of Lausanne, Switzerland&quot; ~ &quot;University of Lausanne&quot;,
    Location == &quot;Université de Poitiers, France&quot; ~ &quot;Université de Poitiers&quot;,
    Location == &quot;Eotvos Lorand University, in Budapest, Hungary&quot; ~ &quot;Eotvos Lorand University&quot;,
    Location == &quot;MTurk India Workers&quot; ~ &quot;MTurk India Workers&quot;,
    Location == &quot;University of Winchester, Winchester, Hampshire, England&quot; ~ &quot;University of Winchester&quot;,
    Location == &quot;Doshisha University, Kyoto, Japan&quot; ~ &quot;Doshisha University&quot;,
    Location == &quot;Victoria University of Wellington, New Zealand&quot; ~ &quot;Victoria University of Wellington&quot;,
    Location == &quot;University of Social Sciences and Humanities, Wroclaw, Poland&quot; ~ &quot;University of Social Sciences and Humanities&quot;,
    Location == &quot;Department of Psychology, SWPS University of Social Sciences and Humanities Campus Sopot, Sopot, Poland&quot; ~ &quot;SWPS University of Social Sciences and Humanities Campus Sopot&quot;,
    Location == &quot;badania.net&quot; ~ &quot;badania.net&quot;,
    Location == &quot;Universidade do Porto, Portugal&quot; ~ &quot;Universidade do Porto&quot;,
    Location == &quot;University of Belgrade, Belgrade, Serbia&quot; ~ &quot;University of Belgrade&quot;,
    Location == &quot;University of Johannesburg, Johanneburg, South Africa&quot; ~ &quot;University of Johannesburg&quot;,
    Location == &quot;Santiago, Chile&quot; ~ &quot;Santiago, Chile&quot;,
    Location == &quot;Universidad de Costa Rica, Costa Rica&quot; ~ &quot;Universidad de Costa Rica&quot;,
    Location == &quot;National Autonomous University of Mexico in Mexico City&quot; ~ &quot;National Autonomous University of Mexico&quot;,
    Location == &quot;University of the Republic, Montevideo, Uruguay&quot; ~ &quot;University of the Republic&quot;,
    Location == &quot;Lund University, Lund, Sweden&quot; ~ &quot;Lund University&quot;,
    Location == &quot;Academia Sinica, Taiwan National Taiwan Normal University, Taiwan&quot; ~ &quot;Taiwan National Taiwan Normal University&quot;,
    Location == &quot;Bilgi University, Istanbul, Turkey&quot; ~ &quot;Bilgi University&quot;,
    Location == &quot;Koç University, Istanbul, Turkey&quot; ~ &quot;Koç University&quot;,
    Location == &quot;American University of Sharjah, United Arab Emirates&quot; ~ &quot;American University of Sharjah&quot;,
    Location == &quot;University of Hawaii, Honolulu, HI&quot; ~ &quot;University of Hawaii&quot;,
    Location == &quot;Social Science and Policy Studies Department, Worcester Polytechnic Institute, Worcester, MA 01609&quot; ~ &quot;Worcester Polytechnic Institute&quot;,
    Location == &quot;Department of Psychology, Washington and Lee University, Lexington, VA 24450&quot; ~ &quot;Washington and Lee University&quot;,
    Location == &quot;Department of Psychology, San Diego State University, San Diego, CA 92182&quot; ~ &quot;San Diego State University&quot;,
    Location == &quot;Tufts&quot; ~ &quot;Tufts&quot;,
    Location == &quot;University of Florida, Florida&quot; ~ &quot;University of Florida&quot;,
    Location == &quot;University of Illinois at Urbana-Champaign, Champaign, IL&quot; ~ &quot;University of Illinois at Urbana-Champaign&quot;,
    Location == &quot;Pacific Lutheran University, Tacoma, WA&quot; ~ &quot;Pacific Lutheran University&quot;,
    Location == &quot;University of Virginia, VA&quot; ~ &quot;University of Virginia&quot;,
    Location == &quot;Marian University, Indianapolis, IN&quot; ~ &quot;Marian University&quot;,
    Location == &quot;Department of Psychology, Ithaca College, Ithaca, NY 14850&quot; ~ &quot;Ithaca College&quot;,
    Location == &quot;University of Michigan&quot; ~ &quot;University of Michigan&quot;,
    Location == &quot;Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001&quot; ~ &quot;Pennsylvania State University Abington&quot;,
    Location == &quot;Department of Psychology, Texas A&amp;M University, College Station, TX 77843&quot; ~ &quot;Texas A&amp;M University&quot;,
    Location == &quot;William Paterson University, Wayne, NJ&quot; ~ &quot;William Paterson University&quot;,
    Location == &quot;Department of Cognitive Science, Occidental College, Los Angeles, CA&quot; ~ &quot;Occidental College&quot;,
    Location == &quot;The Pennsylvania State University&quot; ~ &quot;The Pennsylvania State University&quot;,
    Location == &quot;MTurk US Workers&quot; ~ &quot;MTurk US Workers&quot;,
    Location == &quot;University of Graz AND the Universty of Vienna&quot; ~ &quot;University of Graz and the Universty of Vienna&quot;,
    Location == &quot;University of Potsdam, Germany&quot; ~ &quot;University of Potsdam&quot;,
    Location == &quot;Open University of Hong Kong&quot; ~ &quot;Open University of Hong Kong&quot;,
    Location == &quot;Concepción, Chile&quot; ~ &quot;Concepción&quot;
  )) %&gt;% 
  distinct(loc, lab)

# what is this?
labs %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 59
## Columns: 2
## $ loc &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,…
## $ lab &lt;chr&gt; &quot;University of Brasilia&quot;, &quot;Wilfrid Laurier University&quot;, &quot;University of British Columbi…</code></pre>
<p>Now we’ll do some tricky wrangling with the output from <code>coef()</code> and <code>fixef()</code> to arrange the odds ratio summaries for the population average and the location-specific results.</p>
<pre class="r"><code># this will help us format the labels on the secondary y-axis
my_format &lt;- function(number) {
  formatC(number, digits = 2, format = &quot;f&quot;)
}

# grab the theta_j summaries
groups &lt;-
  coef(me1)$loc[, , &quot;factorSideEffect&quot;] %&gt;% 
  data.frame() %&gt;% 
  mutate(loc = distinct(h, loc) %&gt;% pull()) %&gt;% 
  arrange(Estimate)

# grat the mu summary
average &lt;-
  fixef(me1) %&gt;% 
  data.frame() %&gt;% 
  slice(2) %&gt;% 
  mutate(loc = &quot;Average&quot;)

# combine and wrangle
post &lt;-
  bind_rows(groups, average) %&gt;% 
  mutate(rank     = c(1:59, 0),
         Estimate = exp(Estimate),
         Q2.5     = exp(Q2.5),
         Q97.5    = exp(Q97.5)) %&gt;% 
  left_join(labs, by = &quot;loc&quot;) %&gt;% 
  arrange(rank) %&gt;% 
  mutate(label   = ifelse(is.na(lab), &quot;POPULATION AVERAGE&quot;, lab),
         summary = str_c(my_format(Estimate), &quot; [&quot;, my_format(Q2.5), &quot;, &quot;, my_format(Q97.5), &quot;]&quot;))

# what have we done?
post %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 9
## $ Estimate  &lt;dbl&gt; 13.027040, 5.994537, 7.225509, 7.894728, 7.896201, 7.989348, 8.158148, 8.425675,…
## $ Est.Error &lt;dbl&gt; 0.09183827, 0.23712115, 0.35418533, 0.32549107, 0.35978096, 0.23125168, 0.343829…
## $ Q2.5      &lt;dbl&gt; 10.937724, 3.752456, 3.537577, 4.170577, 3.898701, 5.147465, 4.109451, 4.488119,…
## $ Q97.5     &lt;dbl&gt; 15.731289, 9.501053, 14.080042, 15.016368, 15.701244, 12.588834, 16.272517, 15.8…
## $ loc       &lt;chr&gt; &quot;Average&quot;, &quot;19&quot;, &quot;38&quot;, &quot;8&quot;, &quot;32&quot;, &quot;55&quot;, &quot;5&quot;, &quot;34&quot;, &quot;22&quot;, &quot;9&quot;, &quot;6&quot;, &quot;58&quot;, &quot;24&quot;, &quot;…
## $ rank      &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22…
## $ lab       &lt;chr&gt; NA, &quot;MTurk India Workers&quot;, &quot;University of Hawaii&quot;, &quot;Guangdong Literature &amp; Art V…
## $ label     &lt;chr&gt; &quot;POPULATION AVERAGE&quot;, &quot;MTurk India Workers&quot;, &quot;University of Hawaii&quot;, &quot;Guangdong …
## $ summary   &lt;chr&gt; &quot;13.03 [10.94, 15.73]&quot;, &quot;5.99 [3.75, 9.50]&quot;, &quot;7.23 [3.54, 14.08]&quot;, &quot;7.89 [4.17, …</code></pre>
<p>Here’s our custom forest plot.</p>
<pre class="r"><code>post %&gt;% 
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +
  geom_interval(aes(color = label == &quot;POPULATION AVERAGE&quot;),
                size = 1/2) +
  geom_point(aes(size = 1 - Est.Error, color = label == &quot;POPULATION AVERAGE&quot;),
             shape = 15) +
  scale_color_viridis_d(option = &quot;C&quot;, begin = .33, end = .67) +
  scale_size_continuous(range = c(1, 3.5)) +
  scale_x_continuous(&quot;odds ratio&quot;, breaks = 1:6 * 10, expand = expansion(mult = c(0.005, 0.005))) +
  scale_y_continuous(NULL, breaks = 0:59, limits = c(-1, 60), expand = c(0, 0),
                     labels = pull(post, label),
                     sec.axis = dup_axis(labels = pull(post, summary))) +
  theme(text = element_text(family = &quot;Times&quot;),
        axis.text.y = element_text(hjust = 0, color = &quot;white&quot;, size = 7),
        axis.text.y.right = element_text(hjust = 1, size = 7),
        axis.ticks.y = element_blank(),
        panel.background = element_rect(fill = &quot;grey8&quot;),
        panel.border = element_rect(color = &quot;transparent&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>You may have noticed this plot is based on the results of our multilevel model, <code>me1</code>. We could have done the same basic thing with the results from the more conventional meta-analysis model, <code>me0</code>, too.</p>
<p>I’m not aware this it typical in random effect meta-analyses, but it might be useful to further clarify the meaning of the two primary parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>. Like with the forest plot, above, we could examine these with either <code>me0</code> or <code>me1</code>. For kicks, we’ll use <code>me0</code> (the conventional Bayesian meta-analysis). In the output from <code>posterior_samples(me0)</code>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are in the columns named <code>b_Intercept</code> and <code>sd_loc__Intercept</code>, respectively.</p>
<pre class="r"><code>post &lt;- posterior_samples(me0)

post %&gt;% 
  select(b_Intercept:sd_loc__Intercept) %&gt;% 
  head()</code></pre>
<pre><code>##   b_Intercept sd_loc__Intercept
## 1    2.378526         0.4688289
## 2    2.562858         0.4555103
## 3    2.435846         0.3252279
## 4    2.658129         0.3895584
## 5    2.451356         0.3583352
## 6    2.672061         0.5595212</code></pre>
<p>If you scroll back above, you’ll see our random effect meta-analysis explicitly presumed our empirical effect-size estimates <span class="math inline">\(y_j\)</span> are approximations of the true effect sizes <span class="math inline">\(\theta_j\)</span>, which are themselves normally distributed in the population of possible effect sizes from similar studies: <span class="math inline">\(\theta_j \sim \operatorname{Normal}(\mu, \tau)\)</span>. Why not use our posterior samples to simulate draws from <span class="math inline">\(\operatorname{Normal}(\mu, \tau)\)</span> to get a sense of what this distribution might look like? Recall that the parameters are in the log-odds metric. We’ll present the distribution in that metric and as odds ratios.</p>
<pre class="r"><code>color &lt;- viridis_pal(option = &quot;C&quot;)(7)[6]
set.seed(15)

post %&gt;% 
  transmute(lo = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept),
            or = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept) %&gt;% exp()) %&gt;% 
  slice(1:1e3) %&gt;% 
  pivot_longer(lo:or, values_to = &quot;effect size&quot;) %&gt;% 
  mutate(name = factor(name, labels = c(&quot;log-odds&quot;, &quot;odds ratio&quot;))) %&gt;% 
  
  ggplot(aes(x = `effect size`, y = 0)) +
  geom_dots(color = color, fill = color) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(Normal(mu*&#39;, &#39;*tau))) +
  theme(text = element_text(family = &quot;Times&quot;),
        strip.background = element_rect(color = &quot;transparent&quot;)) +
  facet_wrap(~name, scales = &quot;free&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Both panels show 1,000 draws, each of which is depicted by a single dot. If we were to run this experiment 1,000 times and compute the effect size separately for each one, this is what we’d expect those distributions of effect sizes to look like. Seems like there’s a lot of variation in there, eh? The next time you observe your fellow scientists debating over whether a study replicated or not, keep these distributions in mind. Once you start thinking about distributions, replication becomes a tricky notion.</p>
</div>
<div id="parting-thoughts." class="section level3">
<h3>Parting thoughts.</h3>
<p>There are other things you might do with these data. For example, you might inspect how much the effect size varies between those from WEIRD and non-WEIRD countries. You might also model the data as clustered by <code>Language</code> rather than by <code>Location</code>. But I think we’ve gone far enough to get you started.</p>
<p>If you’d like to learn more about these methods, do check out Vourre’s <a href="https://mvuorre.github.io/post/2016/09/29/meta-analysis-is-a-special-case-of-bayesian-multilevel-modeling/"><em>Meta-analysis is a special case of Bayesian multilevel modeling</em></a>. You might also read Williams, Rast, and Bürkner’s <span class="citation">(<a href="#ref-williamsBayesianMetaanalysisWeakly2018" role="doc-biblioref">2018</a>)</span> manuscript, <a href="https://psyarxiv.com/7tbrm/"><em>Bayesian meta-analysis with weakly informative prior distributions</em></a>. For an alternative workflow, consider the <a href="https://github.com/wwiecek/baggr"><strong>baggr</strong> package</a> <span class="citation">(<a href="#ref-R-baggr" role="doc-biblioref">Wiecek &amp; Meager, 2020</a>)</span>, which is designed to fit hierarchical Bayesian meta-analyses with Stan under the hood.</p>
</div>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.4 (2021-02-15)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_2.3.1   broom_0.7.5       viridis_0.5.1     viridisLite_0.3.0 ggdark_0.2.1     
##  [6] brms_2.15.0       Rcpp_1.0.6        forcats_0.5.1     stringr_1.4.0     dplyr_1.0.5      
## [11] purrr_0.3.4       readr_1.4.0       tidyr_1.1.3       tibble_3.1.0      ggplot2_3.3.3    
## [16] tidyverse_1.3.0  
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.2.1      plyr_1.8.6           igraph_1.2.6        
##   [5] svUnit_1.0.3         splines_4.0.4        crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.17        digest_0.6.27        htmltools_0.5.1.1   
##  [13] rsconnect_0.8.16     fansi_0.4.2          magrittr_2.0.1       modelr_0.1.8        
##  [17] RcppParallel_5.0.2   matrixStats_0.57.0   xts_0.12.1           sandwich_3.0-0      
##  [21] prettyunits_1.1.1    colorspace_2.0-0     rvest_0.3.6          ggdist_2.4.0.9000   
##  [25] haven_2.3.1          xfun_0.22            callr_3.5.1          crayon_1.4.1        
##  [29] jsonlite_1.7.2       lme4_1.1-25          survival_3.2-10      zoo_1.8-8           
##  [33] glue_1.4.2           gtable_0.3.0         emmeans_1.5.2-1      V8_3.4.0            
##  [37] distributional_0.2.2 pkgbuild_1.2.0       rstan_2.21.2         abind_1.4-5         
##  [41] scales_1.1.1         mvtnorm_1.1-1        DBI_1.1.0            miniUI_0.1.1.1      
##  [45] xtable_1.8-4         stats4_4.0.4         StanHeaders_2.21.0-7 DT_0.16             
##  [49] htmlwidgets_1.5.2    httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  
##  [53] ellipsis_0.3.1       farver_2.0.3         pkgconfig_2.0.3      loo_2.4.1           
##  [57] dbplyr_2.0.0         utf8_1.1.4           labeling_0.4.2       tidyselect_1.1.0    
##  [61] rlang_0.4.10         reshape2_1.4.4       later_1.1.0.1        munsell_0.5.0       
##  [65] cellranger_1.1.0     tools_4.0.4          cli_2.3.1            generics_0.1.0      
##  [69] ggridges_0.5.2       evaluate_0.14        fastmap_1.0.1        yaml_2.2.1          
##  [73] processx_3.4.5       knitr_1.31           fs_1.5.0             nlme_3.1-152        
##  [77] mime_0.10            projpred_2.0.2       xml2_1.3.2           compiler_4.0.4      
##  [81] bayesplot_1.8.0      shinythemes_1.1.2    rstudioapi_0.13      gamm4_0.2-6         
##  [85] curl_4.3             reprex_0.3.0         statmod_1.4.35       stringi_1.5.3       
##  [89] highr_0.8            ps_1.6.0             blogdown_1.3         Brobdingnag_1.2-6   
##  [93] lattice_0.20-41      Matrix_1.3-2         nloptr_1.2.2.2       markdown_1.1        
##  [97] shinyjs_2.0.0        vctrs_0.3.6          pillar_1.5.1         lifecycle_1.0.0     
## [101] bridgesampling_1.0-0 estimability_1.3     httpuv_1.5.4         R6_2.5.0            
## [105] bookdown_0.21        promises_1.1.1       gridExtra_2.3        codetools_0.2-18    
## [109] boot_1.3-26          colourpicker_1.1.0   MASS_7.3-53          gtools_3.8.2        
## [113] assertthat_0.2.1     withr_2.4.1          shinystan_2.5.0      multcomp_1.4-16     
## [117] mgcv_1.8-33          parallel_4.0.4       hms_0.5.3            grid_4.0.4          
## [121] coda_0.19-4          minqa_1.2.4          rmarkdown_2.7        shiny_1.5.0         
## [125] lubridate_1.7.9.2    base64enc_0.1-3      dygraphs_1.1.1.6</code></pre>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-gelman2013bayesian" class="csl-entry">
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). <em>Bayesian data analysis</em> (Third Edition). <span>CRC press</span>. <a href="https://stat.columbia.edu/~gelman/book/">https://stat.columbia.edu/~gelman/book/</a>
</div>
<div id="ref-hauserDissociationMoralJudgments2007" class="csl-entry">
Hauser, M., Cushman, F., Young, L., Jin, R. K.-X., &amp; Mikhail, J. (2007). A dissociation between moral judgments and justifications. <em>Mind &amp; Language</em>, <em>22</em>(1), 1–21. <a href="https://doi.org/10.1111/j.1468-0017.2006.00297.x">https://doi.org/10.1111/j.1468-0017.2006.00297.x</a>
</div>
<div id="ref-kleinManyLabsInvestigating2018" class="csl-entry">
Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many <span>Labs</span> 2: <span>Investigating</span> variation in replicability across samples and settings. <em>Advances in Methods and Practices in Psychological Science</em>, <em>1</em>(4), 443–490. <a href="https://doi.org/10.1177/2515245918810225">https://doi.org/10.1177/2515245918810225</a>
</div>
<div id="ref-kurzStatisticalRethinkingBrms2020" class="csl-entry">
Kurz, A. S. (2020a). <em>Statistical rethinking with brms, <span class="nocase">ggplot2</span>, and the tidyverse</em> (version 1.2.0). <a href="https://doi.org/10.5281/zenodo.3693202">https://doi.org/10.5281/zenodo.3693202</a>
</div>
<div id="ref-kurzStatisticalRethinkingSecondEd2020" class="csl-entry">
Kurz, A. S. (2020b). <em>Statistical rethinking with brms, Ggplot2, and the tidyverse: <span>Second</span> edition</em> (version 0.1.1). <a href="https://bookdown.org/content/4857/">https://bookdown.org/content/4857/</a>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2020" class="csl-entry">
McElreath, R. (2020). <em>Statistical rethinking: <span>A Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (Second Edition). <span>CRC Press</span>. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2015" class="csl-entry">
McElreath, R. (2015). <em>Statistical rethinking: <span>A Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em>. <span>CRC press</span>. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a>
</div>
<div id="ref-R-baggr" class="csl-entry">
Wiecek, W., &amp; Meager, R. (2020). <em><span class="nocase">baggr</span>: <span>Bayesian</span> aggregate treatment effects</em> [Manual]. <a href="https://CRAN.R-project.org/package=baggr">https://CRAN.R-project.org/package=baggr</a>
</div>
<div id="ref-williamsBayesianMetaanalysisWeakly2018" class="csl-entry">
Williams, D. R., Rast, P., &amp; Bürkner, P.-C. (2018). <em>Bayesian meta-analysis with weakly informative prior distributions</em>. <a href="https://doi.org/10.31234/osf.io/7tbrm">https://doi.org/10.31234/osf.io/7tbrm</a>
</div>
</div>
</div>
